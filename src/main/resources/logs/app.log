2022-02-10 17:22:14 INFO  UseCase2:40 - ------------------------------------------running UseCase 2------------------------------------------------------
2022-02-10 17:22:15 INFO  SparkContext:54 - Running Spark version 2.4.8
2022-02-10 17:22:15 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 17:22:15 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 17:22:15 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
2022-02-10 17:22:15 DEBUG MetricsSystemImpl:231 - UgiMetrics, User and group related metrics
2022-02-10 17:22:15 DEBUG KerberosName:88 - Kerberos krb5 configuration not found, setting default realm to empty
2022-02-10 17:22:15 DEBUG Groups:291 -  Creating new Groups object
2022-02-10 17:22:15 DEBUG NativeCodeLoader:46 - Trying to load the custom-built native-hadoop library...
2022-02-10 17:22:15 DEBUG NativeCodeLoader:50 - Loaded the native-hadoop library
2022-02-10 17:22:15 DEBUG JniBasedUnixGroupsMapping:50 - Using JniBasedUnixGroupsMapping for Group resolution
2022-02-10 17:22:15 DEBUG JniBasedUnixGroupsMappingWithFallback:45 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
2022-02-10 17:22:15 ERROR Shell:396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\Users\Anukul Thalkar\hadoop\bin\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:293)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2526)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at util.getSparkSession(util.java:4)
	at UseCase1.getOrders(UseCase1.java:19)
	at UseCase2.main(UseCase2.java:42)
2022-02-10 17:22:15 DEBUG Groups:103 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2022-02-10 17:22:15 DEBUG UserGroupInformation:221 - hadoop login
2022-02-10 17:22:15 DEBUG UserGroupInformation:156 - hadoop login commit
2022-02-10 17:22:15 DEBUG UserGroupInformation:186 - using local user:NTUserPrincipal: Anukul Thalkar
2022-02-10 17:22:15 DEBUG UserGroupInformation:192 - Using user: "NTUserPrincipal: Anukul Thalkar" with name Anukul Thalkar
2022-02-10 17:22:15 DEBUG UserGroupInformation:202 - User entry: "Anukul Thalkar"
2022-02-10 17:22:15 DEBUG UserGroupInformation:825 - UGI loginUser:Anukul Thalkar (auth:SIMPLE)
2022-02-10 17:22:15 INFO  SparkContext:54 - Submitted application: e205ba19-f336-4c44-aba2-ebd4756b746c
2022-02-10 17:22:15 INFO  SecurityManager:54 - Changing view acls to: Anukul Thalkar
2022-02-10 17:22:15 INFO  SecurityManager:54 - Changing modify acls to: Anukul Thalkar
2022-02-10 17:22:15 INFO  SecurityManager:54 - Changing view acls groups to: 
2022-02-10 17:22:15 INFO  SecurityManager:54 - Changing modify acls groups to: 
2022-02-10 17:22:15 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Anukul Thalkar); groups with view permissions: Set(); users  with modify permissions: Set(Anukul Thalkar); groups with modify permissions: Set()
2022-02-10 17:22:15 DEBUG InternalLoggerFactory:45 - Using SLF4J as the default logging framework
2022-02-10 17:22:15 DEBUG InternalThreadLocalMap:56 - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2022-02-10 17:22:15 DEBUG InternalThreadLocalMap:59 - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2022-02-10 17:22:15 DEBUG MultithreadEventLoopGroup:44 - -Dio.netty.eventLoopThreads: 16
2022-02-10 17:22:15 DEBUG NioEventLoop:106 - -Dio.netty.noKeySetOptimization: false
2022-02-10 17:22:16 DEBUG NioEventLoop:107 - -Dio.netty.selectorAutoRebuildThreshold: 512
2022-02-10 17:22:16 DEBUG PlatformDependent:1003 - Platform: Windows
2022-02-10 17:22:16 DEBUG PlatformDependent0:396 - -Dio.netty.noUnsafe: false
2022-02-10 17:22:16 DEBUG PlatformDependent0:852 - Java version: 8
2022-02-10 17:22:16 DEBUG PlatformDependent0:121 - sun.misc.Unsafe.theUnsafe: available
2022-02-10 17:22:16 DEBUG PlatformDependent0:145 - sun.misc.Unsafe.copyMemory: available
2022-02-10 17:22:16 DEBUG PlatformDependent0:183 - java.nio.Buffer.address: available
2022-02-10 17:22:16 DEBUG PlatformDependent0:244 - direct buffer constructor: available
2022-02-10 17:22:16 DEBUG PlatformDependent0:314 - java.nio.Bits.unaligned: available, true
2022-02-10 17:22:16 DEBUG PlatformDependent0:379 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2022-02-10 17:22:16 DEBUG PlatformDependent0:386 - java.nio.DirectByteBuffer.<init>(long, int): available
2022-02-10 17:22:16 DEBUG PlatformDependent:1046 - sun.misc.Unsafe: available
2022-02-10 17:22:16 DEBUG PlatformDependent:1165 - -Dio.netty.tmpdir: C:\Users\ANUKUL~1\AppData\Local\Temp (java.io.tmpdir)
2022-02-10 17:22:16 DEBUG PlatformDependent:1244 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
2022-02-10 17:22:16 DEBUG PlatformDependent:177 - -Dio.netty.maxDirectMemory: 3758096384 bytes
2022-02-10 17:22:16 DEBUG PlatformDependent:184 - -Dio.netty.uninitializedArrayAllocationThreshold: -1
2022-02-10 17:22:16 DEBUG CleanerJava6:92 - java.nio.ByteBuffer.cleaner(): available
2022-02-10 17:22:16 DEBUG PlatformDependent:204 - -Dio.netty.noPreferDirect: false
2022-02-10 17:22:16 DEBUG PlatformDependent:907 - org.jctools-core.MpscChunkedArrayQueue: available
2022-02-10 17:22:16 DEBUG ResourceLeakDetector:130 - -Dio.netty.leakDetection.level: simple
2022-02-10 17:22:16 DEBUG ResourceLeakDetector:131 - -Dio.netty.leakDetection.targetRecords: 4
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:156 - -Dio.netty.allocator.numHeapArenas: 16
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:157 - -Dio.netty.allocator.numDirectArenas: 16
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:159 - -Dio.netty.allocator.pageSize: 8192
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:164 - -Dio.netty.allocator.maxOrder: 11
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:168 - -Dio.netty.allocator.chunkSize: 16777216
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:169 - -Dio.netty.allocator.tinyCacheSize: 512
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:170 - -Dio.netty.allocator.smallCacheSize: 256
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:171 - -Dio.netty.allocator.normalCacheSize: 64
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:172 - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:173 - -Dio.netty.allocator.cacheTrimInterval: 8192
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:174 - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:175 - -Dio.netty.allocator.useCacheForAllThreads: true
2022-02-10 17:22:16 DEBUG PooledByteBufAllocator:176 - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2022-02-10 17:22:16 DEBUG DefaultChannelId:79 - -Dio.netty.processId: 14136 (auto-detected)
2022-02-10 17:22:16 DEBUG NetUtil:139 - -Djava.net.preferIPv4Stack: false
2022-02-10 17:22:16 DEBUG NetUtil:140 - -Djava.net.preferIPv6Addresses: false
2022-02-10 17:22:16 DEBUG NetUtil:224 - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)
2022-02-10 17:22:16 DEBUG NetUtil:289 - Failed to get SOMAXCONN from sysctl and file \proc\sys\net\core\somaxconn. Default: 200
2022-02-10 17:22:16 DEBUG DefaultChannelId:101 - -Dio.netty.machineId: 7c:70:db:ff:fe:41:5e:f6 (auto-detected)
2022-02-10 17:22:16 DEBUG ByteBufUtil:86 - -Dio.netty.allocator.type: pooled
2022-02-10 17:22:16 DEBUG ByteBufUtil:95 - -Dio.netty.threadLocalDirectBufferSize: 0
2022-02-10 17:22:16 DEBUG ByteBufUtil:98 - -Dio.netty.maxThreadLocalCharBufferSize: 16384
2022-02-10 17:22:16 DEBUG TransportServer:141 - Shuffle server started on port: 59487
2022-02-10 17:22:16 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 59487.
2022-02-10 17:22:16 DEBUG SparkEnv:58 - Using serializer: class org.apache.spark.serializer.JavaSerializer
2022-02-10 17:22:16 INFO  SparkEnv:54 - Registering MapOutputTracker
2022-02-10 17:22:16 DEBUG MapOutputTrackerMasterEndpoint:58 - init
2022-02-10 17:22:16 INFO  SparkEnv:54 - Registering BlockManagerMaster
2022-02-10 17:22:16 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2022-02-10 17:22:16 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2022-02-10 17:22:16 INFO  DiskBlockManager:54 - Created local directory at C:\Users\Anukul Thalkar\AppData\Local\Temp\blockmgr-d75dc1f1-62ac-42f1-8785-59aa483df1ba
2022-02-10 17:22:16 DEBUG DiskBlockManager:58 - Adding shutdown hook
2022-02-10 17:22:16 DEBUG ShutdownHookManager:58 - Adding shutdown hook
2022-02-10 17:22:16 INFO  MemoryStore:54 - MemoryStore started with capacity 1970.4 MB
2022-02-10 17:22:16 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2022-02-10 17:22:16 DEBUG OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:58 - init
2022-02-10 17:22:16 DEBUG SecurityManager:58 - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2022-02-10 17:22:16 DEBUG log:159 - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.spark_project.jetty.util.log) via org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 17:22:16 INFO  log:169 - Logging initialized @3056ms to org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@52cb4f50
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1c5c616f{/,null,STOPPED} added {ServletHandler@6c806c8b{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6c806c8b{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0==org.apache.spark.ui.JettyUtils$$anon$3@5d2af02{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6c806c8b{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3ed03652
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@4aedaf61{/,null,STOPPED} added {ServletHandler@173797f0{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@173797f0{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-3c35c345==org.apache.spark.ui.JettyUtils$$anon$3@1f2d48f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@173797f0{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-3c35c345,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4f3e7344
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@7808f638{/,null,STOPPED} added {ServletHandler@62d73ead{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@62d73ead{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-1e141e42==org.apache.spark.ui.JettyUtils$$anon$3@986f29b5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@62d73ead{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-1e141e42,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@228cea97
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1d0a61c8{/,null,STOPPED} added {ServletHandler@46731692{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@46731692{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-782bf610==org.apache.spark.ui.JettyUtils$$anon$3@bf92452c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@46731692{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-782bf610,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1846579f
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6cd166b8{/,null,STOPPED} added {ServletHandler@2650f79{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@2650f79{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-75fc1992==org.apache.spark.ui.JettyUtils$$anon$3@79919fc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@2650f79{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-75fc1992,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@5fac521d
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@38af1bf6{/,null,STOPPED} added {ServletHandler@129bd55d{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@129bd55d{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7be7e15==org.apache.spark.ui.JettyUtils$$anon$3@e539d400{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@129bd55d{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7be7e15,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7a0f244f
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3672276e{/,null,STOPPED} added {ServletHandler@4248b963{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@4248b963{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7f08caf==org.apache.spark.ui.JettyUtils$$anon$3@e922cba5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@4248b963{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7f08caf,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4defd42
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@2330e3e0{/,null,STOPPED} added {ServletHandler@24b4d544{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@24b4d544{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-27a2a089==org.apache.spark.ui.JettyUtils$$anon$3@d0d32b12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@24b4d544{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-27a2a089,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@706eab5d
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@72725ee1{/,null,STOPPED} added {ServletHandler@40e60ece{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@40e60ece{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed==org.apache.spark.ui.JettyUtils$$anon$3@aaa44e10{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@40e60ece{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3a230001
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5ac6c4f2{/,null,STOPPED} added {ServletHandler@2aa6311a{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@2aa6311a{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-61f39bb==org.apache.spark.ui.JettyUtils$$anon$3@5cc1c945{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@2aa6311a{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-61f39bb,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@51b35e4e
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@abff8b7{/,null,STOPPED} added {ServletHandler@6d7cada5{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6d7cada5{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-350a94ce==org.apache.spark.ui.JettyUtils$$anon$3@7dcea13c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6d7cada5{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-350a94ce,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7e00ed0f
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@b0fc838{/,null,STOPPED} added {ServletHandler@3964d79{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@3964d79{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-62db0521==org.apache.spark.ui.JettyUtils$$anon$3@6f85742a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@3964d79{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-62db0521,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6ef1a1b9
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5fbdc49b{/,null,STOPPED} added {ServletHandler@65753040{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@65753040{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea==org.apache.spark.ui.JettyUtils$$anon$3@efa72201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@65753040{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4acb2510
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@7be3a9ce{/,null,STOPPED} added {ServletHandler@37d871c2{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@37d871c2{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-3baf6936==org.apache.spark.ui.JettyUtils$$anon$3@1e29a746{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@37d871c2{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-3baf6936,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@476e8796
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@4eed2acf{/,null,STOPPED} added {ServletHandler@36fc05ff{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@36fc05ff{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e==org.apache.spark.ui.JettyUtils$$anon$3@bea3295{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@36fc05ff{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@642505c7
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@4339e0de{/,null,STOPPED} added {ServletHandler@153cd6bb{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@153cd6bb{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-61d84e08==org.apache.spark.ui.JettyUtils$$anon$3@3851e334{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@153cd6bb{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-61d84e08,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2a685eba
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@c2e3264{/,null,STOPPED} added {ServletHandler@107f4980{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@107f4980{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-75a118e6==org.apache.spark.ui.JettyUtils$$anon$3@6e48cf12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@107f4980{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-75a118e6,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1d540566
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6014a9ba{/,null,STOPPED} added {ServletHandler@acdcf71{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@acdcf71{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-77d680e6==org.apache.spark.ui.JettyUtils$$anon$3@dd7d5587{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@acdcf71{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-77d680e6,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@f08fdce
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6bda1d19{/,null,STOPPED} added {ServletHandler@28c86134{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@28c86134{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4492eede==org.apache.spark.ui.JettyUtils$$anon$3@bda39826{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@28c86134{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4492eede,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@cbc8d0f
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@37b57b54{/,null,STOPPED} added {ServletHandler@5c1f6d57{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@5c1f6d57{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-f288c14==org.apache.spark.ui.JettyUtils$$anon$3@471c624e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@5c1f6d57{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-f288c14,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7be71476
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5cb5bb88{/,null,STOPPED} added {ServletHandler@17b6d426{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG PreEncodedHttpField:61 - HttpField encoders loaded: []
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@17b6d426{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-10641c09==org.spark_project.jetty.servlet.DefaultServlet@48a7ae87{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@17b6d426{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-10641c09,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@27e5b378
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@4422dd48{/,null,STOPPED} added {ServletHandler@764cba{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@764cba{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb==org.apache.spark.ui.JettyUtils$$anon$4@a86fda63{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@764cba{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1a87b51
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@12968227{/,null,STOPPED} added {ServletHandler@144ab54{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@144ab54{STOPPED} added {org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@144ab54{STOPPED} added {[/*]=>org.glassfish.jersey.servlet.ServletContainer-79d06bbd,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3efedc6f
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@45bf6f39{/,null,STOPPED} added {ServletHandler@6c42f2a1{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6c42f2a1{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-17a703f5==org.apache.spark.ui.JettyUtils$$anon$4@5d6baf79{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@6c42f2a1{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-17a703f5,POJO}
2022-02-10 17:22:16 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3531f3ca
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@7fcf294e{/,null,STOPPED} added {ServletHandler@4867ab9f{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@4867ab9f{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0==org.apache.spark.ui.JettyUtils$$anon$4@35f80f7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - ServletHandler@4867ab9f{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0,POJO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[qtp1565614310]@5d5160e6{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY] added {org.spark_project.jetty.util.thread.ThreadPoolBudget@ef1695a,POJO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - Server@4d8286c4{STOPPED}[9.4.z-SNAPSHOT] added {QueuedThreadPool[SparkUI]@5d5160e6{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY],AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - Server@4d8286c4{STOPPED}[9.4.z-SNAPSHOT] added {ErrorHandler@69228e85{STOPPED},AUTO}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - Server@4d8286c4{STOPPED}[9.4.z-SNAPSHOT] added {ContextHandlerCollection@fb6097b{STOPPED},MANAGED}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:201 - starting Server@4d8286c4{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 17:22:16 INFO  Server:375 - jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_281-b09
2022-02-10 17:22:16 DEBUG AbstractHandler:94 - starting Server@4d8286c4{STARTING}[9.4.z-SNAPSHOT]
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:201 - starting QueuedThreadPool[SparkUI]@5d5160e6{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY]
2022-02-10 17:22:16 DEBUG ReservedThreadExecutor:85 - ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}
2022-02-10 17:22:16 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=0<=200,i=0,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}] added {ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0},AUTO}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:201 - starting ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:191 - STARTED @3179ms ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-29,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-30,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-31,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-32,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=4<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-33,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=5<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-34,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-35,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=6<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-36,5,main]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:191 - STARTED @3183ms QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:201 - starting ErrorHandler@69228e85{STOPPED}
2022-02-10 17:22:16 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:16 DEBUG AbstractHandler:94 - starting ErrorHandler@69228e85{STARTING}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:191 - STARTED @3184ms ErrorHandler@69228e85{STARTED}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:201 - starting ContextHandlerCollection@fb6097b{STOPPED}
2022-02-10 17:22:16 DEBUG AbstractHandler:94 - starting ContextHandlerCollection@fb6097b{STARTING}
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:191 - STARTED @3184ms ContextHandlerCollection@fb6097b{STARTED}
2022-02-10 17:22:16 INFO  Server:415 - Started @3184ms
2022-02-10 17:22:16 DEBUG AbstractLifeCycle:191 - STARTED @3185ms Server@4d8286c4{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 17:22:16 DEBUG JettyUtils:58 - Using requestHeaderSize: 8192
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - HttpConnectionFactory@3db972d2[HTTP/1.1] added {HttpConfiguration@1debc91c{32768/8192,8192/8192,https://:0,[]},POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{null, ()}{0.0.0.0:0} added {Server@4d8286c4{STARTED}[9.4.z-SNAPSHOT],UNMANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{null, ()}{0.0.0.0:0} added {QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}],UNMANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{null, ()}{0.0.0.0:0} added {ScheduledExecutorScheduler@64fe9da7{STOPPED},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{null, ()}{0.0.0.0:0} added {org.spark_project.jetty.io.ArrayByteBufferPool@70cccd8f,POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{null, (http/1.1)}{0.0.0.0:0} added {HttpConnectionFactory@3db972d2[HTTP/1.1],AUTO}
2022-02-10 17:22:17 DEBUG AbstractConnector:484 - ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added HttpConnectionFactory@3db972d2[HTTP/1.1]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added {SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:0},MANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:4040],POJO}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ScheduledExecutorScheduler@64fe9da7{STOPPED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3215ms ScheduledExecutorScheduler@64fe9da7{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting HttpConnectionFactory@3db972d2[HTTP/1.1]
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3216ms HttpConnectionFactory@3db972d2[HTTP/1.1]
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.047+05:30 added {SelectorProducer@a2ddf26,POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.05+05:30 added {QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}],UNMANAGED}
2022-02-10 17:22:17 DEBUG EatWhatYouKill:93 - EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.05+05:30 created
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ManagedSelector@7a04fea7{STOPPED} id=0 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.05+05:30,MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@7a04fea7{STOPPED} id=0 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.051+05:30 added {SelectorProducer@7b6e5c12,POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.051+05:30 added {QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}],UNMANAGED}
2022-02-10 17:22:17 DEBUG EatWhatYouKill:93 - EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.051+05:30 created
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ManagedSelector@4f66ffc8{STOPPED} id=1 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.051+05:30,MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@4f66ffc8{STOPPED} id=1 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.052+05:30 added {SelectorProducer@2def7a7a,POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.052+05:30 added {QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}],UNMANAGED}
2022-02-10 17:22:17 DEBUG EatWhatYouKill:93 - EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.052+05:30 created
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ManagedSelector@24e83d19{STOPPED} id=2 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.052+05:30,MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@24e83d19{STOPPED} id=2 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.053+05:30 added {SelectorProducer@188cbcde,POJO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.053+05:30 added {QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}],UNMANAGED}
2022-02-10 17:22:17 DEBUG EatWhatYouKill:93 - EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.053+05:30 created
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ManagedSelector@4ee6291f{STOPPED} id=3 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.053+05:30,MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@4ee6291f{STOPPED} id=3 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@7a04fea7{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.054+05:30
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3246ms EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.054+05:30
2022-02-10 17:22:17 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@4af70944 startThread=0
2022-02-10 17:22:17 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@4af70944 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:17 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@36a6bea6 on ManagedSelector@7a04fea7{STARTING} id=0 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG EatWhatYouKill:141 - EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.057+05:30 tryProduce false
2022-02-10 17:22:17 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:22:17 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@36a6bea6
2022-02-10 17:22:17 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:22:17 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 waiting with 0 keys
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3252ms ManagedSelector@7a04fea7{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@4f66ffc8{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.061+05:30
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3253ms EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.061+05:30
2022-02-10 17:22:17 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@44e93c1f startThread=0
2022-02-10 17:22:17 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@44e93c1f in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:17 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@42373389 on ManagedSelector@4f66ffc8{STARTING} id=1 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG EatWhatYouKill:141 - EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.061+05:30 tryProduce false
2022-02-10 17:22:17 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:22:17 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@42373389
2022-02-10 17:22:17 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:22:17 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 waiting with 0 keys
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3254ms ManagedSelector@4f66ffc8{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@24e83d19{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.063+05:30
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3255ms EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.063+05:30
2022-02-10 17:22:17 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@9b21bd3 startThread=0
2022-02-10 17:22:17 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@9b21bd3 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:17 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@a62c7cd on ManagedSelector@24e83d19{STARTING} id=2 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG EatWhatYouKill:141 - EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.064+05:30 tryProduce false
2022-02-10 17:22:17 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:22:17 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@a62c7cd
2022-02-10 17:22:17 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3256ms ManagedSelector@24e83d19{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 waiting with 0 keys
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@4ee6291f{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.065+05:30
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3257ms EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.065+05:30
2022-02-10 17:22:17 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@65c33b92 startThread=0
2022-02-10 17:22:17 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@65c33b92 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:17 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@7903d448 on ManagedSelector@4ee6291f{STARTING} id=3 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG EatWhatYouKill:141 - EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:22:17.066+05:30 tryProduce false
2022-02-10 17:22:17 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:22:17 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@7903d448
2022-02-10 17:22:17 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:22:17 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 waiting with 0 keys
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3258ms ManagedSelector@4ee6291f{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3259ms SelectorManager@ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {acceptor-0@1f0b3cfe,POJO}
2022-02-10 17:22:17 DEBUG QueuedThreadPool:719 - queue acceptor-0@1f0b3cfe startThread=0
2022-02-10 17:22:17 DEBUG QueuedThreadPool:1035 - run acceptor-0@1f0b3cfe in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:22:17 INFO  AbstractConnector:331 - Started ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3260ms ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:22:17 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - Server@4d8286c4{STARTED}[9.4.z-SNAPSHOT] added {Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040},UNMANAGED}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@5d5c04f9{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6f49d153{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5d5c04f9{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@5d5c04f9{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5d5c04f9{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5d5c04f9{STARTING,min=32,inflate=-1} added {DeflaterPool@1292071f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@5d5c04f9{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6c806c8b{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0==org.apache.spark.ui.JettyUtils$$anon$3@5d2af02{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0=org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0==org.apache.spark.ui.JettyUtils$$anon$3@5d2af02{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@6c806c8b{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3286ms ServletHandler@6c806c8b{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0==org.apache.spark.ui.JettyUtils$$anon$3@5d2af02{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3291ms org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0==org.apache.spark.ui.JettyUtils$$anon$3@5d2af02{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-23f3dbf0
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3293ms o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1292071f{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3293ms DeflaterPool@1292071f{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3293ms GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@660f0c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@7a0ef219{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@660f0c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@660f0c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@660f0c{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@660f0c{STARTING,min=32,inflate=-1} added {DeflaterPool@1b1f5012{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@660f0c{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@173797f0{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-3c35c345[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-3c35c345==org.apache.spark.ui.JettyUtils$$anon$3@1f2d48f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3c35c345=org.apache.spark.ui.JettyUtils$$anon$3-3c35c345==org.apache.spark.ui.JettyUtils$$anon$3@1f2d48f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@173797f0{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3296ms ServletHandler@173797f0{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-3c35c345==org.apache.spark.ui.JettyUtils$$anon$3@1f2d48f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3296ms org.apache.spark.ui.JettyUtils$$anon$3-3c35c345==org.apache.spark.ui.JettyUtils$$anon$3@1f2d48f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-3c35c345
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3296ms o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1b1f5012{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3296ms DeflaterPool@1b1f5012{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3296ms GzipHandler@660f0c{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@7add323c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@4a734c04{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@7add323c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@7add323c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@7add323c{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@7add323c{STARTING,min=32,inflate=-1} added {DeflaterPool@4760f169{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@7add323c{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@62d73ead{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-1e141e42[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-1e141e42==org.apache.spark.ui.JettyUtils$$anon$3@986f29b5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-1e141e42=org.apache.spark.ui.JettyUtils$$anon$3-1e141e42==org.apache.spark.ui.JettyUtils$$anon$3@986f29b5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@62d73ead{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3300ms ServletHandler@62d73ead{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-1e141e42==org.apache.spark.ui.JettyUtils$$anon$3@986f29b5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3301ms org.apache.spark.ui.JettyUtils$$anon$3-1e141e42==org.apache.spark.ui.JettyUtils$$anon$3@986f29b5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-1e141e42
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3302ms o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@4760f169{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3302ms DeflaterPool@4760f169{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3302ms GzipHandler@7add323c{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@658255aa{STOPPED,min=32,inflate=-1} mime types IncludeExclude@25290bca{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@658255aa{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@658255aa{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@658255aa{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@658255aa{STARTING,min=32,inflate=-1} added {DeflaterPool@76563d26{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@658255aa{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@46731692{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-782bf610[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-782bf610==org.apache.spark.ui.JettyUtils$$anon$3@bf92452c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-782bf610=org.apache.spark.ui.JettyUtils$$anon$3-782bf610==org.apache.spark.ui.JettyUtils$$anon$3@bf92452c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@46731692{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3306ms ServletHandler@46731692{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-782bf610==org.apache.spark.ui.JettyUtils$$anon$3@bf92452c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3306ms org.apache.spark.ui.JettyUtils$$anon$3-782bf610==org.apache.spark.ui.JettyUtils$$anon$3@bf92452c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-782bf610
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3307ms o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@76563d26{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3307ms DeflaterPool@76563d26{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3307ms GzipHandler@658255aa{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@4ac86d6a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3d904e9c{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@4ac86d6a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@4ac86d6a{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4ac86d6a{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@4ac86d6a{STARTING,min=32,inflate=-1} added {DeflaterPool@508a65bf{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@4ac86d6a{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2650f79{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-75fc1992[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-75fc1992==org.apache.spark.ui.JettyUtils$$anon$3@79919fc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-75fc1992=org.apache.spark.ui.JettyUtils$$anon$3-75fc1992==org.apache.spark.ui.JettyUtils$$anon$3@79919fc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@2650f79{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3309ms ServletHandler@2650f79{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-75fc1992==org.apache.spark.ui.JettyUtils$$anon$3@79919fc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3310ms org.apache.spark.ui.JettyUtils$$anon$3-75fc1992==org.apache.spark.ui.JettyUtils$$anon$3@79919fc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-75fc1992
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3310ms o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@508a65bf{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3310ms DeflaterPool@508a65bf{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3311ms GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@17f2dd85{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1e58512c{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@17f2dd85{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@17f2dd85{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@17f2dd85{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@17f2dd85{STARTING,min=32,inflate=-1} added {DeflaterPool@210308d5{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@17f2dd85{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@129bd55d{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7be7e15[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7be7e15==org.apache.spark.ui.JettyUtils$$anon$3@e539d400{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7be7e15=org.apache.spark.ui.JettyUtils$$anon$3-7be7e15==org.apache.spark.ui.JettyUtils$$anon$3@e539d400{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@129bd55d{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3313ms ServletHandler@129bd55d{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7be7e15==org.apache.spark.ui.JettyUtils$$anon$3@e539d400{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3314ms org.apache.spark.ui.JettyUtils$$anon$3-7be7e15==org.apache.spark.ui.JettyUtils$$anon$3@e539d400{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7be7e15
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3314ms o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@210308d5{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3314ms DeflaterPool@210308d5{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3314ms GzipHandler@17f2dd85{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@22a736d7{STOPPED,min=32,inflate=-1} mime types IncludeExclude@23b8d9f3{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@22a736d7{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@22a736d7{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@22a736d7{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@22a736d7{STARTING,min=32,inflate=-1} added {DeflaterPool@7f353d99{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@22a736d7{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4248b963{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7f08caf[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7f08caf==org.apache.spark.ui.JettyUtils$$anon$3@e922cba5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7f08caf=org.apache.spark.ui.JettyUtils$$anon$3-7f08caf==org.apache.spark.ui.JettyUtils$$anon$3@e922cba5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@4248b963{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3317ms ServletHandler@4248b963{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7f08caf==org.apache.spark.ui.JettyUtils$$anon$3@e922cba5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3319ms org.apache.spark.ui.JettyUtils$$anon$3-7f08caf==org.apache.spark.ui.JettyUtils$$anon$3@e922cba5{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7f08caf
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3319ms o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@7f353d99{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3319ms DeflaterPool@7f353d99{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3319ms GzipHandler@22a736d7{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@66273da0{STOPPED,min=32,inflate=-1} mime types IncludeExclude@2127e66e{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@66273da0{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@66273da0{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@66273da0{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@66273da0{STARTING,min=32,inflate=-1} added {DeflaterPool@1229a2b7{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@66273da0{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@24b4d544{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-27a2a089[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-27a2a089==org.apache.spark.ui.JettyUtils$$anon$3@d0d32b12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-27a2a089=org.apache.spark.ui.JettyUtils$$anon$3-27a2a089==org.apache.spark.ui.JettyUtils$$anon$3@d0d32b12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@24b4d544{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3323ms ServletHandler@24b4d544{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-27a2a089==org.apache.spark.ui.JettyUtils$$anon$3@d0d32b12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3323ms org.apache.spark.ui.JettyUtils$$anon$3-27a2a089==org.apache.spark.ui.JettyUtils$$anon$3@d0d32b12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-27a2a089
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3324ms o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1229a2b7{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3324ms DeflaterPool@1229a2b7{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3324ms GzipHandler@66273da0{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@e5cbff2{STOPPED,min=32,inflate=-1} mime types IncludeExclude@51c959a4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@e5cbff2{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@e5cbff2{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@e5cbff2{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@e5cbff2{STARTING,min=32,inflate=-1} added {DeflaterPool@4fc3c165{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@e5cbff2{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@40e60ece{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed==org.apache.spark.ui.JettyUtils$$anon$3@aaa44e10{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed=org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed==org.apache.spark.ui.JettyUtils$$anon$3@aaa44e10{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@40e60ece{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3326ms ServletHandler@40e60ece{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed==org.apache.spark.ui.JettyUtils$$anon$3@aaa44e10{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3326ms org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed==org.apache.spark.ui.JettyUtils$$anon$3@aaa44e10{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-3f9270ed
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3326ms o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@4fc3c165{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3326ms DeflaterPool@4fc3c165{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3326ms GzipHandler@e5cbff2{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@10a0fe30{STOPPED,min=32,inflate=-1} mime types IncludeExclude@7b6860f9{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@10a0fe30{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@10a0fe30{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@10a0fe30{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@10a0fe30{STARTING,min=32,inflate=-1} added {DeflaterPool@60f70249{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@10a0fe30{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2aa6311a{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-61f39bb[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-61f39bb==org.apache.spark.ui.JettyUtils$$anon$3@5cc1c945{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-61f39bb=org.apache.spark.ui.JettyUtils$$anon$3-61f39bb==org.apache.spark.ui.JettyUtils$$anon$3@5cc1c945{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@2aa6311a{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3329ms ServletHandler@2aa6311a{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-61f39bb==org.apache.spark.ui.JettyUtils$$anon$3@5cc1c945{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3329ms org.apache.spark.ui.JettyUtils$$anon$3-61f39bb==org.apache.spark.ui.JettyUtils$$anon$3@5cc1c945{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-61f39bb
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3329ms o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@60f70249{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3329ms DeflaterPool@60f70249{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3329ms GzipHandler@10a0fe30{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@31ee2fdb{STOPPED,min=32,inflate=-1} mime types IncludeExclude@262816a8{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@31ee2fdb{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@abff8b7{/storage,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@31ee2fdb{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@31ee2fdb{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@31ee2fdb{STARTING,min=32,inflate=-1} added {DeflaterPool@1effd53c{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@31ee2fdb{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@abff8b7{/storage,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@abff8b7{/storage,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6d7cada5{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-350a94ce[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-350a94ce==org.apache.spark.ui.JettyUtils$$anon$3@7dcea13c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-350a94ce=org.apache.spark.ui.JettyUtils$$anon$3-350a94ce==org.apache.spark.ui.JettyUtils$$anon$3@7dcea13c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@6d7cada5{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3334ms ServletHandler@6d7cada5{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-350a94ce==org.apache.spark.ui.JettyUtils$$anon$3@7dcea13c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3334ms org.apache.spark.ui.JettyUtils$$anon$3-350a94ce==org.apache.spark.ui.JettyUtils$$anon$3@7dcea13c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-350a94ce
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3335ms o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1effd53c{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3335ms DeflaterPool@1effd53c{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3335ms GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@46c269e0{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6920614{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@46c269e0{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@46c269e0{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@46c269e0{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@46c269e0{STARTING,min=32,inflate=-1} added {DeflaterPool@6069dd38{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@46c269e0{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3964d79{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-62db0521[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-62db0521==org.apache.spark.ui.JettyUtils$$anon$3@6f85742a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-62db0521=org.apache.spark.ui.JettyUtils$$anon$3-62db0521==org.apache.spark.ui.JettyUtils$$anon$3@6f85742a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@3964d79{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3339ms ServletHandler@3964d79{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-62db0521==org.apache.spark.ui.JettyUtils$$anon$3@6f85742a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3340ms org.apache.spark.ui.JettyUtils$$anon$3-62db0521==org.apache.spark.ui.JettyUtils$$anon$3@6f85742a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-62db0521
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3340ms o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@6069dd38{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3340ms DeflaterPool@6069dd38{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3340ms GzipHandler@46c269e0{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@5fa23c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@558756be{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5fa23c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@5fa23c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5fa23c{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5fa23c{STARTING,min=32,inflate=-1} added {DeflaterPool@433348bc{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@5fa23c{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@65753040{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea==org.apache.spark.ui.JettyUtils$$anon$3@efa72201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea=org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea==org.apache.spark.ui.JettyUtils$$anon$3@efa72201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@65753040{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3343ms ServletHandler@65753040{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea==org.apache.spark.ui.JettyUtils$$anon$3@efa72201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3343ms org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea==org.apache.spark.ui.JettyUtils$$anon$3@efa72201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-2954b5ea
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3344ms o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@433348bc{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3344ms DeflaterPool@433348bc{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3344ms GzipHandler@5fa23c{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@6d1dcdff{STOPPED,min=32,inflate=-1} mime types IncludeExclude@102ecc22{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@6d1dcdff{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@6d1dcdff{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@6d1dcdff{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@6d1dcdff{STARTING,min=32,inflate=-1} added {DeflaterPool@7ff35a3f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@6d1dcdff{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@37d871c2{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-3baf6936[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-3baf6936==org.apache.spark.ui.JettyUtils$$anon$3@1e29a746{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3baf6936=org.apache.spark.ui.JettyUtils$$anon$3-3baf6936==org.apache.spark.ui.JettyUtils$$anon$3@1e29a746{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@37d871c2{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3348ms ServletHandler@37d871c2{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-3baf6936==org.apache.spark.ui.JettyUtils$$anon$3@1e29a746{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3348ms org.apache.spark.ui.JettyUtils$$anon$3-3baf6936==org.apache.spark.ui.JettyUtils$$anon$3@1e29a746{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-3baf6936
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3349ms o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@7ff35a3f{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3349ms DeflaterPool@7ff35a3f{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3349ms GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@26dc9bd5{STOPPED,min=32,inflate=-1} mime types IncludeExclude@252dc8c4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@26dc9bd5{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@26dc9bd5{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@26dc9bd5{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@26dc9bd5{STARTING,min=32,inflate=-1} added {DeflaterPool@43045f9f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@26dc9bd5{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@36fc05ff{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e==org.apache.spark.ui.JettyUtils$$anon$3@bea3295{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e=org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e==org.apache.spark.ui.JettyUtils$$anon$3@bea3295{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@36fc05ff{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3356ms ServletHandler@36fc05ff{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e==org.apache.spark.ui.JettyUtils$$anon$3@bea3295{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3356ms org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e==org.apache.spark.ui.JettyUtils$$anon$3@bea3295{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-57c47a9e
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3356ms o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@43045f9f{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3356ms DeflaterPool@43045f9f{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3356ms GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@2643d762{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6403e24c{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@2643d762{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@2643d762{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@2643d762{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@2643d762{STARTING,min=32,inflate=-1} added {DeflaterPool@2f236de0{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@2643d762{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@153cd6bb{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-61d84e08[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-61d84e08==org.apache.spark.ui.JettyUtils$$anon$3@3851e334{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-61d84e08=org.apache.spark.ui.JettyUtils$$anon$3-61d84e08==org.apache.spark.ui.JettyUtils$$anon$3@3851e334{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@153cd6bb{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3361ms ServletHandler@153cd6bb{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-61d84e08==org.apache.spark.ui.JettyUtils$$anon$3@3851e334{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3362ms org.apache.spark.ui.JettyUtils$$anon$3-61d84e08==org.apache.spark.ui.JettyUtils$$anon$3@3851e334{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-61d84e08
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3362ms o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2f236de0{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3362ms DeflaterPool@2f236de0{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3362ms GzipHandler@2643d762{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@4eba373c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@767a014e{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@4eba373c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@c2e3264{/executors,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@4eba373c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4eba373c{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@4eba373c{STARTING,min=32,inflate=-1} added {DeflaterPool@d109c4f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@4eba373c{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@c2e3264{/executors,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@c2e3264{/executors,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@107f4980{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-75a118e6[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-75a118e6==org.apache.spark.ui.JettyUtils$$anon$3@6e48cf12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-75a118e6=org.apache.spark.ui.JettyUtils$$anon$3-75a118e6==org.apache.spark.ui.JettyUtils$$anon$3@6e48cf12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@107f4980{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3367ms ServletHandler@107f4980{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-75a118e6==org.apache.spark.ui.JettyUtils$$anon$3@6e48cf12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3368ms org.apache.spark.ui.JettyUtils$$anon$3-75a118e6==org.apache.spark.ui.JettyUtils$$anon$3@6e48cf12{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-75a118e6
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3369ms o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@d109c4f{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3369ms DeflaterPool@d109c4f{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3369ms GzipHandler@4eba373c{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@3968bc60{STOPPED,min=32,inflate=-1} mime types IncludeExclude@26f46fa6{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@3968bc60{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@3968bc60{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@3968bc60{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@3968bc60{STARTING,min=32,inflate=-1} added {DeflaterPool@227a47{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@3968bc60{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@acdcf71{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-77d680e6[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-77d680e6==org.apache.spark.ui.JettyUtils$$anon$3@dd7d5587{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-77d680e6=org.apache.spark.ui.JettyUtils$$anon$3-77d680e6==org.apache.spark.ui.JettyUtils$$anon$3@dd7d5587{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@acdcf71{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3373ms ServletHandler@acdcf71{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-77d680e6==org.apache.spark.ui.JettyUtils$$anon$3@dd7d5587{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3374ms org.apache.spark.ui.JettyUtils$$anon$3-77d680e6==org.apache.spark.ui.JettyUtils$$anon$3@dd7d5587{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-77d680e6
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3374ms o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@227a47{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3374ms DeflaterPool@227a47{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3374ms GzipHandler@3968bc60{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@9596ce8{STOPPED,min=32,inflate=-1} mime types IncludeExclude@75ae4a1f{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@9596ce8{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@9596ce8{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@9596ce8{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@9596ce8{STARTING,min=32,inflate=-1} added {DeflaterPool@70228253{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@9596ce8{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@28c86134{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4492eede[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4492eede==org.apache.spark.ui.JettyUtils$$anon$3@bda39826{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4492eede=org.apache.spark.ui.JettyUtils$$anon$3-4492eede==org.apache.spark.ui.JettyUtils$$anon$3@bda39826{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@28c86134{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3380ms ServletHandler@28c86134{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4492eede==org.apache.spark.ui.JettyUtils$$anon$3@bda39826{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3380ms org.apache.spark.ui.JettyUtils$$anon$3-4492eede==org.apache.spark.ui.JettyUtils$$anon$3@bda39826{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4492eede
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3381ms o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@70228253{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3381ms DeflaterPool@70228253{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3381ms GzipHandler@9596ce8{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@63c12e52{STOPPED,min=32,inflate=-1} mime types IncludeExclude@21bd20ee{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@63c12e52{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@63c12e52{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@63c12e52{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@63c12e52{STARTING,min=32,inflate=-1} added {DeflaterPool@26c47874{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@63c12e52{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@5c1f6d57{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-f288c14[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-f288c14==org.apache.spark.ui.JettyUtils$$anon$3@471c624e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-f288c14=org.apache.spark.ui.JettyUtils$$anon$3-f288c14==org.apache.spark.ui.JettyUtils$$anon$3@471c624e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@5c1f6d57{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3386ms ServletHandler@5c1f6d57{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-f288c14==org.apache.spark.ui.JettyUtils$$anon$3@471c624e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3386ms org.apache.spark.ui.JettyUtils$$anon$3-f288c14==org.apache.spark.ui.JettyUtils$$anon$3@471c624e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-f288c14
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3386ms o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@26c47874{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3386ms DeflaterPool@26c47874{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3386ms GzipHandler@63c12e52{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@421056e5{STOPPED,min=32,inflate=-1} mime types IncludeExclude@2849434b{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@421056e5{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@421056e5{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@421056e5{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@421056e5{STARTING,min=32,inflate=-1} added {DeflaterPool@60bbacfc{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@421056e5{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@17b6d426{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-10641c09[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-10641c09==org.spark_project.jetty.servlet.DefaultServlet@48a7ae87{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-10641c09=org.spark_project.jetty.servlet.DefaultServlet-10641c09==org.spark_project.jetty.servlet.DefaultServlet@48a7ae87{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@17b6d426{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3389ms ServletHandler@17b6d426{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-10641c09==org.spark_project.jetty.servlet.DefaultServlet@48a7ae87{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3389ms org.spark_project.jetty.servlet.DefaultServlet-10641c09==org.spark_project.jetty.servlet.DefaultServlet@48a7ae87{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-10641c09
2022-02-10 17:22:17 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-core_2.11/2.4.8/spark-core_2.11-2.4.8.jar!/org/apache/spark/ui/static
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3401ms o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@60bbacfc{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3402ms DeflaterPool@60bbacfc{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3402ms GzipHandler@421056e5{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@277b8fa4{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6cd64ee8{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@277b8fa4{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@4422dd48{/,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@277b8fa4{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@277b8fa4{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@277b8fa4{STARTING,min=32,inflate=-1} added {DeflaterPool@620c8641{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@277b8fa4{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@4422dd48{/,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@4422dd48{/,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@764cba{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb==org.apache.spark.ui.JettyUtils$$anon$4@a86fda63{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb=org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb==org.apache.spark.ui.JettyUtils$$anon$4@a86fda63{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@764cba{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3406ms ServletHandler@764cba{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb==org.apache.spark.ui.JettyUtils$$anon$4@a86fda63{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3406ms org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb==org.apache.spark.ui.JettyUtils$$anon$4@a86fda63{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-523d6bdb
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3407ms o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@620c8641{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3407ms DeflaterPool@620c8641{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3407ms GzipHandler@277b8fa4{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@2f1d0bbc{STOPPED,min=32,inflate=-1} mime types IncludeExclude@5460b754{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@2f1d0bbc{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@12968227{/api,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@2f1d0bbc{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@2f1d0bbc{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@2f1d0bbc{STARTING,min=32,inflate=-1} added {DeflaterPool@a9f023e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@2f1d0bbc{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@12968227{/api,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@12968227{/api,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@144ab54{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-79d06bbd[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-79d06bbd=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG ServletHandler:169 - Adding Default404Servlet to ServletHandler@144ab54{STARTING}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@144ab54{STARTING} added {org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@144ab54{STARTING} added {[/]=>org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56,POJO}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-79d06bbd[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-79d06bbd=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-79d06bbd[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-79d06bbd=org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@144ab54{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms ServletHandler@144ab54{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms org.glassfish.jersey.servlet.ServletContainer-79d06bbd==org.glassfish.jersey.servlet.ServletContainer@9f01706a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-1a2bcd56==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@dbb8ba06{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@a9f023e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms DeflaterPool@a9f023e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3413ms GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@68d7a2df{STOPPED,min=32,inflate=-1} mime types IncludeExclude@59dc36d4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@68d7a2df{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@68d7a2df{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@68d7a2df{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@68d7a2df{STARTING,min=32,inflate=-1} added {DeflaterPool@12fcc71f{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@68d7a2df{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6c42f2a1{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-17a703f5[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-17a703f5==org.apache.spark.ui.JettyUtils$$anon$4@5d6baf79{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-17a703f5=org.apache.spark.ui.JettyUtils$$anon$4-17a703f5==org.apache.spark.ui.JettyUtils$$anon$4@5d6baf79{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@6c42f2a1{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3417ms ServletHandler@6c42f2a1{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-17a703f5==org.apache.spark.ui.JettyUtils$$anon$4@5d6baf79{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3417ms org.apache.spark.ui.JettyUtils$$anon$4-17a703f5==org.apache.spark.ui.JettyUtils$$anon$4@5d6baf79{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-17a703f5
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3417ms o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@12fcc71f{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3417ms DeflaterPool@12fcc71f{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3417ms GzipHandler@68d7a2df{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG GzipHandler:208 - GzipHandler@5679e96b{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3ed7821{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5679e96b{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {GzipHandler@5679e96b{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5679e96b{STOPPED,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - GzipHandler@5679e96b{STARTING,min=32,inflate=-1} added {DeflaterPool@3bbf841e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting GzipHandler@5679e96b{STARTING,min=32,inflate=-1}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4867ab9f{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0==org.apache.spark.ui.JettyUtils$$anon$4@35f80f7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0=org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0==org.apache.spark.ui.JettyUtils$$anon$4@35f80f7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@4867ab9f{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3422ms ServletHandler@4867ab9f{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0==org.apache.spark.ui.JettyUtils$$anon$4@35f80f7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3423ms org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0==org.apache.spark.ui.JettyUtils$$anon$4@35f80f7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-65f2f9b0
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3423ms o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@3bbf841e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3423ms DeflaterPool@3bbf841e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3423ms GzipHandler@5679e96b{STARTED,min=32,inflate=-1}
2022-02-10 17:22:17 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://Clairvoyant-324.mshome.net:4040
2022-02-10 17:22:17 INFO  Executor:54 - Starting executor ID driver on host localhost
2022-02-10 17:22:17 DEBUG TransportServer:141 - Shuffle server started on port: 59502
2022-02-10 17:22:17 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59502.
2022-02-10 17:22:17 INFO  NettyBlockTransferService:54 - Server created on Clairvoyant-324.mshome.net:59502
2022-02-10 17:22:17 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2022-02-10 17:22:17 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:22:17 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:22:17 INFO  BlockManagerMasterEndpoint:54 - Registering block manager Clairvoyant-324.mshome.net:59502 with 1970.4 MB RAM, BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:22:17 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:22:17 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6413d7e7
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@d16be4f{/,null,STOPPED} added {ServletHandler@17b37e9a{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@17b37e9a{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6==org.apache.spark.ui.JettyUtils$$anon$3@58fc2709{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@17b37e9a{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6,POJO}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@17b37e9a{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6==org.apache.spark.ui.JettyUtils$$anon$3@58fc2709{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6=org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6==org.apache.spark.ui.JettyUtils$$anon$3@58fc2709{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@17b37e9a{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3747ms ServletHandler@17b37e9a{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6==org.apache.spark.ui.JettyUtils$$anon$3@58fc2709{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3747ms org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6==org.apache.spark.ui.JettyUtils$$anon$3@58fc2709{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-6bcb12e6
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3747ms o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG SparkContext:58 - Adding shutdown hook
2022-02-10 17:22:17 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse').
2022-02-10 17:22:17 INFO  SharedState:54 - Warehouse path is 'file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse'.
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@574059d5
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6d672bd4{/,null,STOPPED} added {ServletHandler@67770b37{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@67770b37{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-72906e==org.apache.spark.ui.JettyUtils$$anon$3@cee7e8d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@67770b37{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-72906e,POJO}
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@5529ff44
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@14447be{/,null,STOPPED} added {ServletHandler@5eb5da12{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@5eb5da12{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a==org.apache.spark.ui.JettyUtils$$anon$3@32b2fed{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@5eb5da12{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a,POJO}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@67770b37{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-72906e[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-72906e==org.apache.spark.ui.JettyUtils$$anon$3@cee7e8d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-72906e=org.apache.spark.ui.JettyUtils$$anon$3-72906e==org.apache.spark.ui.JettyUtils$$anon$3@cee7e8d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@67770b37{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3896ms ServletHandler@67770b37{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-72906e==org.apache.spark.ui.JettyUtils$$anon$3@cee7e8d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3896ms org.apache.spark.ui.JettyUtils$$anon$3-72906e==org.apache.spark.ui.JettyUtils$$anon$3@cee7e8d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-72906e
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3896ms o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@5eb5da12{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a==org.apache.spark.ui.JettyUtils$$anon$3@32b2fed{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a=org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a==org.apache.spark.ui.JettyUtils$$anon$3@32b2fed{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@5eb5da12{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3901ms ServletHandler@5eb5da12{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a==org.apache.spark.ui.JettyUtils$$anon$3@32b2fed{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3902ms org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a==org.apache.spark.ui.JettyUtils$$anon$3@32b2fed{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4a50d04a
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3902ms o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@73c31181
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3d620a1{/,null,STOPPED} added {ServletHandler@4f486211{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@4f486211{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e==org.apache.spark.ui.JettyUtils$$anon$3@d1f43093{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@4f486211{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e,POJO}
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3a479fda
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@8a6631b{/,null,STOPPED} added {ServletHandler@472d0f4{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@472d0f4{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27==org.apache.spark.ui.JettyUtils$$anon$3@86c3fc9a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@472d0f4{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27,POJO}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4f486211{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e==org.apache.spark.ui.JettyUtils$$anon$3@d1f43093{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e=org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e==org.apache.spark.ui.JettyUtils$$anon$3@d1f43093{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@4f486211{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3907ms ServletHandler@4f486211{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e==org.apache.spark.ui.JettyUtils$$anon$3@d1f43093{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3908ms org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e==org.apache.spark.ui.JettyUtils$$anon$3@d1f43093{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-51e3d37e
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3908ms o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@472d0f4{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27==org.apache.spark.ui.JettyUtils$$anon$3@86c3fc9a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27=org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27==org.apache.spark.ui.JettyUtils$$anon$3@86c3fc9a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@472d0f4{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3913ms ServletHandler@472d0f4{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27==org.apache.spark.ui.JettyUtils$$anon$3@86c3fc9a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3913ms org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27==org.apache.spark.ui.JettyUtils$$anon$3@86c3fc9a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4bc59b27
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3914ms o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1da1380b
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1095d23a{/,null,STOPPED} added {ServletHandler@3332c7a5{STOPPED},MANAGED}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@3332c7a5{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-2accaec2==org.spark_project.jetty.servlet.DefaultServlet@201076f1{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ServletHandler@3332c7a5{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-2accaec2,POJO}
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@277b8fa4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4422dd48{/,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@5fa23c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5fbdc49b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@31ee2fdb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@abff8b7{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@6d1dcdff{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7be3a9ce{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@8a6631b{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@2f1d0bbc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@12968227{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@10a0fe30{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ac6c4f2{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@e5cbff2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@72725ee1{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@660f0c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4aedaf61{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@421056e5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cb5bb88{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@3968bc60{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6014a9ba{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@66273da0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2330e3e0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@63c12e52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@37b57b54{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@2643d762{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4339e0de{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@658255aa{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1d0a61c8{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@5d5c04f9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1c5c616f{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@17f2dd85{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@38af1bf6{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@22a736d7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3672276e{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@46c269e0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b0fc838{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@6d672bd4{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - static/sql->[{o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,STOPPED,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@5679e96b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7fcf294e{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@7add323c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7808f638{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@26dc9bd5{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4eed2acf{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@4ac86d6a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6cd166b8{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@4eba373c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c2e3264{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@14447be{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@68d7a2df{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@45bf6f39{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@d16be4f{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3d620a1{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@9596ce8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6bda1d19{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 17:22:17 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@fb6097b{STARTED} added {o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,STOPPED,@Spark}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,STARTING,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3332c7a5{STOPPED}
2022-02-10 17:22:17 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-2accaec2[EMBEDDED:null]
2022-02-10 17:22:17 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-2accaec2==org.spark_project.jetty.servlet.DefaultServlet@201076f1{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 17:22:17 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 17:22:17 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 17:22:17 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 17:22:17 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-2accaec2=org.spark_project.jetty.servlet.DefaultServlet-2accaec2==org.spark_project.jetty.servlet.DefaultServlet@201076f1{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 17:22:17 DEBUG AbstractHandler:94 - starting ServletHandler@3332c7a5{STARTING}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3920ms ServletHandler@3332c7a5{STARTED}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-2accaec2==org.spark_project.jetty.servlet.DefaultServlet@201076f1{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3921ms org.spark_project.jetty.servlet.DefaultServlet-2accaec2==org.spark_project.jetty.servlet.DefaultServlet@201076f1{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 17:22:17 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-2accaec2
2022-02-10 17:22:17 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-sql_2.11/2.4.8/spark-sql_2.11-2.4.8.jar!/org/apache/spark/sql/execution/ui/static
2022-02-10 17:22:17 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 17:22:17 DEBUG AbstractLifeCycle:191 - STARTED @3940ms o.s.j.s.ServletContextHandler@1095d23a{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 17:22:18 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2022-02-10 17:22:18 INFO  InMemoryFileIndex:54 - It took 31 ms to list leaf files for 1 paths.
2022-02-10 17:22:18 INFO  InMemoryFileIndex:54 - It took 6 ms to list leaf files for 2 paths.
2022-02-10 17:22:19 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#0]
 +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#4: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#4: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 17:22:19 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#0, None)) > 0)
 +- Project [value#0]                       +- Project [value#0]
    +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#5: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#5: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#6: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#6: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 17:22:19 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                    GlobalLimit 1
 +- LocalLimit 1                                  +- LocalLimit 1
    +- Filter (length(trim(value#0, None)) > 0)      +- Filter (length(trim(value#0, None)) > 0)
!      +- Project [value#0]                             +- Relation[value#0] text
!         +- Relation[value#0] text               
          
2022-02-10 17:22:20 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:20 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)
2022-02-10 17:22:20 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:20 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:20 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:20 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:20 INFO  CodeGenerator:54 - Code generated in 225.8062 ms
2022-02-10 17:22:20 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:20 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:20 INFO  CodeGenerator:54 - Code generated in 18.4918 ms
2022-02-10 17:22:20 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 221.9 KB, free 1970.2 MB)
2022-02-10 17:22:20 DEBUG BlockManager:58 - Put block broadcast_0 locally took  44 ms
2022-02-10 17:22:20 DEBUG BlockManager:58 - Putting block broadcast_0 without replication took  45 ms
2022-02-10 17:22:20 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1970.2 MB)
2022-02-10 17:22:20 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 17:22:20 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:22:20 DEBUG BlockManager:58 - Told master about block broadcast_0_piece0
2022-02-10 17:22:20 DEBUG BlockManager:58 - Put block broadcast_0_piece0 locally took  9 ms
2022-02-10 17:22:20 DEBUG BlockManager:58 - Putting block broadcast_0_piece0 without replication took  9 ms
2022-02-10 17:22:20 INFO  SparkContext:54 - Created broadcast 0 from load at UseCase1.java:19
2022-02-10 17:22:20 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:20 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:21 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Got job 0 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (load at UseCase1.java:19)
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - submitStage(ResultStage 0 (name=load at UseCase1.java:19;jobs=0))
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at UseCase1.java:19), which has no missing parents
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 0)
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 1970.2 MB)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_1 locally took  2 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_1 without replication took  3 ms
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1970.1 MB)
2022-02-10 17:22:21 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.4 MB)
2022-02-10 17:22:21 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_1_piece0 locally took  1 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_1_piece0 without replication took  1 ms
2022-02-10 17:22:21 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:21 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - Epoch for TaskSet 0.0: 0
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 17:22:21 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 17:22:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:21 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Getting local block broadcast_1
2022-02-10 17:22:21 DEBUG BlockManager:58 - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:21 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:21 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:21 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:21 INFO  CodeGenerator:54 - Code generated in 11.8464 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Getting local block broadcast_0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:21 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1343 bytes result sent to driver
2022-02-10 17:22:21 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 139 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2022-02-10 17:22:21 INFO  DAGScheduler:54 - ResultStage 0 (load at UseCase1.java:19) finished in 0.232 s
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - After removal of stage 0, remaining stages = 0
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Job 0 finished: load at UseCase1.java:19, took 0.282826 s
2022-02-10 17:22:21 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#8: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String
 +- Project [value#0]                                                                                                                                                     +- Project [value#0]
    +- Relation[value#0] text                                                                                                                                                +- Relation[value#0] text
          
2022-02-10 17:22:21 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String   DeserializeToObject value#0.toString, obj#8: java.lang.String
!+- Project [value#0]                                                            +- Relation[value#0] text
!   +- Relation[value#0] text                                                    
          
2022-02-10 17:22:21 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:21 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:21 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:21 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:21 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:21 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:21 INFO  CodeGenerator:54 - Code generated in 6.2121 ms
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 221.9 KB, free 1969.9 MB)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_2 locally took  5 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_2 without replication took  5 ms
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.9 MB)
2022-02-10 17:22:21 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 17:22:21 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Told master about block broadcast_2_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_2_piece0 locally took  4 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_2_piece0 without replication took  4 ms
2022-02-10 17:22:21 INFO  SparkContext:54 - Created broadcast 2 from load at UseCase1.java:19
2022-02-10 17:22:21 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:21 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:21 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Got job 1 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (load at UseCase1.java:19)
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - submitStage(ResultStage 1 (name=load at UseCase1.java:19;jobs=1))
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at UseCase1.java:19), which has no missing parents
2022-02-10 17:22:21 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 1)
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 13.9 KB, free 1969.9 MB)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_3 locally took  2 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_3 without replication took  3 ms
2022-02-10 17:22:21 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.9 MB)
2022-02-10 17:22:21 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:21 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Told master about block broadcast_3_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Put block broadcast_3_piece0 locally took  2 ms
2022-02-10 17:22:21 DEBUG BlockManager:58 - Putting block broadcast_3_piece0 without replication took  2 ms
2022-02-10 17:22:21 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:21 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - Epoch for TaskSet 1.0: 0
2022-02-10 17:22:21 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
2022-02-10 17:22:21 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 17:22:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:21 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Getting local block broadcast_3
2022-02-10 17:22:21 DEBUG BlockManager:58 - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:21 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:21 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:21 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:21 DEBUG BlockManager:58 - Getting local block broadcast_2
2022-02-10 17:22:21 DEBUG BlockManager:58 - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(21)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 21
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 21
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(29)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 29
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 29
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(20)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 20
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 20
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(24)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 24
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 24
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(6)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 6
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 6
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(1)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning broadcast 1
2022-02-10 17:22:21 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 1
2022-02-10 17:22:21 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 1
2022-02-10 17:22:21 DEBUG BlockManager:58 - Removing broadcast 1
2022-02-10 17:22:21 DEBUG BlockManager:58 - Removing block broadcast_1
2022-02-10 17:22:21 DEBUG MemoryStore:58 - Block broadcast_1 of size 9144 dropped from memory (free 2065590604)
2022-02-10 17:22:21 DEBUG BlockManager:58 - Removing block broadcast_1_piece0
2022-02-10 17:22:21 DEBUG MemoryStore:58 - Block broadcast_1_piece0 of size 4741 dropped from memory (free 2065595345)
2022-02-10 17:22:21 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.4 MB)
2022-02-10 17:22:21 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 17:22:21 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 17:22:21 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 1, response is 0
2022-02-10 17:22:21 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaned broadcast 1
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(8)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 8
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 8
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(27)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 27
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 27
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(16)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 16
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 16
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(14)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 14
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 14
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(17)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 17
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 17
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(19)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 19
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 19
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(13)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 13
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 13
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(23)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 23
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 23
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(25)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 25
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 25
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(26)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 26
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 26
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(9)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 9
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 9
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(22)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 22
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 22
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(11)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 11
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 11
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(7)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 7
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 7
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(18)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 18
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 18
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(10)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 10
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 10
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(30)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 30
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 30
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(15)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 15
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 15
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(12)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 12
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 12
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(28)
2022-02-10 17:22:21 DEBUG ContextCleaner:58 - Cleaning accumulator 28
2022-02-10 17:22:21 INFO  ContextCleaner:54 - Cleaned accumulator 28
2022-02-10 17:22:23 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:25 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1673 bytes result sent to driver
2022-02-10 17:22:25 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 17:22:25 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:25 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 4528 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:25 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2022-02-10 17:22:25 INFO  DAGScheduler:54 - ResultStage 1 (load at UseCase1.java:19) finished in 4.540 s
2022-02-10 17:22:25 DEBUG DAGScheduler:58 - After removal of stage 1, remaining stages = 0
2022-02-10 17:22:25 INFO  DAGScheduler:54 - Job 1 finished: load at UseCase1.java:19, took 4.543817 s
2022-02-10 17:22:25 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(order_id#10 as string), None), unresolvedalias(cast(order_date#11 as string), None), unresolvedalias(cast(order_customer_id#12 as string), None), unresolvedalias(cast(order_status#13 as string), None)]   Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]
 +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                                            +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 17:22:25 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]   Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]
 +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                          +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 17:22:25 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                 GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                               +- LocalLimit 21
!   +- Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]      +- Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, order_status#13]
       +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                                +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 17:22:25 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:25 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:25 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_id: int, order_date: timestamp, order_customer_id: int, order_status: string ... 2 more fields>
2022-02-10 17:22:25 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true))), obj#30: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(order_id#22.toString, order_date#25.toString, order_customer_id#23.toString, order_status#24.toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true)), obj#30: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [order_id#22, order_date#25, order_customer_id#23, order_status#24]                                                                                                                                                                                                                                                                                                                                                                 +- LocalRelation <empty>, [order_id#22, order_date#25, order_customer_id#23, order_status#24]
          
2022-02-10 17:22:26 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[4];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */   }
/* 063 */
/* 064 */
/* 065 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 066 */
/* 067 */     boolean isNull_2 = i.isNullAt(0);
/* 068 */     UTF8String value_2 = isNull_2 ?
/* 069 */     null : (i.getUTF8String(0));
/* 070 */     boolean isNull_1 = true;
/* 071 */     java.lang.String value_1 = null;
/* 072 */     if (!isNull_2) {
/* 073 */
/* 074 */       isNull_1 = false;
/* 075 */       if (!isNull_1) {
/* 076 */
/* 077 */         Object funcResult_0 = null;
/* 078 */         funcResult_0 = value_2.toString();
/* 079 */         value_1 = (java.lang.String) funcResult_0;
/* 080 */
/* 081 */       }
/* 082 */     }
/* 083 */     if (isNull_1) {
/* 084 */       values_0[0] = null;
/* 085 */     } else {
/* 086 */       values_0[0] = value_1;
/* 087 */     }
/* 088 */
/* 089 */     boolean isNull_4 = i.isNullAt(1);
/* 090 */     UTF8String value_4 = isNull_4 ?
/* 091 */     null : (i.getUTF8String(1));
/* 092 */     boolean isNull_3 = true;
/* 093 */     java.lang.String value_3 = null;
/* 094 */     if (!isNull_4) {
/* 095 */
/* 096 */       isNull_3 = false;
/* 097 */       if (!isNull_3) {
/* 098 */
/* 099 */         Object funcResult_1 = null;
/* 100 */         funcResult_1 = value_4.toString();
/* 101 */         value_3 = (java.lang.String) funcResult_1;
/* 102 */
/* 103 */       }
/* 104 */     }
/* 105 */     if (isNull_3) {
/* 106 */       values_0[1] = null;
/* 107 */     } else {
/* 108 */       values_0[1] = value_3;
/* 109 */     }
/* 110 */
/* 111 */     boolean isNull_6 = i.isNullAt(2);
/* 112 */     UTF8String value_6 = isNull_6 ?
/* 113 */     null : (i.getUTF8String(2));
/* 114 */     boolean isNull_5 = true;
/* 115 */     java.lang.String value_5 = null;
/* 116 */     if (!isNull_6) {
/* 117 */
/* 118 */       isNull_5 = false;
/* 119 */       if (!isNull_5) {
/* 120 */
/* 121 */         Object funcResult_2 = null;
/* 122 */         funcResult_2 = value_6.toString();
/* 123 */         value_5 = (java.lang.String) funcResult_2;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_5) {
/* 128 */       values_0[2] = null;
/* 129 */     } else {
/* 130 */       values_0[2] = value_5;
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

2022-02-10 17:22:26 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[4];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */   }
/* 063 */
/* 064 */
/* 065 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 066 */
/* 067 */     boolean isNull_2 = i.isNullAt(0);
/* 068 */     UTF8String value_2 = isNull_2 ?
/* 069 */     null : (i.getUTF8String(0));
/* 070 */     boolean isNull_1 = true;
/* 071 */     java.lang.String value_1 = null;
/* 072 */     if (!isNull_2) {
/* 073 */
/* 074 */       isNull_1 = false;
/* 075 */       if (!isNull_1) {
/* 076 */
/* 077 */         Object funcResult_0 = null;
/* 078 */         funcResult_0 = value_2.toString();
/* 079 */         value_1 = (java.lang.String) funcResult_0;
/* 080 */
/* 081 */       }
/* 082 */     }
/* 083 */     if (isNull_1) {
/* 084 */       values_0[0] = null;
/* 085 */     } else {
/* 086 */       values_0[0] = value_1;
/* 087 */     }
/* 088 */
/* 089 */     boolean isNull_4 = i.isNullAt(1);
/* 090 */     UTF8String value_4 = isNull_4 ?
/* 091 */     null : (i.getUTF8String(1));
/* 092 */     boolean isNull_3 = true;
/* 093 */     java.lang.String value_3 = null;
/* 094 */     if (!isNull_4) {
/* 095 */
/* 096 */       isNull_3 = false;
/* 097 */       if (!isNull_3) {
/* 098 */
/* 099 */         Object funcResult_1 = null;
/* 100 */         funcResult_1 = value_4.toString();
/* 101 */         value_3 = (java.lang.String) funcResult_1;
/* 102 */
/* 103 */       }
/* 104 */     }
/* 105 */     if (isNull_3) {
/* 106 */       values_0[1] = null;
/* 107 */     } else {
/* 108 */       values_0[1] = value_3;
/* 109 */     }
/* 110 */
/* 111 */     boolean isNull_6 = i.isNullAt(2);
/* 112 */     UTF8String value_6 = isNull_6 ?
/* 113 */     null : (i.getUTF8String(2));
/* 114 */     boolean isNull_5 = true;
/* 115 */     java.lang.String value_5 = null;
/* 116 */     if (!isNull_6) {
/* 117 */
/* 118 */       isNull_5 = false;
/* 119 */       if (!isNull_5) {
/* 120 */
/* 121 */         Object funcResult_2 = null;
/* 122 */         funcResult_2 = value_6.toString();
/* 123 */         value_5 = (java.lang.String) funcResult_2;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_5) {
/* 128 */       values_0[2] = null;
/* 129 */     } else {
/* 130 */       values_0[2] = value_5;
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

2022-02-10 17:22:26 INFO  CodeGenerator:54 - Code generated in 9.7531 ms
2022-02-10 17:22:26 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       long scan_value_1 = scan_isNull_1 ?
/* 038 */       -1L : (scan_row_0.getLong(1));
/* 039 */       boolean project_isNull_2 = scan_isNull_1;
/* 040 */       UTF8String project_value_2 = null;
/* 041 */       if (!scan_isNull_1) {
/* 042 */         project_value_2 = UTF8String.fromString(
/* 043 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[1] /* timeZone */)));
/* 044 */       }
/* 045 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 046 */       int scan_value_2 = scan_isNull_2 ?
/* 047 */       -1 : (scan_row_0.getInt(2));
/* 048 */       boolean project_isNull_4 = scan_isNull_2;
/* 049 */       UTF8String project_value_4 = null;
/* 050 */       if (!scan_isNull_2) {
/* 051 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));
/* 052 */       }
/* 053 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 054 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 055 */       null : (scan_row_0.getUTF8String(3));
/* 056 */       project_mutableStateArray_0[0].reset();
/* 057 */
/* 058 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 059 */
/* 060 */       if (project_isNull_0) {
/* 061 */         project_mutableStateArray_0[0].setNullAt(0);
/* 062 */       } else {
/* 063 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 064 */       }
/* 065 */
/* 066 */       if (project_isNull_2) {
/* 067 */         project_mutableStateArray_0[0].setNullAt(1);
/* 068 */       } else {
/* 069 */         project_mutableStateArray_0[0].write(1, project_value_2);
/* 070 */       }
/* 071 */
/* 072 */       if (project_isNull_4) {
/* 073 */         project_mutableStateArray_0[0].setNullAt(2);
/* 074 */       } else {
/* 075 */         project_mutableStateArray_0[0].write(2, project_value_4);
/* 076 */       }
/* 077 */
/* 078 */       if (scan_isNull_3) {
/* 079 */         project_mutableStateArray_0[0].setNullAt(3);
/* 080 */       } else {
/* 081 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 082 */       }
/* 083 */       append((project_mutableStateArray_0[0].getRow()));
/* 084 */       if (shouldStop()) return;
/* 085 */     }
/* 086 */   }
/* 087 */
/* 088 */ }

2022-02-10 17:22:26 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       long scan_value_1 = scan_isNull_1 ?
/* 038 */       -1L : (scan_row_0.getLong(1));
/* 039 */       boolean project_isNull_2 = scan_isNull_1;
/* 040 */       UTF8String project_value_2 = null;
/* 041 */       if (!scan_isNull_1) {
/* 042 */         project_value_2 = UTF8String.fromString(
/* 043 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[1] /* timeZone */)));
/* 044 */       }
/* 045 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 046 */       int scan_value_2 = scan_isNull_2 ?
/* 047 */       -1 : (scan_row_0.getInt(2));
/* 048 */       boolean project_isNull_4 = scan_isNull_2;
/* 049 */       UTF8String project_value_4 = null;
/* 050 */       if (!scan_isNull_2) {
/* 051 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));
/* 052 */       }
/* 053 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 054 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 055 */       null : (scan_row_0.getUTF8String(3));
/* 056 */       project_mutableStateArray_0[0].reset();
/* 057 */
/* 058 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 059 */
/* 060 */       if (project_isNull_0) {
/* 061 */         project_mutableStateArray_0[0].setNullAt(0);
/* 062 */       } else {
/* 063 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 064 */       }
/* 065 */
/* 066 */       if (project_isNull_2) {
/* 067 */         project_mutableStateArray_0[0].setNullAt(1);
/* 068 */       } else {
/* 069 */         project_mutableStateArray_0[0].write(1, project_value_2);
/* 070 */       }
/* 071 */
/* 072 */       if (project_isNull_4) {
/* 073 */         project_mutableStateArray_0[0].setNullAt(2);
/* 074 */       } else {
/* 075 */         project_mutableStateArray_0[0].write(2, project_value_4);
/* 076 */       }
/* 077 */
/* 078 */       if (scan_isNull_3) {
/* 079 */         project_mutableStateArray_0[0].setNullAt(3);
/* 080 */       } else {
/* 081 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 082 */       }
/* 083 */       append((project_mutableStateArray_0[0].getRow()));
/* 084 */       if (shouldStop()) return;
/* 085 */     }
/* 086 */   }
/* 087 */
/* 088 */ }

2022-02-10 17:22:26 INFO  CodeGenerator:54 - Code generated in 15.9388 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 221.8 KB, free 1969.7 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_4 locally took  7 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_4 without replication took  7 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.7 MB)
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_4_piece0 locally took  3 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_4_piece0 without replication took  3 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 4 from show at UseCase2.java:42
2022-02-10 17:22:26 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:26 INFO  SparkContext:54 - Starting job: show at UseCase2.java:42
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Got job 2 (show at UseCase2.java:42) with 1 output partitions
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (show at UseCase2.java:42)
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitStage(ResultStage 2 (name=show at UseCase2.java:42;jobs=2))
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[13] at show at UseCase2.java:42), which has no missing parents
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 2)
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 12.4 KB, free 1969.7 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_5 locally took  2 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_5 without replication took  2 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1969.7 MB)
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_5_piece0 locally took  2 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_5_piece0 without replication took  2 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at show at UseCase2.java:42) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:26 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Epoch for TaskSet 2.0: 0
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 2.0: NO_PREF, ANY
2022-02-10 17:22:26 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 17:22:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 17:22:26 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 2)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_5
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:26 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:26 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true],input[3, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */
/* 058 */     boolean isNull_3 = i.isNullAt(3);
/* 059 */     UTF8String value_3 = isNull_3 ?
/* 060 */     null : (i.getUTF8String(3));
/* 061 */     if (isNull_3) {
/* 062 */       mutableStateArray_0[0].setNullAt(3);
/* 063 */     } else {
/* 064 */       mutableStateArray_0[0].write(3, value_3);
/* 065 */     }
/* 066 */     return (mutableStateArray_0[0].getRow());
/* 067 */   }
/* 068 */
/* 069 */
/* 070 */ }

2022-02-10 17:22:26 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */
/* 058 */     boolean isNull_3 = i.isNullAt(3);
/* 059 */     UTF8String value_3 = isNull_3 ?
/* 060 */     null : (i.getUTF8String(3));
/* 061 */     if (isNull_3) {
/* 062 */       mutableStateArray_0[0].setNullAt(3);
/* 063 */     } else {
/* 064 */       mutableStateArray_0[0].write(3, value_3);
/* 065 */     }
/* 066 */     return (mutableStateArray_0[0].getRow());
/* 067 */   }
/* 068 */
/* 069 */
/* 070 */ }

2022-02-10 17:22:26 INFO  CodeGenerator:54 - Code generated in 9.6576 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_4
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:26 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 2). 1752 bytes result sent to driver
2022-02-10 17:22:26 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:26 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 46 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:26 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2022-02-10 17:22:26 INFO  DAGScheduler:54 - ResultStage 2 (show at UseCase2.java:42) finished in 0.054 s
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - After removal of stage 2, remaining stages = 0
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Job 2 finished: show at UseCase2.java:42, took 0.057250 s
2022-02-10 17:22:26 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:22:26 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 17:22:26 INFO  InMemoryFileIndex:54 - It took 2 ms to list leaf files for 2 paths.
2022-02-10 17:22:26 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#35
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#35]
 +- Relation[value#35] text                 +- Relation[value#35] text
          
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#39: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#39: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 17:22:26 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#35
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#35, None)) > 0)
 +- Project [value#35]                      +- Project [value#35]
    +- Relation[value#35] text                 +- Relation[value#35] text
          
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#40: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#40: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#41: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#41: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#35, None)) > 0)      +- Filter (length(trim(value#35, None)) > 0)
!      +- Project [value#35]                             +- Relation[value#35] text
!         +- Relation[value#35] text               
          
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#35, None)) > 0)
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:26 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:26 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:26 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_6 locally took  6 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_6 without replication took  6 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_6_piece0 locally took  2 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_6_piece0 without replication took  2 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 6 from load at UseCase1.java:19
2022-02-10 17:22:26 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:26 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Got job 3 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (load at UseCase1.java:19)
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitStage(ResultStage 3 (name=load at UseCase1.java:19;jobs=3))
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[17] at load at UseCase1.java:19), which has no missing parents
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 3)
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_7 locally took  3 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_7 without replication took  3 ms
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(79)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 79
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 79
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(78)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 78
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 78
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(82)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 82
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 82
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(77)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 77
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 77
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(76)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 76
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 76
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(75)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 75
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 75
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(81)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 81
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 81
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(83)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 83
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 83
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(66)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 66
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 66
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(64)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 64
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 64
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(88)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 88
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 88
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(5)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning broadcast 5
2022-02-10 17:22:26 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 5
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_7_piece0 locally took  4 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_7_piece0 without replication took  4 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:26 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 1 tasks
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Epoch for TaskSet 3.0: 0
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 3.0: NO_PREF, ANY
2022-02-10 17:22:26 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 17:22:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:26 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_7
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:26 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:26 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_6
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:26 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 1257 bytes result sent to driver
2022-02-10 17:22:26 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 5
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing broadcast 5
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing block broadcast_5
2022-02-10 17:22:26 DEBUG MemoryStore:58 - Block broadcast_5 of size 12712 dropped from memory (free 2065078225)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing block broadcast_5_piece0
2022-02-10 17:22:26 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:26 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2022-02-10 17:22:26 DEBUG MemoryStore:58 - Block broadcast_5_piece0 of size 6597 dropped from memory (free 2065084822)
2022-02-10 17:22:26 INFO  DAGScheduler:54 - ResultStage 3 (load at UseCase1.java:19) finished in 0.029 s
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - After removal of stage 3, remaining stages = 0
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Job 3 finished: load at UseCase1.java:19, took 0.031900 s
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 5, response is 0
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaned broadcast 5
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(65)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 65
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 65
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(89)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 89
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 89
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(4)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning broadcast 4
2022-02-10 17:22:26 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 4
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 4
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing broadcast 4
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing block broadcast_4_piece0
2022-02-10 17:22:26 DEBUG MemoryStore:58 - Block broadcast_4_piece0 of size 21159 dropped from memory (free 2065105981)
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#43: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#43: java.lang.String
 +- Project [value#35]                                                                                                                                                     +- Project [value#35]
    +- Relation[value#35] text                                                                                                                                                +- Relation[value#35] text
          
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Removing block broadcast_4
2022-02-10 17:22:26 DEBUG MemoryStore:58 - Block broadcast_4 of size 227072 dropped from memory (free 2065333053)
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 4, response is 0
2022-02-10 17:22:26 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaned broadcast 4
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(61)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 61
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 61
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(70)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 70
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 70
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(68)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 68
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 68
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(85)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 85
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 85
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(84)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 84
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 84
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(67)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 67
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 67
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(90)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 90
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 90
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(73)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 73
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 73
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(87)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 87
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 87
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(86)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 86
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 86
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(69)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 69
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 69
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(62)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 62
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 62
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(80)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 80
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 80
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(63)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 63
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 63
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(74)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 74
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 74
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(72)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 72
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 72
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(71)
2022-02-10 17:22:26 DEBUG ContextCleaner:58 - Cleaning accumulator 71
2022-02-10 17:22:26 INFO  ContextCleaner:54 - Cleaned accumulator 71
2022-02-10 17:22:26 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#35 as string).toString, obj#43: java.lang.String   DeserializeToObject value#35.toString, obj#43: java.lang.String
!+- Project [value#35]                                                             +- Relation[value#35] text
!   +- Relation[value#35] text                                                     
          
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:26 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:26 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:26 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_8 locally took  4 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_8 without replication took  4 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_8_piece0 locally took  4 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_8_piece0 without replication took  5 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 8 from load at UseCase1.java:19
2022-02-10 17:22:26 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:26 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:26 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Got job 4 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (load at UseCase1.java:19)
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitStage(ResultStage 4 (name=load at UseCase1.java:19;jobs=4))
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting ResultStage 4 (MapPartitionsRDD[23] at load at UseCase1.java:19), which has no missing parents
2022-02-10 17:22:26 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 4)
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 13.9 KB, free 1969.4 MB)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_9 locally took  2 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_9 without replication took  2 ms
2022-02-10 17:22:26 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.4 MB)
2022-02-10 17:22:26 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:26 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 17:22:26 DEBUG BlockManager:58 - Put block broadcast_9_piece0 locally took  3 ms
2022-02-10 17:22:26 DEBUG BlockManager:58 - Putting block broadcast_9_piece0 without replication took  3 ms
2022-02-10 17:22:26 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:26 INFO  TaskSchedulerImpl:54 - Adding task set 4.0 with 1 tasks
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Epoch for TaskSet 4.0: 0
2022-02-10 17:22:26 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
2022-02-10 17:22:26 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 17:22:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:26 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 4)
2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_9
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_9 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:26 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:26 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:26 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:26 DEBUG BlockManager:58 - Getting local block broadcast_8
2022-02-10 17:22:26 DEBUG BlockManager:58 - Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(114)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 114
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 114
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(106)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 106
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 106
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(101)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 101
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 101
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(112)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 112
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 112
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(113)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 113
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 113
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(95)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 95
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 95
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(94)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 94
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 94
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(97)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 97
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 97
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(105)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 105
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 105
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(104)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 104
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 104
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(111)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 111
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 111
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(98)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 98
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 98
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(118)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 118
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 118
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(110)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 110
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 110
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(93)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 93
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 93
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(121)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 121
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 121
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(116)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 116
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 116
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(7)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning broadcast 7
2022-02-10 17:22:28 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 7
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 7
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing broadcast 7
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing block broadcast_7
2022-02-10 17:22:28 DEBUG MemoryStore:58 - Block broadcast_7 of size 9144 dropped from memory (free 2065071796)
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing block broadcast_7_piece0
2022-02-10 17:22:28 DEBUG MemoryStore:58 - Block broadcast_7_piece0 of size 4742 dropped from memory (free 2065076538)
2022-02-10 17:22:28 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:28 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 17:22:28 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 7, response is 0
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaned broadcast 7
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(109)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 109
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 109
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(100)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 100
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 100
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(91)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 91
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 91
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(92)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 92
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 92
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(108)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 108
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 108
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(115)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 115
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 115
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(117)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 117
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 117
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(6)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning broadcast 6
2022-02-10 17:22:28 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 6
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 6
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing broadcast 6
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing block broadcast_6_piece0
2022-02-10 17:22:28 DEBUG MemoryStore:58 - Block broadcast_6_piece0 of size 21174 dropped from memory (free 2065097712)
2022-02-10 17:22:28 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:28 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 17:22:28 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 17:22:28 DEBUG BlockManager:58 - Removing block broadcast_6
2022-02-10 17:22:28 DEBUG MemoryStore:58 - Block broadcast_6 of size 227232 dropped from memory (free 2065324944)
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 6, response is 0
2022-02-10 17:22:28 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaned broadcast 6
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(99)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 99
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 99
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(103)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 103
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 103
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(119)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 119
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 119
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(102)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 102
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 102
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(96)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 96
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 96
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(120)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 120
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 120
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(107)
2022-02-10 17:22:28 DEBUG ContextCleaner:58 - Cleaning accumulator 107
2022-02-10 17:22:28 INFO  ContextCleaner:54 - Cleaned accumulator 107
2022-02-10 17:22:28 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:30 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 4). 1587 bytes result sent to driver
2022-02-10 17:22:30 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:30 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 4209 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:30 INFO  TaskSchedulerImpl:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2022-02-10 17:22:30 INFO  DAGScheduler:54 - ResultStage 4 (load at UseCase1.java:19) finished in 4.217 s
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - After removal of stage 4, remaining stages = 0
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Job 4 finished: load at UseCase1.java:19, took 4.219854 s
2022-02-10 17:22:30 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:22:30 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 17:22:30 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 17:22:30 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#53
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#53]
 +- Relation[value#53] text                 +- Relation[value#53] text
          
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#57: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#57: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 17:22:30 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#53
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#53, None)) > 0)
 +- Project [value#53]                      +- Project [value#53]
    +- Relation[value#53] text                 +- Relation[value#53] text
          
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#58: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#58: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#59: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#59: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#53, None)) > 0)      +- Filter (length(trim(value#53, None)) > 0)
!      +- Project [value#53]                             +- Relation[value#53] text
!         +- Relation[value#53] text               
          
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#53, None)) > 0)
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:30 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:30 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:30 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_10 locally took  4 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_10 without replication took  4 ms
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 17:22:30 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:30 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_10_piece0 locally took  3 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_10_piece0 without replication took  3 ms
2022-02-10 17:22:30 INFO  SparkContext:54 - Created broadcast 10 from load at UseCase1.java:25
2022-02-10 17:22:30 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:30 INFO  SparkContext:54 - Starting job: load at UseCase1.java:25
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Got job 5 (load at UseCase1.java:25) with 1 output partitions
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (load at UseCase1.java:25)
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - submitStage(ResultStage 5 (name=load at UseCase1.java:25;jobs=5))
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[27] at load at UseCase1.java:25), which has no missing parents
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 5)
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_11 locally took  2 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_11 without replication took  2 ms
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 17:22:30 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:30 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_11_piece0 locally took  3 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_11_piece0 without replication took  3 ms
2022-02-10 17:22:30 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[27] at load at UseCase1.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:30 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 1 tasks
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - Epoch for TaskSet 5.0: 0
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 5.0: NO_PREF, ANY
2022-02-10 17:22:30 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 17:22:30 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:30 INFO  Executor:54 - Running task 0.0 in stage 5.0 (TID 5)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Getting local block broadcast_11
2022-02-10 17:22:30 DEBUG BlockManager:58 - Level for block broadcast_11 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:30 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:30 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:30 DEBUG BlockManager:58 - Getting local block broadcast_10
2022-02-10 17:22:30 DEBUG BlockManager:58 - Level for block broadcast_10 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:30 INFO  Executor:54 - Finished task 0.0 in stage 5.0 (TID 5). 1248 bytes result sent to driver
2022-02-10 17:22:30 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:30 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 8 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:30 INFO  TaskSchedulerImpl:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2022-02-10 17:22:30 INFO  DAGScheduler:54 - ResultStage 5 (load at UseCase1.java:25) finished in 0.018 s
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - After removal of stage 5, remaining stages = 0
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Job 5 finished: load at UseCase1.java:25, took 0.022263 s
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#61: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#61: java.lang.String
 +- Project [value#53]                                                                                                                                                     +- Project [value#53]
    +- Relation[value#53] text                                                                                                                                                +- Relation[value#53] text
          
2022-02-10 17:22:30 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#53 as string).toString, obj#61: java.lang.String   DeserializeToObject value#53.toString, obj#61: java.lang.String
!+- Project [value#53]                                                             +- Relation[value#53] text
!   +- Relation[value#53] text                                                     
          
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:30 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:30 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:30 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_12 locally took  3 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_12 without replication took  3 ms
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 17:22:30 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:30 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_12_piece0 locally took  3 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_12_piece0 without replication took  3 ms
2022-02-10 17:22:30 INFO  SparkContext:54 - Created broadcast 12 from load at UseCase1.java:25
2022-02-10 17:22:30 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:30 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:30 INFO  SparkContext:54 - Starting job: load at UseCase1.java:25
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Got job 6 (load at UseCase1.java:25) with 1 output partitions
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (load at UseCase1.java:25)
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - submitStage(ResultStage 6 (name=load at UseCase1.java:25;jobs=6))
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Submitting ResultStage 6 (MapPartitionsRDD[33] at load at UseCase1.java:25), which has no missing parents
2022-02-10 17:22:30 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 6)
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 14.0 KB, free 1969.1 MB)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_13 locally took  2 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_13 without replication took  2 ms
2022-02-10 17:22:30 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.1 MB)
2022-02-10 17:22:30 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:30 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 17:22:30 DEBUG BlockManager:58 - Put block broadcast_13_piece0 locally took  2 ms
2022-02-10 17:22:30 DEBUG BlockManager:58 - Putting block broadcast_13_piece0 without replication took  2 ms
2022-02-10 17:22:30 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:30 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at load at UseCase1.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:30 INFO  TaskSchedulerImpl:54 - Adding task set 6.0 with 1 tasks
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - Epoch for TaskSet 6.0: 0
2022-02-10 17:22:30 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 6.0: NO_PREF, ANY
2022-02-10 17:22:30 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 17:22:30 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:30 INFO  Executor:54 - Running task 0.0 in stage 6.0 (TID 6)
2022-02-10 17:22:30 DEBUG BlockManager:58 - Getting local block broadcast_13
2022-02-10 17:22:30 DEBUG BlockManager:58 - Level for block broadcast_13 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:30 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:30 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:30 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:30 DEBUG BlockManager:58 - Getting local block broadcast_12
2022-02-10 17:22:30 DEBUG BlockManager:58 - Level for block broadcast_12 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:30 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:31 INFO  Executor:54 - Finished task 0.0 in stage 6.0 (TID 6). 1516 bytes result sent to driver
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 487 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2022-02-10 17:22:31 INFO  DAGScheduler:54 - ResultStage 6 (load at UseCase1.java:25) finished in 0.495 s
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - After removal of stage 6, remaining stages = 0
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Job 6 finished: load at UseCase1.java:25, took 0.497754 s
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(customer_id#63 as string), None), unresolvedalias(cast(customer_fname#64 as string), None), unresolvedalias(cast(customer_lname#65 as string), None), unresolvedalias(cast(customer_email#66 as string), None), unresolvedalias(cast(customer_password#67 as string), None), unresolvedalias(cast(customer_street#68 as string), None), unresolvedalias(cast(customer_city#69 as string), None), unresolvedalias(cast(customer_state#70 as string), None), unresolvedalias(cast(customer_zipcode#71 as string), None)]   Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
 +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]   Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
 +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                   +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             +- LocalLimit 21
!   +- Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]      +- Project [cast(customer_id#63 as string) AS customer_id#90, customer_fname#64, customer_lname#65, customer_email#66, customer_password#67, customer_street#68, customer_city#69, customer_state#70, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
       +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                         +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int, customer_fname: string, customer_lname: string, customer_email: string, customer_password: string ... 7 more fields>
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true))), obj#108: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(customer_id#90.toString, customer_fname#91.toString, customer_lname#92.toString, customer_email#93.toString, customer_password#94.toString, customer_street#95.toString, customer_city#96.toString, customer_state#97.toString, customer_zipcode#98.toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)), obj#108: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [customer_id#90, customer_fname#91, customer_lname#92, customer_email#93, customer_password#94, customer_street#95, customer_city#96, customer_state#97, customer_zipcode#98]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- LocalRelation <empty>, [customer_id#90, customer_fname#91, customer_lname#92, customer_email#93, customer_password#94, customer_street#95, customer_city#96, customer_state#97, customer_zipcode#98]
          
2022-02-10 17:22:31 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, true].toString, input[7, string, true].toString, input[8, string, true].toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 17:22:31 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 17:22:31 INFO  CodeGenerator:54 - Code generated in 19.5614 ms
2022-02-10 17:22:31 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       UTF8String scan_value_1 = scan_isNull_1 ?
/* 038 */       null : (scan_row_0.getUTF8String(1));
/* 039 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 040 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 041 */       null : (scan_row_0.getUTF8String(2));
/* 042 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 043 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 044 */       null : (scan_row_0.getUTF8String(3));
/* 045 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 046 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 047 */       null : (scan_row_0.getUTF8String(4));
/* 048 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 049 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 050 */       null : (scan_row_0.getUTF8String(5));
/* 051 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 052 */       UTF8String scan_value_6 = scan_isNull_6 ?
/* 053 */       null : (scan_row_0.getUTF8String(6));
/* 054 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 055 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 056 */       null : (scan_row_0.getUTF8String(7));
/* 057 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 058 */       int scan_value_8 = scan_isNull_8 ?
/* 059 */       -1 : (scan_row_0.getInt(8));
/* 060 */       boolean project_isNull_9 = scan_isNull_8;
/* 061 */       UTF8String project_value_9 = null;
/* 062 */       if (!scan_isNull_8) {
/* 063 */         project_value_9 = UTF8String.fromString(String.valueOf(scan_value_8));
/* 064 */       }
/* 065 */       project_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       if (project_isNull_0) {
/* 070 */         project_mutableStateArray_0[0].setNullAt(0);
/* 071 */       } else {
/* 072 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 073 */       }
/* 074 */
/* 075 */       if (scan_isNull_1) {
/* 076 */         project_mutableStateArray_0[0].setNullAt(1);
/* 077 */       } else {
/* 078 */         project_mutableStateArray_0[0].write(1, scan_value_1);
/* 079 */       }
/* 080 */
/* 081 */       if (scan_isNull_2) {
/* 082 */         project_mutableStateArray_0[0].setNullAt(2);
/* 083 */       } else {
/* 084 */         project_mutableStateArray_0[0].write(2, scan_value_2);
/* 085 */       }
/* 086 */
/* 087 */       if (scan_isNull_3) {
/* 088 */         project_mutableStateArray_0[0].setNullAt(3);
/* 089 */       } else {
/* 090 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 091 */       }
/* 092 */
/* 093 */       if (scan_isNull_4) {
/* 094 */         project_mutableStateArray_0[0].setNullAt(4);
/* 095 */       } else {
/* 096 */         project_mutableStateArray_0[0].write(4, scan_value_4);
/* 097 */       }
/* 098 */
/* 099 */       if (scan_isNull_5) {
/* 100 */         project_mutableStateArray_0[0].setNullAt(5);
/* 101 */       } else {
/* 102 */         project_mutableStateArray_0[0].write(5, scan_value_5);
/* 103 */       }
/* 104 */
/* 105 */       if (scan_isNull_6) {
/* 106 */         project_mutableStateArray_0[0].setNullAt(6);
/* 107 */       } else {
/* 108 */         project_mutableStateArray_0[0].write(6, scan_value_6);
/* 109 */       }
/* 110 */
/* 111 */       if (scan_isNull_7) {
/* 112 */         project_mutableStateArray_0[0].setNullAt(7);
/* 113 */       } else {
/* 114 */         project_mutableStateArray_0[0].write(7, scan_value_7);
/* 115 */       }
/* 116 */
/* 117 */       if (project_isNull_9) {
/* 118 */         project_mutableStateArray_0[0].setNullAt(8);
/* 119 */       } else {
/* 120 */         project_mutableStateArray_0[0].write(8, project_value_9);
/* 121 */       }
/* 122 */       append((project_mutableStateArray_0[0].getRow()));
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */   }
/* 126 */
/* 127 */ }

2022-02-10 17:22:31 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       UTF8String scan_value_1 = scan_isNull_1 ?
/* 038 */       null : (scan_row_0.getUTF8String(1));
/* 039 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 040 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 041 */       null : (scan_row_0.getUTF8String(2));
/* 042 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 043 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 044 */       null : (scan_row_0.getUTF8String(3));
/* 045 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 046 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 047 */       null : (scan_row_0.getUTF8String(4));
/* 048 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 049 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 050 */       null : (scan_row_0.getUTF8String(5));
/* 051 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 052 */       UTF8String scan_value_6 = scan_isNull_6 ?
/* 053 */       null : (scan_row_0.getUTF8String(6));
/* 054 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 055 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 056 */       null : (scan_row_0.getUTF8String(7));
/* 057 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 058 */       int scan_value_8 = scan_isNull_8 ?
/* 059 */       -1 : (scan_row_0.getInt(8));
/* 060 */       boolean project_isNull_9 = scan_isNull_8;
/* 061 */       UTF8String project_value_9 = null;
/* 062 */       if (!scan_isNull_8) {
/* 063 */         project_value_9 = UTF8String.fromString(String.valueOf(scan_value_8));
/* 064 */       }
/* 065 */       project_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       if (project_isNull_0) {
/* 070 */         project_mutableStateArray_0[0].setNullAt(0);
/* 071 */       } else {
/* 072 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 073 */       }
/* 074 */
/* 075 */       if (scan_isNull_1) {
/* 076 */         project_mutableStateArray_0[0].setNullAt(1);
/* 077 */       } else {
/* 078 */         project_mutableStateArray_0[0].write(1, scan_value_1);
/* 079 */       }
/* 080 */
/* 081 */       if (scan_isNull_2) {
/* 082 */         project_mutableStateArray_0[0].setNullAt(2);
/* 083 */       } else {
/* 084 */         project_mutableStateArray_0[0].write(2, scan_value_2);
/* 085 */       }
/* 086 */
/* 087 */       if (scan_isNull_3) {
/* 088 */         project_mutableStateArray_0[0].setNullAt(3);
/* 089 */       } else {
/* 090 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 091 */       }
/* 092 */
/* 093 */       if (scan_isNull_4) {
/* 094 */         project_mutableStateArray_0[0].setNullAt(4);
/* 095 */       } else {
/* 096 */         project_mutableStateArray_0[0].write(4, scan_value_4);
/* 097 */       }
/* 098 */
/* 099 */       if (scan_isNull_5) {
/* 100 */         project_mutableStateArray_0[0].setNullAt(5);
/* 101 */       } else {
/* 102 */         project_mutableStateArray_0[0].write(5, scan_value_5);
/* 103 */       }
/* 104 */
/* 105 */       if (scan_isNull_6) {
/* 106 */         project_mutableStateArray_0[0].setNullAt(6);
/* 107 */       } else {
/* 108 */         project_mutableStateArray_0[0].write(6, scan_value_6);
/* 109 */       }
/* 110 */
/* 111 */       if (scan_isNull_7) {
/* 112 */         project_mutableStateArray_0[0].setNullAt(7);
/* 113 */       } else {
/* 114 */         project_mutableStateArray_0[0].write(7, scan_value_7);
/* 115 */       }
/* 116 */
/* 117 */       if (project_isNull_9) {
/* 118 */         project_mutableStateArray_0[0].setNullAt(8);
/* 119 */       } else {
/* 120 */         project_mutableStateArray_0[0].write(8, project_value_9);
/* 121 */       }
/* 122 */       append((project_mutableStateArray_0[0].getRow()));
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */   }
/* 126 */
/* 127 */ }

2022-02-10 17:22:31 INFO  CodeGenerator:54 - Code generated in 14.8299 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 221.8 KB, free 1968.9 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_14 locally took  3 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_14 without replication took  3 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_14_piece0 locally took  2 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_14_piece0 without replication took  2 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 14 from show at UseCase2.java:44
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:31 INFO  SparkContext:54 - Starting job: show at UseCase2.java:44
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Got job 7 (show at UseCase2.java:44) with 1 output partitions
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (show at UseCase2.java:44)
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitStage(ResultStage 7 (name=show at UseCase2.java:44;jobs=7))
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting ResultStage 7 (MapPartitionsRDD[37] at show at UseCase2.java:44), which has no missing parents
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 7)
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 13.9 KB, free 1968.9 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_15 locally took  1 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_15 without replication took  1 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1968.9 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 6.7 KB, free: 1970.2 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_15_piece0 locally took  2 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_15_piece0 without replication took  2 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[37] at show at UseCase2.java:44) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Adding task set 7.0 with 1 tasks
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Epoch for TaskSet 7.0: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 7.0: NO_PREF, ANY
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 17:22:31 INFO  Executor:54 - Running task 0.0 in stage 7.0 (TID 7)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_15
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_15 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:31 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 17:22:31 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 17:22:31 INFO  CodeGenerator:54 - Code generated in 14.6479 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_14
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_14 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 INFO  Executor:54 - Finished task 0.0 in stage 7.0 (TID 7). 3041 bytes result sent to driver
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 29 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2022-02-10 17:22:31 INFO  DAGScheduler:54 - ResultStage 7 (show at UseCase2.java:44) finished in 0.038 s
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - After removal of stage 7, remaining stages = 0
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Job 7 finished: show at UseCase2.java:44, took 0.041369 s
2022-02-10 17:22:31 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:22:31 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 17:22:31 INFO  InMemoryFileIndex:54 - It took 2 ms to list leaf files for 2 paths.
2022-02-10 17:22:31 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#118
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#118]
 +- Relation[value#118] text                +- Relation[value#118] text
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#122: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#122: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 17:22:31 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#118
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#118, None)) > 0)
 +- Project [value#118]                     +- Project [value#118]
    +- Relation[value#118] text                +- Relation[value#118] text
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#123: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#123: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#124: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#124: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#118, None)) > 0)      +- Filter (length(trim(value#118, None)) > 0)
!      +- Project [value#118]                             +- Relation[value#118] text
!         +- Relation[value#118] text               
          
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#118, None)) > 0)
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:31 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:31 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 221.9 KB, free 1968.7 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_16 locally took  7 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_16 without replication took  8 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.6 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_16_piece0 locally took  2 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_16_piece0 without replication took  2 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 16 from load at UseCase1.java:25
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:31 INFO  SparkContext:54 - Starting job: load at UseCase1.java:25
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Got job 8 (load at UseCase1.java:25) with 1 output partitions
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (load at UseCase1.java:25)
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitStage(ResultStage 8 (name=load at UseCase1.java:25;jobs=8))
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting ResultStage 8 (MapPartitionsRDD[41] at load at UseCase1.java:25), which has no missing parents
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 8)
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 8.9 KB, free 1968.6 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_17 locally took  1 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_17 without replication took  1 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.6 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_17_piece0 locally took  3 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_17_piece0 without replication took  3 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at load at UseCase1.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Adding task set 8.0 with 1 tasks
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Epoch for TaskSet 8.0: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 8.0: NO_PREF, ANY
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:31 INFO  Executor:54 - Running task 0.0 in stage 8.0 (TID 8)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_17
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_17 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:31 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_16
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_16 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 INFO  Executor:54 - Finished task 0.0 in stage 8.0 (TID 8). 1248 bytes result sent to driver
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 17 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2022-02-10 17:22:31 INFO  DAGScheduler:54 - ResultStage 8 (load at UseCase1.java:25) finished in 0.028 s
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - After removal of stage 8, remaining stages = 0
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Job 8 finished: load at UseCase1.java:25, took 0.030394 s
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#126: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#126: java.lang.String
 +- Project [value#118]                                                                                                                                                     +- Project [value#118]
    +- Relation[value#118] text                                                                                                                                                +- Relation[value#118] text
          
2022-02-10 17:22:31 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#118 as string).toString, obj#126: java.lang.String   DeserializeToObject value#118.toString, obj#126: java.lang.String
!+- Project [value#118]                                                              +- Relation[value#118] text
!   +- Relation[value#118] text                                                      
          
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:31 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:31 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 221.9 KB, free 1968.4 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_18 locally took  4 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_18 without replication took  5 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.4 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_18_piece0 locally took  3 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_18_piece0 without replication took  3 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 18 from load at UseCase1.java:25
2022-02-10 17:22:31 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:31 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:31 INFO  SparkContext:54 - Starting job: load at UseCase1.java:25
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Got job 9 (load at UseCase1.java:25) with 1 output partitions
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (load at UseCase1.java:25)
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitStage(ResultStage 9 (name=load at UseCase1.java:25;jobs=9))
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting ResultStage 9 (MapPartitionsRDD[47] at load at UseCase1.java:25), which has no missing parents
2022-02-10 17:22:31 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 9)
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 14.0 KB, free 1968.4 MB)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_19 locally took  2 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_19 without replication took  2 ms
2022-02-10 17:22:31 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1968.4 MB)
2022-02-10 17:22:31 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 17:22:31 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 17:22:31 DEBUG BlockManager:58 - Put block broadcast_19_piece0 locally took  2 ms
2022-02-10 17:22:31 DEBUG BlockManager:58 - Putting block broadcast_19_piece0 without replication took  2 ms
2022-02-10 17:22:31 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[47] at load at UseCase1.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:31 INFO  TaskSchedulerImpl:54 - Adding task set 9.0 with 1 tasks
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Epoch for TaskSet 9.0: 0
2022-02-10 17:22:31 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 9.0: NO_PREF, ANY
2022-02-10 17:22:31 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 17:22:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:31 INFO  Executor:54 - Running task 0.0 in stage 9.0 (TID 9)
2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_19
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_19 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:31 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:31 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:31 DEBUG BlockManager:58 - Getting local block broadcast_18
2022-02-10 17:22:31 DEBUG BlockManager:58 - Level for block broadcast_18 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:31 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:32 INFO  Executor:54 - Finished task 0.0 in stage 9.0 (TID 9). 1516 bytes result sent to driver
2022-02-10 17:22:32 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 447 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:32 INFO  TaskSchedulerImpl:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2022-02-10 17:22:32 INFO  DAGScheduler:54 - ResultStage 9 (load at UseCase1.java:25) finished in 0.457 s
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - After removal of stage 9, remaining stages = 0
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Job 9 finished: load at UseCase1.java:25, took 0.459730 s
2022-02-10 17:22:32 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:22:32 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 17:22:32 INFO  InMemoryFileIndex:54 - It took 2 ms to list leaf files for 2 paths.
2022-02-10 17:22:32 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#146
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#146]
 +- Relation[value#146] text                +- Relation[value#146] text
          
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#150: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#150: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 17:22:32 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#146
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#146, None)) > 0)
 +- Project [value#146]                     +- Project [value#146]
    +- Relation[value#146] text                +- Relation[value#146] text
          
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#151: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#151: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#152: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#152: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#146, None)) > 0)      +- Filter (length(trim(value#146, None)) > 0)
!      +- Project [value#146]                             +- Relation[value#146] text
!         +- Relation[value#146] text               
          
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(247)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 247
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 247
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(19)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 19
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 19
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#146, None)) > 0)
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:32 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:32 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:32 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 221.9 KB, free 1968.2 MB)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_20 locally took  3 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_20 without replication took  3 ms
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 19
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 19
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_19_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_19_piece0 of size 7781 dropped from memory (free 2063770531)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_19
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_19 of size 14368 dropped from memory (free 2063784899)
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 19, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 19
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(123)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 123
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 123
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(214)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 214
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 214
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(9)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 9
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 9
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 9
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 9
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_9_piece0
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.2 MB)
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_9_piece0 of size 7739 dropped from memory (free 2063771464)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_9
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_20_piece0 locally took  3 ms
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_9 of size 14256 dropped from memory (free 2063785720)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_20_piece0 without replication took  4 ms
2022-02-10 17:22:32 INFO  SparkContext:54 - Created broadcast 20 from load at UseCase2.java:19
2022-02-10 17:22:32 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 9, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 9
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(155)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 155
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 155
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(295)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 295
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 295
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(178)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 178
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 178
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(260)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 260
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 260
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(257)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 257
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 257
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(208)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 208
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 208
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(217)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 217
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 217
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(278)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 278
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 278
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(203)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 203
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 203
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(205)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 205
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 205
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(12)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 12
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 12
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 12
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 12
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_12_piece0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_12_piece0 of size 21174 dropped from memory (free 2063806894)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_12
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_12 of size 227232 dropped from memory (free 2064034126)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 12, response is 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 12
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(133)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 133
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 133
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(152)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 152
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 152
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(238)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 238
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 238
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(137)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 137
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 137
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(131)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 131
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 131
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(17)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 17
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 17
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 17
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 17
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_17
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_17 of size 9144 dropped from memory (free 2064043270)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_17_piece0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_17_piece0 of size 4742 dropped from memory (free 2064048012)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 17, response is 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 17
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(218)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 218
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 218
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(180)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 180
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 180
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(192)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 192
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 192
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(221)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 221
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 221
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(128)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 128
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 128
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(14)
2022-02-10 17:22:32 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 14
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 14
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Got job 10 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (load at UseCase2.java:19)
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 14
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 14
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_14
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - submitStage(ResultStage 10 (name=load at UseCase2.java:19;jobs=10))
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_14 of size 227072 dropped from memory (free 2064275084)
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Submitting ResultStage 10 (MapPartitionsRDD[51] at load at UseCase2.java:19), which has no missing parents
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_14_piece0
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 10)
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_14_piece0 of size 21159 dropped from memory (free 2064296243)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 8.9 KB, free 1968.7 MB)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_21 locally took  0 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_21 without replication took  0 ms
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 14, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.7 MB)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 14
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(290)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 290
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 290
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(153)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 153
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 153
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(156)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 156
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 156
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(204)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 204
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 204
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(250)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 250
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 250
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(197)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 197
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 197
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(273)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 273
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 273
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(229)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 229
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 229
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(299)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 299
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 299
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(184)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 184
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 184
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(162)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 162
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 162
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(173)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 173
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 173
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(146)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 146
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 146
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(157)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 157
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 157
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(126)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 126
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 126
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(219)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 219
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 219
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(223)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 223
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 223
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(244)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 244
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 244
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(210)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 210
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 210
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(143)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 143
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 143
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(261)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 261
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 261
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(228)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 228
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 228
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(242)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 242
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 242
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(245)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_21_piece0 locally took  4 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_21_piece0 without replication took  4 ms
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 245
2022-02-10 17:22:32 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 245
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(256)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 256
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 256
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(211)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 211
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 211
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(220)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 220
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 220
2022-02-10 17:22:32 INFO  TaskSchedulerImpl:54 - Adding task set 10.0 with 1 tasks
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(191)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 191
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 191
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(266)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 266
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 266
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(254)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 254
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - Epoch for TaskSet 10.0: 0
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 254
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 10.0: NO_PREF, ANY
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(264)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 264
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 264
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(201)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 201
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 201
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(170)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 170
2022-02-10 17:22:32 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 170
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(169)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 169
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 169
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(277)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 277
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 277
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(225)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 225
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 225
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(302)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 302
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 302
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(148)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 148
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 148
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(233)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 233
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 233
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(267)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 267
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 267
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(130)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 130
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 130
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(255)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 255
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 255
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(230)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 230
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 230
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(161)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 161
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 161
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(236)
2022-02-10 17:22:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 236
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 236
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(183)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 183
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 183
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(272)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 272
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 272
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(293)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 293
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 293
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(285)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 285
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 285
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(301)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 301
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 301
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(190)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 190
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 190
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(292)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 292
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 292
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(253)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 253
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 253
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(249)
2022-02-10 17:22:32 INFO  Executor:54 - Running task 0.0 in stage 10.0 (TID 10)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 249
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 249
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(135)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 135
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 135
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(185)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 185
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 185
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(259)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 259
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 259
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(187)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 187
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 187
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(280)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 280
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 280
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(303)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 303
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 303
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(193)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 193
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 193
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(165)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 165
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 165
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(258)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 258
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 258
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(215)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 215
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 215
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(200)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 200
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 200
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(213)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 213
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 213
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(227)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 227
2022-02-10 17:22:32 DEBUG BlockManager:58 - Getting local block broadcast_21
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 227
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(283)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 283
2022-02-10 17:22:32 DEBUG BlockManager:58 - Level for block broadcast_21 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 283
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(271)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 271
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 271
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(297)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 297
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 297
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(194)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 194
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 194
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(174)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 174
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 174
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(206)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 206
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 206
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(195)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 195
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 195
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(216)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 216
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 216
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(163)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 163
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 163
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(231)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 231
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 231
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(241)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 241
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 241
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(279)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 279
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 279
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(286)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 286
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 286
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(239)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 239
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 239
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(179)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 179
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 179
2022-02-10 17:22:32 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(10)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 10
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 10
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 10
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 10
2022-02-10 17:22:32 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_10
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_10 of size 227232 dropped from memory (free 2064509587)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Getting local block broadcast_20
2022-02-10 17:22:32 DEBUG BlockManager:58 - Level for block broadcast_20 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_10_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_10_piece0 of size 21174 dropped from memory (free 2064530761)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 10, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 10
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(142)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 142
2022-02-10 17:22:32 INFO  Executor:54 - Finished task 0.0 in stage 10.0 (TID 10). 1214 bytes result sent to driver
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 142
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(198)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 198
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 198
2022-02-10 17:22:32 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(122)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 122
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 122
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(207)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 207
2022-02-10 17:22:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 9 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 207
2022-02-10 17:22:32 INFO  TaskSchedulerImpl:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(226)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 226
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 226
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(164)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 164
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 164
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(282)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 282
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 282
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(154)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 154
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 154
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(132)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 132
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 132
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(127)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 127
2022-02-10 17:22:32 INFO  DAGScheduler:54 - ResultStage 10 (load at UseCase2.java:19) finished in 0.017 s
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 127
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(189)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 189
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - After removal of stage 10, remaining stages = 0
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 189
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(16)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 16
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 16
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Job 10 finished: load at UseCase2.java:19, took 0.020671 s
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 16
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 16
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_16
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_16 of size 227232 dropped from memory (free 2064757993)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_16_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_16_piece0 of size 21174 dropped from memory (free 2064779167)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 16, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#154: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#154: java.lang.String
 +- Project [value#146]                                                                                                                                                     +- Project [value#146]
    +- Relation[value#146] text                                                                                                                                                +- Relation[value#146] text
          
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 16
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(237)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 237
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 237
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(144)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 144
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 144
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(296)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 296
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 296
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(182)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 182
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 182
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(240)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 240
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 240
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(15)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 15
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 15
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 15
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 15
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_15
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_15 of size 14200 dropped from memory (free 2064793367)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_15_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_15_piece0 of size 6837 dropped from memory (free 2064800204)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 6.7 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 15, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 15
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(166)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 166
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 166
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(176)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 176
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 176
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(188)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 188
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 188
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(124)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 124
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 124
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(263)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 263
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 263
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(209)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 209
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 209
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(268)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 268
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 268
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(288)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 288
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 288
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(248)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 248
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 248
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(167)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 167
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 167
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(175)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 175
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 175
2022-02-10 17:22:32 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#146 as string).toString, obj#154: java.lang.String   DeserializeToObject value#146.toString, obj#154: java.lang.String
!+- Project [value#146]                                                              +- Relation[value#146] text
!   +- Relation[value#146] text                                                      
          
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(274)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 274
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 274
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(140)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 140
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 140
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(289)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 289
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 289
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(136)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 136
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 136
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(125)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 125
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 125
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(18)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 18
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 18
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 18
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 18
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_18_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_18_piece0 of size 21174 dropped from memory (free 2064821378)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_18
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_18 of size 227232 dropped from memory (free 2065048610)
2022-02-10 17:22:32 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:32 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 18, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 18
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(235)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 235
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 235
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(168)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 168
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 168
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(151)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 151
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 151
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(186)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 186
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 186
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(202)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 202
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 202
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(232)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 232
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 232
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(13)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 13
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 13
2022-02-10 17:22:32 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 13
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 13
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_13_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_13_piece0 of size 7781 dropped from memory (free 2065056391)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_13
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_13 of size 14368 dropped from memory (free 2065070759)
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 13, response is 0
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 13
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(147)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 147
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 147
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(262)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 262
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 262
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(222)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 222
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 222
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(270)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 270
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 270
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(8)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 8
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 8
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_22 locally took  5 ms
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 8
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_22 without replication took  5 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 8
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_8
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_8 of size 227232 dropped from memory (free 2065070759)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_8_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_8_piece0 of size 21174 dropped from memory (free 2065091933)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 8, response is 0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 8
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(129)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 129
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 129
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(159)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 159
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 159
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(276)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 276
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 276
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(177)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 177
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 177
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(172)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 172
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 172
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(139)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 139
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 139
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(269)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 269
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 269
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(246)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 246
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 246
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(199)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 199
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 199
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(224)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 224
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 224
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(281)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 281
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 281
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(149)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 149
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 149
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(150)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 150
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 150
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(134)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 134
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 134
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(275)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 275
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 275
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(298)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 298
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 298
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(294)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 294
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 294
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(145)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 145
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 145
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(287)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 287
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 287
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(243)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 243
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 243
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(252)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 252
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 252
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(171)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 171
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 171
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(158)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 158
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 158
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(265)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 265
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 265
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(196)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 196
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 196
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(284)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 284
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 284
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(138)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 138
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 138
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(234)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 234
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 234
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(291)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 291
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 291
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(160)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 160
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 160
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(141)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 141
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 141
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(212)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 212
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 212
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(181)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 181
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 181
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(251)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 251
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 251
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(300)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning accumulator 300
2022-02-10 17:22:32 INFO  ContextCleaner:54 - Cleaned accumulator 300
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(11)
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaning broadcast 11
2022-02-10 17:22:32 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 11
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 11
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing broadcast 11
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_11
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_11 of size 9144 dropped from memory (free 2065101077)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Removing block broadcast_11_piece0
2022-02-10 17:22:32 DEBUG MemoryStore:58 - Block broadcast_11_piece0 of size 4742 dropped from memory (free 2065084645)
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 11, response is 0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_22_piece0 locally took  2 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_22_piece0 without replication took  2 ms
2022-02-10 17:22:32 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:32 INFO  SparkContext:54 - Created broadcast 22 from load at UseCase2.java:19
2022-02-10 17:22:32 DEBUG ContextCleaner:58 - Cleaned broadcast 11
2022-02-10 17:22:32 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:32 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:32 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Got job 11 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Final stage: ResultStage 11 (load at UseCase2.java:19)
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - submitStage(ResultStage 11 (name=load at UseCase2.java:19;jobs=11))
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Submitting ResultStage 11 (MapPartitionsRDD[57] at load at UseCase2.java:19), which has no missing parents
2022-02-10 17:22:32 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 11)
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 13.9 KB, free 1969.4 MB)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_23 locally took  2 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_23 without replication took  2 ms
2022-02-10 17:22:32 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.4 MB)
2022-02-10 17:22:32 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:32 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 17:22:32 DEBUG BlockManager:58 - Put block broadcast_23_piece0 locally took  5 ms
2022-02-10 17:22:32 DEBUG BlockManager:58 - Putting block broadcast_23_piece0 without replication took  5 ms
2022-02-10 17:22:32 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[57] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:32 INFO  TaskSchedulerImpl:54 - Adding task set 11.0 with 1 tasks
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - Epoch for TaskSet 11.0: 0
2022-02-10 17:22:32 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 11.0: NO_PREF, ANY
2022-02-10 17:22:32 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 17:22:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:22:32 INFO  Executor:54 - Running task 0.0 in stage 11.0 (TID 11)
2022-02-10 17:22:32 DEBUG BlockManager:58 - Getting local block broadcast_23
2022-02-10 17:22:32 DEBUG BlockManager:58 - Level for block broadcast_23 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:32 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:32 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:32 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:32 DEBUG BlockManager:58 - Getting local block broadcast_22
2022-02-10 17:22:32 DEBUG BlockManager:58 - Level for block broadcast_22 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(319)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 319
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 319
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(316)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 316
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 316
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(313)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 313
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 313
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(310)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 310
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 310
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(325)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 325
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 325
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(331)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 331
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 331
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(327)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 327
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 327
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(305)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 305
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 305
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(306)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 306
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 306
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(20)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning broadcast 20
2022-02-10 17:22:34 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 20
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 20
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing broadcast 20
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing block broadcast_20_piece0
2022-02-10 17:22:34 DEBUG MemoryStore:58 - Block broadcast_20_piece0 of size 21174 dropped from memory (free 2065083821)
2022-02-10 17:22:34 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:34 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 17:22:34 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing block broadcast_20
2022-02-10 17:22:34 DEBUG MemoryStore:58 - Block broadcast_20 of size 227232 dropped from memory (free 2065311053)
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 20, response is 0
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaned broadcast 20
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(332)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 332
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 332
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(312)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 312
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 312
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(308)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 308
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 308
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(323)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 323
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 323
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(318)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 318
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 318
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(320)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 320
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 320
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(21)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning broadcast 21
2022-02-10 17:22:34 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 21
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 21
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing broadcast 21
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing block broadcast_21_piece0
2022-02-10 17:22:34 DEBUG MemoryStore:58 - Block broadcast_21_piece0 of size 4744 dropped from memory (free 2065315797)
2022-02-10 17:22:34 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:34 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 17:22:34 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 17:22:34 DEBUG BlockManager:58 - Removing block broadcast_21
2022-02-10 17:22:34 DEBUG MemoryStore:58 - Block broadcast_21 of size 9144 dropped from memory (free 2065324941)
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 21, response is 0
2022-02-10 17:22:34 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaned broadcast 21
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(329)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 329
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 329
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(330)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 330
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 330
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(314)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 314
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 314
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(309)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 309
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 309
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(324)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 324
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 324
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(317)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 317
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 317
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(307)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 307
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 307
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(334)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 334
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 334
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(311)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 311
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 311
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(322)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 322
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 322
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(333)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 333
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 333
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(328)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 328
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 328
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(321)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 321
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 321
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(315)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 315
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 315
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(304)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 304
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 304
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(326)
2022-02-10 17:22:34 DEBUG ContextCleaner:58 - Cleaning accumulator 326
2022-02-10 17:22:34 INFO  ContextCleaner:54 - Cleaned accumulator 326
2022-02-10 17:22:34 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:37 INFO  Executor:54 - Finished task 0.0 in stage 11.0 (TID 11). 1630 bytes result sent to driver
2022-02-10 17:22:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:37 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 5302 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:37 INFO  TaskSchedulerImpl:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2022-02-10 17:22:37 INFO  DAGScheduler:54 - ResultStage 11 (load at UseCase2.java:19) finished in 5.313 s
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - After removal of stage 11, remaining stages = 0
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Job 11 finished: load at UseCase2.java:19, took 5.315759 s
2022-02-10 17:22:37 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:22:37 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 17:22:37 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 17:22:37 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#164
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#164]
 +- Relation[value#164] text                +- Relation[value#164] text
          
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#168: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#168: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 17:22:37 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#164
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#164, None)) > 0)
 +- Project [value#164]                     +- Project [value#164]
    +- Relation[value#164] text                +- Relation[value#164] text
          
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#169: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#169: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#170: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#170: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#164, None)) > 0)      +- Filter (length(trim(value#164, None)) > 0)
!      +- Project [value#164]                             +- Relation[value#164] text
!         +- Relation[value#164] text               
          
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#164, None)) > 0)
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:37 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:37 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:37 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_24 locally took  2 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_24 without replication took  3 ms
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 17:22:37 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_24_piece0 locally took  2 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_24_piece0 without replication took  2 ms
2022-02-10 17:22:37 INFO  SparkContext:54 - Created broadcast 24 from load at UseCase2.java:25
2022-02-10 17:22:37 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:37 INFO  SparkContext:54 - Starting job: load at UseCase2.java:25
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Got job 12 (load at UseCase2.java:25) with 1 output partitions
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (load at UseCase2.java:25)
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - submitStage(ResultStage 12 (name=load at UseCase2.java:25;jobs=12))
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Submitting ResultStage 12 (MapPartitionsRDD[61] at load at UseCase2.java:25), which has no missing parents
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 12)
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_25 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_25 locally took  3 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_25 without replication took  3 ms
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 17:22:37 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_25_piece0 locally took  2 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_25_piece0 without replication took  2 ms
2022-02-10 17:22:37 INFO  SparkContext:54 - Created broadcast 25 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[61] at load at UseCase2.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:37 INFO  TaskSchedulerImpl:54 - Adding task set 12.0 with 1 tasks
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - Epoch for TaskSet 12.0: 0
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 12.0: NO_PREF, ANY
2022-02-10 17:22:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 17:22:37 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:37 INFO  Executor:54 - Running task 0.0 in stage 12.0 (TID 12)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Getting local block broadcast_25
2022-02-10 17:22:37 DEBUG BlockManager:58 - Level for block broadcast_25 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:37 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:37 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:37 DEBUG BlockManager:58 - Getting local block broadcast_24
2022-02-10 17:22:37 DEBUG BlockManager:58 - Level for block broadcast_24 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:37 INFO  Executor:54 - Finished task 0.0 in stage 12.0 (TID 12). 1291 bytes result sent to driver
2022-02-10 17:22:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:37 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 10 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:37 INFO  TaskSchedulerImpl:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2022-02-10 17:22:37 INFO  DAGScheduler:54 - ResultStage 12 (load at UseCase2.java:25) finished in 0.018 s
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - After removal of stage 12, remaining stages = 0
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Job 12 finished: load at UseCase2.java:25, took 0.021611 s
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#172: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#172: java.lang.String
 +- Project [value#164]                                                                                                                                                     +- Project [value#164]
    +- Relation[value#164] text                                                                                                                                                +- Relation[value#164] text
          
2022-02-10 17:22:37 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#164 as string).toString, obj#172: java.lang.String   DeserializeToObject value#164.toString, obj#172: java.lang.String
!+- Project [value#164]                                                              +- Relation[value#164] text
!   +- Relation[value#164] text                                                      
          
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:22:37 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:22:37 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:22:37 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_26 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_26 locally took  3 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_26 without replication took  3 ms
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 17:22:37 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_26_piece0 locally took  2 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_26_piece0 without replication took  2 ms
2022-02-10 17:22:37 INFO  SparkContext:54 - Created broadcast 26 from load at UseCase2.java:25
2022-02-10 17:22:37 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:37 INFO  SparkContext:54 - Starting job: load at UseCase2.java:25
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Got job 13 (load at UseCase2.java:25) with 1 output partitions
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Final stage: ResultStage 13 (load at UseCase2.java:25)
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - submitStage(ResultStage 13 (name=load at UseCase2.java:25;jobs=13))
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Submitting ResultStage 13 (MapPartitionsRDD[67] at load at UseCase2.java:25), which has no missing parents
2022-02-10 17:22:37 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 13)
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_27 stored as values in memory (estimated size 14.0 KB, free 1969.1 MB)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_27 locally took  3 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_27 without replication took  3 ms
2022-02-10 17:22:37 INFO  MemoryStore:54 - Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.1 MB)
2022-02-10 17:22:37 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 17:22:37 DEBUG BlockManager:58 - Put block broadcast_27_piece0 locally took  2 ms
2022-02-10 17:22:37 DEBUG BlockManager:58 - Putting block broadcast_27_piece0 without replication took  2 ms
2022-02-10 17:22:37 INFO  SparkContext:54 - Created broadcast 27 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:37 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[67] at load at UseCase2.java:25) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:37 INFO  TaskSchedulerImpl:54 - Adding task set 13.0 with 1 tasks
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - Epoch for TaskSet 13.0: 0
2022-02-10 17:22:37 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 13.0: NO_PREF, ANY
2022-02-10 17:22:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 17:22:37 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 17:22:37 INFO  Executor:54 - Running task 0.0 in stage 13.0 (TID 13)
2022-02-10 17:22:37 DEBUG BlockManager:58 - Getting local block broadcast_27
2022-02-10 17:22:37 DEBUG BlockManager:58 - Level for block broadcast_27 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:37 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:22:37 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:37 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:22:37 DEBUG BlockManager:58 - Getting local block broadcast_26
2022-02-10 17:22:37 DEBUG BlockManager:58 - Level for block broadcast_26 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:38 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:38 INFO  Executor:54 - Finished task 0.0 in stage 13.0 (TID 13). 1516 bytes result sent to driver
2022-02-10 17:22:38 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 17:22:38 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:38 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 777 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:38 INFO  TaskSchedulerImpl:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2022-02-10 17:22:38 INFO  DAGScheduler:54 - ResultStage 13 (load at UseCase2.java:25) finished in 0.788 s
2022-02-10 17:22:38 DEBUG DAGScheduler:58 - After removal of stage 13, remaining stages = 0
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Job 13 finished: load at UseCase2.java:25, took 0.791200 s
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (order_date#157 LIKE 2014-01% && isnull(order_customer_id#158))                                                                                                                            Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))
 +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                      +- Join RightOuter, (order_customer_id#158 = customer_id#174)
    :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv      +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [ResolvedStar(customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182)]   Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
 +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))                                                                                                                       +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))
    +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                   +- Join RightOuter, (order_customer_id#158 = customer_id#174)
       :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                             :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
       +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                   +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(customer_id#174 as string), None), unresolvedalias(cast(customer_fname#175 as string), None), unresolvedalias(cast(customer_lname#176 as string), None), unresolvedalias(cast(customer_email#177 as string), None), unresolvedalias(cast(customer_password#178 as string), None), unresolvedalias(cast(customer_street#179 as string), None), unresolvedalias(cast(customer_city#180 as string), None), unresolvedalias(cast(customer_state#181 as string), None), unresolvedalias(cast(customer_zipcode#182 as string), None)]   Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
 +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Sort [customer_id#174 ASC NULLS FIRST], true
    +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                                +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
       +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))
          +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    +- Join RightOuter, (order_customer_id#158 = customer_id#174)
             :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
             +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]   Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
 +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Sort [customer_id#174 ASC NULLS FIRST], true
    +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                        +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
       +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))
          +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- Join RightOuter, (order_customer_id#158 = customer_id#174)
             :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
             +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                            +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- LocalLimit 21
!   +- Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                              +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
!            +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_customer_id#158))                                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- Join Inner, (order_customer_id#158 = customer_id#174)
!               +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :- Project [order_customer_id#158]
!                  :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                         :  +- Filter (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158))
!                  +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                               :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Infer Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
!               :- Project [order_customer_id#158]                                                                                                                                                                                                                                                :- Filter isnotnull(order_customer_id#158)
!               :  +- Filter (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158))                                                                                                                                                                               :  +- Project [order_customer_id#158]
!               :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                         :     +- Filter (isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158)))
!               +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                     :        +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!                                                                                                                                                                                                                                                                                                 +- Filter (isnull(customer_id#174) && isnotnull(customer_id#174))
!                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization after Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
!               :- Filter isnotnull(order_customer_id#158)                                                                                                                                                                                                                                        :- Project [order_customer_id#158]
!               :  +- Project [order_customer_id#158]                                                                                                                                                                                                                                             :  +- Filter ((isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158))) && isnotnull(order_customer_id#158))
!               :     +- Filter (isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158)))                                                                                                                                             :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!               :        +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                      +- Filter (isnull(customer_id#174) && isnotnull(customer_id#174))
!               +- Filter (isnull(customer_id#174) && isnotnull(customer_id#174))                                                                                                                                                                                                                    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
!                  +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                   
          
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch RewriteSubquery ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
                :- Project [order_customer_id#158]                                                                                                                                                                                                                                                :- Project [order_customer_id#158]
!               :  +- Filter ((isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_customer_id#158))) && isnotnull(order_customer_id#158))                                                                                                          :  +- Filter (((isnotnull(order_date#157) && StartsWith(cast(order_date#157 as string), 2014-01)) && isnull(order_customer_id#158)) && isnotnull(order_customer_id#158))
                :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                         :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
                +- Filter (isnull(customer_id#174) && isnotnull(customer_id#174))                                                                                                                                                                                                                 +- Filter (isnull(customer_id#174) && isnotnull(customer_id#174))
                   +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                     +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 17:22:38 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#158 = customer_id#174))
2022-02-10 17:22:38 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#158) | rightKeys:List(customer_id#174)
2022-02-10 17:22:38 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#158 = customer_id#174))
2022-02-10 17:22:38 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#158) | rightKeys:List(customer_id#174)
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(order_date#157),StartsWith(cast(order_date#157 as string), 2014-01),isnull(order_customer_id#158),isnotnull(order_customer_id#158)
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_date: timestamp, order_customer_id: int>
2022-02-10 17:22:38 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(order_date),IsNull(order_customer_id),IsNotNull(order_customer_id)
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnull(customer_id#174),isnotnull(customer_id#174)
2022-02-10 17:22:38 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int, customer_fname: string, customer_lname: string, customer_email: string, customer_password: string ... 7 more fields>
2022-02-10 17:22:38 INFO  FileSourceScanExec:54 - Pushed Filters: IsNull(customer_id),IsNotNull(customer_id)
2022-02-10 17:22:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true))), obj#267: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(customer_id#249.toString, customer_fname#250.toString, customer_lname#251.toString, customer_email#252.toString, customer_password#253.toString, customer_street#254.toString, customer_city#255.toString, customer_state#256.toString, customer_zipcode#257.toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)), obj#267: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [customer_id#249, customer_fname#250, customer_lname#251, customer_email#252, customer_password#253, customer_street#254, customer_city#255, customer_state#256, customer_zipcode#257]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       +- LocalRelation <empty>, [customer_id#249, customer_fname#250, customer_lname#251, customer_email#252, customer_password#253, customer_street#254, customer_city#255, customer_state#256, customer_zipcode#257]
          
2022-02-10 17:22:38 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, true].toString, input[7, string, true].toString, input[8, string, true].toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 17:22:38 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 17:22:38 DEBUG CodeGenerator:58 - 
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 17:22:38 INFO  CodeGenerator:54 - Code generated in 13.737 ms
2022-02-10 17:22:38 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         long scan_value_0 = scan_isNull_0 ?
/* 032 */         -1L : (scan_row_0.getLong(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         boolean filter_isNull_3 = scan_isNull_0;
/* 037 */         UTF8String filter_value_3 = null;
/* 038 */         if (!scan_isNull_0) {
/* 039 */           filter_value_3 = UTF8String.fromString(
/* 040 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_0, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 041 */         }
/* 042 */
/* 043 */         boolean filter_value_2 = false;
/* 044 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 045 */         if (!filter_value_2) continue;
/* 046 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 047 */         int scan_value_1 = scan_isNull_1 ?
/* 048 */         -1 : (scan_row_0.getInt(1));
/* 049 */
/* 050 */         if (!(!scan_isNull_1)) continue;
/* 051 */
/* 052 */         if (!scan_isNull_1) continue;
/* 053 */
/* 054 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 055 */
/* 056 */         filter_mutableStateArray_0[1].reset();
/* 057 */
/* 058 */         if (false) {
/* 059 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 060 */         } else {
/* 061 */           filter_mutableStateArray_0[1].write(0, scan_value_1);
/* 062 */         }
/* 063 */         append((filter_mutableStateArray_0[1].getRow()));
/* 064 */
/* 065 */       } while(false);
/* 066 */       if (shouldStop()) return;
/* 067 */     }
/* 068 */   }
/* 069 */
/* 070 */ }

2022-02-10 17:22:38 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         long scan_value_0 = scan_isNull_0 ?
/* 032 */         -1L : (scan_row_0.getLong(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         boolean filter_isNull_3 = scan_isNull_0;
/* 037 */         UTF8String filter_value_3 = null;
/* 038 */         if (!scan_isNull_0) {
/* 039 */           filter_value_3 = UTF8String.fromString(
/* 040 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_0, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 041 */         }
/* 042 */
/* 043 */         boolean filter_value_2 = false;
/* 044 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 045 */         if (!filter_value_2) continue;
/* 046 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 047 */         int scan_value_1 = scan_isNull_1 ?
/* 048 */         -1 : (scan_row_0.getInt(1));
/* 049 */
/* 050 */         if (!(!scan_isNull_1)) continue;
/* 051 */
/* 052 */         if (!scan_isNull_1) continue;
/* 053 */
/* 054 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 055 */
/* 056 */         filter_mutableStateArray_0[1].reset();
/* 057 */
/* 058 */         if (false) {
/* 059 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 060 */         } else {
/* 061 */           filter_mutableStateArray_0[1].write(0, scan_value_1);
/* 062 */         }
/* 063 */         append((filter_mutableStateArray_0[1].getRow()));
/* 064 */
/* 065 */       } while(false);
/* 066 */       if (shouldStop()) return;
/* 067 */     }
/* 068 */   }
/* 069 */
/* 070 */ }

2022-02-10 17:22:38 INFO  CodeGenerator:54 - Code generated in 16.1277 ms
2022-02-10 17:22:38 INFO  MemoryStore:54 - Block broadcast_28 stored as values in memory (estimated size 221.8 KB, free 1968.9 MB)
2022-02-10 17:22:38 DEBUG BlockManager:58 - Put block broadcast_28 locally took  2 ms
2022-02-10 17:22:38 DEBUG BlockManager:58 - Putting block broadcast_28 without replication took  2 ms
2022-02-10 17:22:38 INFO  MemoryStore:54 - Block broadcast_28_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 17:22:38 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:38 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:22:38 DEBUG BlockManager:58 - Told master about block broadcast_28_piece0
2022-02-10 17:22:38 DEBUG BlockManager:58 - Put block broadcast_28_piece0 locally took  1 ms
2022-02-10 17:22:38 DEBUG BlockManager:58 - Putting block broadcast_28_piece0 without replication took  1 ms
2022-02-10 17:22:38 INFO  SparkContext:54 - Created broadcast 28 from run at ThreadPoolExecutor.java:1149
2022-02-10 17:22:38 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:38 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:22:38 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Got job 14 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (run at ThreadPoolExecutor.java:1149)
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:38 DEBUG DAGScheduler:58 - submitStage(ResultStage 14 (name=run at ThreadPoolExecutor.java:1149;jobs=14))
2022-02-10 17:22:38 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Submitting ResultStage 14 (MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 17:22:38 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 14)
2022-02-10 17:22:38 INFO  MemoryStore:54 - Block broadcast_29 stored as values in memory (estimated size 11.7 KB, free 1968.9 MB)
2022-02-10 17:22:38 DEBUG BlockManager:58 - Put block broadcast_29 locally took  1 ms
2022-02-10 17:22:38 DEBUG BlockManager:58 - Putting block broadcast_29 without replication took  1 ms
2022-02-10 17:22:38 INFO  MemoryStore:54 - Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1968.9 MB)
2022-02-10 17:22:38 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 6.3 KB, free: 1970.2 MB)
2022-02-10 17:22:38 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:22:38 DEBUG BlockManager:58 - Told master about block broadcast_29_piece0
2022-02-10 17:22:38 DEBUG BlockManager:58 - Put block broadcast_29_piece0 locally took  1 ms
2022-02-10 17:22:38 DEBUG BlockManager:58 - Putting block broadcast_29_piece0 without replication took  2 ms
2022-02-10 17:22:38 INFO  SparkContext:54 - Created broadcast 29 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:38 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:38 INFO  TaskSchedulerImpl:54 - Adding task set 14.0 with 1 tasks
2022-02-10 17:22:38 DEBUG TaskSetManager:58 - Epoch for TaskSet 14.0: 0
2022-02-10 17:22:38 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 14.0: NO_PREF, ANY
2022-02-10 17:22:38 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 17:22:38 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 17:22:38 INFO  Executor:54 - Running task 0.0 in stage 14.0 (TID 14)
2022-02-10 17:22:38 DEBUG BlockManager:58 - Getting local block broadcast_29
2022-02-10 17:22:38 DEBUG BlockManager:58 - Level for block broadcast_29 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:38 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:22:38 DEBUG GenerateUnsafeProjection:58 - code for input[0, timestamp, true],input[1, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     int value_1 = isNull_1 ?
/* 042 */     -1 : (i.getInt(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 17:22:38 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     long value_0 = isNull_0 ?
/* 033 */     -1L : (i.getLong(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     int value_1 = isNull_1 ?
/* 042 */     -1 : (i.getInt(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 17:22:38 INFO  CodeGenerator:54 - Code generated in 11.2093 ms
2022-02-10 17:22:38 DEBUG BlockManager:58 - Getting local block broadcast_28
2022-02-10 17:22:38 DEBUG BlockManager:58 - Level for block broadcast_28 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(349)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 349
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 349
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(366)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 366
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 366
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(25)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 25
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 25
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 25
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 25
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_25
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_25 of size 9144 dropped from memory (free 2064534530)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_25_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_25_piece0 of size 4744 dropped from memory (free 2064539274)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_25_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 25, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 25
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(402)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 402
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 402
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(373)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 373
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 373
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(347)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 347
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 347
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(372)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 372
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 372
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(381)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 381
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 381
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(353)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 353
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 353
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(405)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 405
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 405
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(344)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 344
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 344
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(336)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 336
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 336
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(425)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 425
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 425
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(24)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 24
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 24
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 24
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 24
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_24
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_24 of size 227232 dropped from memory (free 2064766506)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_24_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_24_piece0 of size 21174 dropped from memory (free 2064787680)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_24_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 24, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 24
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(22)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 22
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 22
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 22
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 22
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_22
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_22 of size 227232 dropped from memory (free 2065014912)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_22_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_22_piece0 of size 21174 dropped from memory (free 2065036086)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 22, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 22
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(404)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 404
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 404
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(370)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 370
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 370
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(354)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 354
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 354
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(376)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 376
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 376
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(27)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 27
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 27
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 27
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 27
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_27
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_27 of size 14368 dropped from memory (free 2065050454)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_27_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_27_piece0 of size 7779 dropped from memory (free 2065058233)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_27_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 27, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 27
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(416)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 416
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 416
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(422)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 422
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 422
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(397)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 397
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 397
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(415)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 415
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 415
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(363)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 363
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 363
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(348)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 348
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 348
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(355)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 355
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 355
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(408)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 408
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 408
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(335)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 335
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 335
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(423)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 423
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 423
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(401)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 401
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 401
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(343)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 343
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 343
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(398)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 398
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 398
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(375)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 375
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 375
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(388)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 388
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 388
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(392)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 392
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 392
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(351)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 351
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 351
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(367)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 367
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 367
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(406)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 406
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 406
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(403)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 403
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 403
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(389)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 389
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 389
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(413)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 413
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 413
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(420)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 420
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 420
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(384)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 384
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 384
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(417)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 417
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 417
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(399)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 399
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 399
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(337)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 337
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 337
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(385)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 385
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 385
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(356)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 356
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 356
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(377)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 377
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 377
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(424)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 424
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 424
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(26)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 26
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 26
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 26
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 26
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_26
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_26 of size 227232 dropped from memory (free 2065285465)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_26_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_26_piece0 of size 21174 dropped from memory (free 2065306639)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_26_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 26, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 26
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(418)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 418
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 418
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(400)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 400
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 400
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(357)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 357
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 357
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(390)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 390
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 390
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(382)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 382
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 382
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(383)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 383
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 383
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(359)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 359
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 359
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(371)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 371
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 371
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(338)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 338
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 338
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(361)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 361
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 361
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(396)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 396
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 396
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(364)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 364
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 364
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(409)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 409
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 409
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(394)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 394
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 394
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(387)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 387
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 387
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(374)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 374
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 374
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(419)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 419
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 419
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(421)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 421
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 421
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(342)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 342
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 342
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(411)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 411
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 411
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(341)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 341
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 341
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(350)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 350
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 350
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(23)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning broadcast 23
2022-02-10 17:22:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 23
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 23
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing broadcast 23
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_23
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_23 of size 14256 dropped from memory (free 2065320895)
2022-02-10 17:22:40 DEBUG BlockManager:58 - Removing block broadcast_23_piece0
2022-02-10 17:22:40 DEBUG MemoryStore:58 - Block broadcast_23_piece0 of size 7742 dropped from memory (free 2065328637)
2022-02-10 17:22:40 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on Clairvoyant-324.mshome.net:59502 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:22:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 17:22:40 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 23, response is 0
2022-02-10 17:22:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:59487
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaned broadcast 23
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(360)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 360
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 360
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(378)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 378
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 378
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(395)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 395
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 395
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(358)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 358
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 358
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(410)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 410
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 410
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(414)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 414
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 414
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(346)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 346
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 346
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(352)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 352
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 352
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(379)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 379
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 379
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(391)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 391
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 391
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(362)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 362
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 362
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(345)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 345
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 345
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(339)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 339
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 339
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(340)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 340
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 340
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(380)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 380
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 380
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(369)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 369
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 369
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(368)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 368
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 368
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(393)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 393
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 393
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(412)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 412
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 412
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(386)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 386
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 386
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(407)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 407
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 407
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(365)
2022-02-10 17:22:40 DEBUG ContextCleaner:58 - Cleaning accumulator 365
2022-02-10 17:22:40 INFO  ContextCleaner:54 - Cleaned accumulator 365
2022-02-10 17:22:42 INFO  Executor:54 - Finished task 0.0 in stage 14.0 (TID 14). 1466 bytes result sent to driver
2022-02-10 17:22:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 17:22:42 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:42 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 3438 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:42 INFO  TaskSchedulerImpl:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2022-02-10 17:22:42 INFO  DAGScheduler:54 - ResultStage 14 (run at ThreadPoolExecutor.java:1149) finished in 3.445 s
2022-02-10 17:22:42 DEBUG DAGScheduler:58 - After removal of stage 14, remaining stages = 0
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Job 14 finished: run at ThreadPoolExecutor.java:1149, took 3.448720 s
2022-02-10 17:22:42 DEBUG TaskMemoryManager:224 - Task 0 acquired 1024.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@7f8ab26a
2022-02-10 17:22:42 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 17:22:42 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 17:22:42 INFO  CodeGenerator:54 - Code generated in 12.828 ms
2022-02-10 17:22:42 DEBUG TaskMemoryManager:224 - Task 0 acquired 16.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@7f8ab26a
2022-02-10 17:22:42 DEBUG TaskMemoryManager:233 - Task 0 release 16.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@7f8ab26a
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_30 stored as values in memory (estimated size 1024.0 KB, free 1968.7 MB)
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_30 locally took  1 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_30 without replication took  1 ms
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_30_piece0 stored as bytes in memory (estimated size 181.0 B, free 1968.7 MB)
2022-02-10 17:22:42 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:22:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Told master about block broadcast_30_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_30_piece0 locally took  3 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_30_piece0 without replication took  3 ms
2022-02-10 17:22:42 INFO  SparkContext:54 - Created broadcast 30 from run at ThreadPoolExecutor.java:1149
2022-02-10 17:22:42 DEBUG BlockManager:58 - Getting local block broadcast_30
2022-02-10 17:22:42 DEBUG BlockManager:58 - Level for block broadcast_30 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:42 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 023 */
/* 024 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 025 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 026 */
/* 027 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 224);
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       do {
/* 037 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 038 */         int scan_value_0 = scan_isNull_0 ?
/* 039 */         -1 : (scan_row_0.getInt(0));
/* 040 */
/* 041 */         if (!(!scan_isNull_0)) continue;
/* 042 */
/* 043 */         if (!scan_isNull_0) continue;
/* 044 */
/* 045 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 046 */
/* 047 */         // generate join key for stream side
/* 048 */         boolean bhj_isNull_0 = false;
/* 049 */         long bhj_value_0 = -1L;
/* 050 */         if (!false) {
/* 051 */           bhj_value_0 = (long) scan_value_0;
/* 052 */         }
/* 053 */         // find matches from HashedRelation
/* 054 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 055 */         if (bhj_matched_0 != null) {
/* 056 */           {
/* 057 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 058 */
/* 059 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 060 */             UTF8String scan_value_1 = scan_isNull_1 ?
/* 061 */             null : (scan_row_0.getUTF8String(1));
/* 062 */             boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 063 */             UTF8String scan_value_2 = scan_isNull_2 ?
/* 064 */             null : (scan_row_0.getUTF8String(2));
/* 065 */             boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 066 */             UTF8String scan_value_3 = scan_isNull_3 ?
/* 067 */             null : (scan_row_0.getUTF8String(3));
/* 068 */             boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 069 */             UTF8String scan_value_4 = scan_isNull_4 ?
/* 070 */             null : (scan_row_0.getUTF8String(4));
/* 071 */             boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 072 */             UTF8String scan_value_5 = scan_isNull_5 ?
/* 073 */             null : (scan_row_0.getUTF8String(5));
/* 074 */             boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 075 */             UTF8String scan_value_6 = scan_isNull_6 ?
/* 076 */             null : (scan_row_0.getUTF8String(6));
/* 077 */             boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 078 */             UTF8String scan_value_7 = scan_isNull_7 ?
/* 079 */             null : (scan_row_0.getUTF8String(7));
/* 080 */             boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 081 */             int scan_value_8 = scan_isNull_8 ?
/* 082 */             -1 : (scan_row_0.getInt(8));
/* 083 */             filter_mutableStateArray_0[3].reset();
/* 084 */
/* 085 */             filter_mutableStateArray_0[3].zeroOutNullBytes();
/* 086 */
/* 087 */             if (false) {
/* 088 */               filter_mutableStateArray_0[3].setNullAt(0);
/* 089 */             } else {
/* 090 */               filter_mutableStateArray_0[3].write(0, scan_value_0);
/* 091 */             }
/* 092 */
/* 093 */             if (scan_isNull_1) {
/* 094 */               filter_mutableStateArray_0[3].setNullAt(1);
/* 095 */             } else {
/* 096 */               filter_mutableStateArray_0[3].write(1, scan_value_1);
/* 097 */             }
/* 098 */
/* 099 */             if (scan_isNull_2) {
/* 100 */               filter_mutableStateArray_0[3].setNullAt(2);
/* 101 */             } else {
/* 102 */               filter_mutableStateArray_0[3].write(2, scan_value_2);
/* 103 */             }
/* 104 */
/* 105 */             if (scan_isNull_3) {
/* 106 */               filter_mutableStateArray_0[3].setNullAt(3);
/* 107 */             } else {
/* 108 */               filter_mutableStateArray_0[3].write(3, scan_value_3);
/* 109 */             }
/* 110 */
/* 111 */             if (scan_isNull_4) {
/* 112 */               filter_mutableStateArray_0[3].setNullAt(4);
/* 113 */             } else {
/* 114 */               filter_mutableStateArray_0[3].write(4, scan_value_4);
/* 115 */             }
/* 116 */
/* 117 */             if (scan_isNull_5) {
/* 118 */               filter_mutableStateArray_0[3].setNullAt(5);
/* 119 */             } else {
/* 120 */               filter_mutableStateArray_0[3].write(5, scan_value_5);
/* 121 */             }
/* 122 */
/* 123 */             if (scan_isNull_6) {
/* 124 */               filter_mutableStateArray_0[3].setNullAt(6);
/* 125 */             } else {
/* 126 */               filter_mutableStateArray_0[3].write(6, scan_value_6);
/* 127 */             }
/* 128 */
/* 129 */             if (scan_isNull_7) {
/* 130 */               filter_mutableStateArray_0[3].setNullAt(7);
/* 131 */             } else {
/* 132 */               filter_mutableStateArray_0[3].write(7, scan_value_7);
/* 133 */             }
/* 134 */
/* 135 */             if (scan_isNull_8) {
/* 136 */               filter_mutableStateArray_0[3].setNullAt(8);
/* 137 */             } else {
/* 138 */               filter_mutableStateArray_0[3].write(8, scan_value_8);
/* 139 */             }
/* 140 */             append((filter_mutableStateArray_0[3].getRow()));
/* 141 */
/* 142 */           }
/* 143 */         }
/* 144 */
/* 145 */       } while(false);
/* 146 */       if (shouldStop()) return;
/* 147 */     }
/* 148 */   }
/* 149 */
/* 150 */ }

2022-02-10 17:22:42 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 023 */
/* 024 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 025 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 026 */
/* 027 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 224);
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       do {
/* 037 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 038 */         int scan_value_0 = scan_isNull_0 ?
/* 039 */         -1 : (scan_row_0.getInt(0));
/* 040 */
/* 041 */         if (!(!scan_isNull_0)) continue;
/* 042 */
/* 043 */         if (!scan_isNull_0) continue;
/* 044 */
/* 045 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 046 */
/* 047 */         // generate join key for stream side
/* 048 */         boolean bhj_isNull_0 = false;
/* 049 */         long bhj_value_0 = -1L;
/* 050 */         if (!false) {
/* 051 */           bhj_value_0 = (long) scan_value_0;
/* 052 */         }
/* 053 */         // find matches from HashedRelation
/* 054 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 055 */         if (bhj_matched_0 != null) {
/* 056 */           {
/* 057 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 058 */
/* 059 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 060 */             UTF8String scan_value_1 = scan_isNull_1 ?
/* 061 */             null : (scan_row_0.getUTF8String(1));
/* 062 */             boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 063 */             UTF8String scan_value_2 = scan_isNull_2 ?
/* 064 */             null : (scan_row_0.getUTF8String(2));
/* 065 */             boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 066 */             UTF8String scan_value_3 = scan_isNull_3 ?
/* 067 */             null : (scan_row_0.getUTF8String(3));
/* 068 */             boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 069 */             UTF8String scan_value_4 = scan_isNull_4 ?
/* 070 */             null : (scan_row_0.getUTF8String(4));
/* 071 */             boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 072 */             UTF8String scan_value_5 = scan_isNull_5 ?
/* 073 */             null : (scan_row_0.getUTF8String(5));
/* 074 */             boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 075 */             UTF8String scan_value_6 = scan_isNull_6 ?
/* 076 */             null : (scan_row_0.getUTF8String(6));
/* 077 */             boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 078 */             UTF8String scan_value_7 = scan_isNull_7 ?
/* 079 */             null : (scan_row_0.getUTF8String(7));
/* 080 */             boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 081 */             int scan_value_8 = scan_isNull_8 ?
/* 082 */             -1 : (scan_row_0.getInt(8));
/* 083 */             filter_mutableStateArray_0[3].reset();
/* 084 */
/* 085 */             filter_mutableStateArray_0[3].zeroOutNullBytes();
/* 086 */
/* 087 */             if (false) {
/* 088 */               filter_mutableStateArray_0[3].setNullAt(0);
/* 089 */             } else {
/* 090 */               filter_mutableStateArray_0[3].write(0, scan_value_0);
/* 091 */             }
/* 092 */
/* 093 */             if (scan_isNull_1) {
/* 094 */               filter_mutableStateArray_0[3].setNullAt(1);
/* 095 */             } else {
/* 096 */               filter_mutableStateArray_0[3].write(1, scan_value_1);
/* 097 */             }
/* 098 */
/* 099 */             if (scan_isNull_2) {
/* 100 */               filter_mutableStateArray_0[3].setNullAt(2);
/* 101 */             } else {
/* 102 */               filter_mutableStateArray_0[3].write(2, scan_value_2);
/* 103 */             }
/* 104 */
/* 105 */             if (scan_isNull_3) {
/* 106 */               filter_mutableStateArray_0[3].setNullAt(3);
/* 107 */             } else {
/* 108 */               filter_mutableStateArray_0[3].write(3, scan_value_3);
/* 109 */             }
/* 110 */
/* 111 */             if (scan_isNull_4) {
/* 112 */               filter_mutableStateArray_0[3].setNullAt(4);
/* 113 */             } else {
/* 114 */               filter_mutableStateArray_0[3].write(4, scan_value_4);
/* 115 */             }
/* 116 */
/* 117 */             if (scan_isNull_5) {
/* 118 */               filter_mutableStateArray_0[3].setNullAt(5);
/* 119 */             } else {
/* 120 */               filter_mutableStateArray_0[3].write(5, scan_value_5);
/* 121 */             }
/* 122 */
/* 123 */             if (scan_isNull_6) {
/* 124 */               filter_mutableStateArray_0[3].setNullAt(6);
/* 125 */             } else {
/* 126 */               filter_mutableStateArray_0[3].write(6, scan_value_6);
/* 127 */             }
/* 128 */
/* 129 */             if (scan_isNull_7) {
/* 130 */               filter_mutableStateArray_0[3].setNullAt(7);
/* 131 */             } else {
/* 132 */               filter_mutableStateArray_0[3].write(7, scan_value_7);
/* 133 */             }
/* 134 */
/* 135 */             if (scan_isNull_8) {
/* 136 */               filter_mutableStateArray_0[3].setNullAt(8);
/* 137 */             } else {
/* 138 */               filter_mutableStateArray_0[3].write(8, scan_value_8);
/* 139 */             }
/* 140 */             append((filter_mutableStateArray_0[3].getRow()));
/* 141 */
/* 142 */           }
/* 143 */         }
/* 144 */
/* 145 */       } while(false);
/* 146 */       if (shouldStop()) return;
/* 147 */     }
/* 148 */   }
/* 149 */
/* 150 */ }

2022-02-10 17:22:42 INFO  CodeGenerator:54 - Code generated in 17.1249 ms
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_31 stored as values in memory (estimated size 221.8 KB, free 1968.4 MB)
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_31 locally took  5 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_31 without replication took  6 ms
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_31_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.4 MB)
2022-02-10 17:22:42 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:22:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Told master about block broadcast_31_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_31_piece0 locally took  2 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_31_piece0 without replication took  2 ms
2022-02-10 17:22:42 INFO  SparkContext:54 - Created broadcast 31 from show at UseCase2.java:46
2022-02-10 17:22:42 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.apply(java.lang.Object)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.apply(org.apache.spark.sql.catalyst.InternalRow)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3) is now cleaned +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1 org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.$outer
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.apply(java.lang.Object)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.apply(scala.collection.Iterator)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[73] at show at UseCase2.java:46
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1,Set(ord$9, num$5))
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[73] at show at UseCase2.java:46)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.$outer
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final int org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.num$5
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final scala.math.Ordering org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.ord$9
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply()
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[73] at show at UseCase2.java:46
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1,Set(ord$9, num$5))
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[73] at show at UseCase2.java:46)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + the starting closure doesn't actually need MapPartitionsRDD[73] at show at UseCase2.java:46, so we null it out
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1) is now cleaned +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32) is now cleaned +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final org.apache.spark.util.BoundedPriorityQueue org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.apply(org.apache.spark.util.BoundedPriorityQueue,org.apache.spark.util.BoundedPriorityQueue)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49) is now cleaned +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:22:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 17:22:42 INFO  SparkContext:54 - Starting job: show at UseCase2.java:46
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Got job 15 (show at UseCase2.java:46) with 1 output partitions
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (show at UseCase2.java:46)
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:22:42 DEBUG DAGScheduler:58 - submitStage(ResultStage 15 (name=show at UseCase2.java:46;jobs=15))
2022-02-10 17:22:42 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Submitting ResultStage 15 (MapPartitionsRDD[74] at show at UseCase2.java:46), which has no missing parents
2022-02-10 17:22:42 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 15)
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_32 stored as values in memory (estimated size 16.3 KB, free 1968.4 MB)
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_32 locally took  2 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_32 without replication took  2 ms
2022-02-10 17:22:42 INFO  MemoryStore:54 - Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.8 KB, free 1968.4 MB)
2022-02-10 17:22:42 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:22:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Told master about block broadcast_32_piece0
2022-02-10 17:22:42 DEBUG BlockManager:58 - Put block broadcast_32_piece0 locally took  2 ms
2022-02-10 17:22:42 DEBUG BlockManager:58 - Putting block broadcast_32_piece0 without replication took  2 ms
2022-02-10 17:22:42 INFO  SparkContext:54 - Created broadcast 32 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:22:42 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[74] at show at UseCase2.java:46) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:22:42 INFO  TaskSchedulerImpl:54 - Adding task set 15.0 with 1 tasks
2022-02-10 17:22:42 DEBUG TaskSetManager:58 - Epoch for TaskSet 15.0: 0
2022-02-10 17:22:42 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 15.0: NO_PREF, ANY
2022-02-10 17:22:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 17:22:42 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 17:22:42 INFO  Executor:54 - Running task 0.0 in stage 15.0 (TID 15)
2022-02-10 17:22:42 DEBUG BlockManager:58 - Getting local block broadcast_32
2022-02-10 17:22:42 DEBUG BlockManager:58 - Level for block broadcast_32 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 17:22:42 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 17:22:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 17:22:42 DEBUG BlockManager:58 - Getting local block broadcast_31
2022-02-10 17:22:42 DEBUG BlockManager:58 - Level for block broadcast_31 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:22:43 INFO  Executor:54 - Finished task 0.0 in stage 15.0 (TID 15). 2840 bytes result sent to driver
2022-02-10 17:22:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 17:22:43 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:22:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 17:22:43 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 580 ms on localhost (executor driver) (1/1)
2022-02-10 17:22:43 INFO  TaskSchedulerImpl:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2022-02-10 17:22:43 INFO  DAGScheduler:54 - ResultStage 15 (show at UseCase2.java:46) finished in 0.595 s
2022-02-10 17:22:43 DEBUG DAGScheduler:58 - After removal of stage 15, remaining stages = 0
2022-02-10 17:22:43 INFO  DAGScheduler:54 - Job 15 finished: show at UseCase2.java:46, took 0.598677 s
2022-02-10 17:22:43 DEBUG package$ExpressionCanonicalizer:58 - 
=== Result of Batch CleanExpressions ===
!cast(input[0, int, true] as string) AS customer_id#249   cast(input[0, int, true] as string)
!+- cast(input[0, int, true] as string)                   +- input[0, int, true]
!   +- input[0, int, true]                                
          
2022-02-10 17:22:43 DEBUG package$ExpressionCanonicalizer:58 - 
=== Result of Batch CleanExpressions ===
!cast(input[8, int, true] as string) AS customer_zipcode#257   cast(input[8, int, true] as string)
!+- cast(input[8, int, true] as string)                        +- input[8, int, true]
!   +- input[8, int, true]                                     
          
2022-02-10 17:22:43 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as string),input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],cast(input[8, int, true] as string):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_6 = i.isNullAt(5);
/* 039 */     UTF8String value_6 = isNull_6 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_6) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_6);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_7 = i.isNullAt(6);
/* 048 */     UTF8String value_7 = isNull_7 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_7) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_7);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_8 = i.isNullAt(7);
/* 057 */     UTF8String value_8 = isNull_8 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_8) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_8);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_10 = i.isNullAt(8);
/* 066 */     int value_10 = isNull_10 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     boolean isNull_9 = isNull_10;
/* 069 */     UTF8String value_9 = null;
/* 070 */     if (!isNull_10) {
/* 071 */       value_9 = UTF8String.fromString(String.valueOf(value_10));
/* 072 */     }
/* 073 */     if (isNull_9) {
/* 074 */       mutableStateArray_0[0].setNullAt(8);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(8, value_9);
/* 077 */     }
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */
/* 082 */   private void writeFields_0_0(InternalRow i) {
/* 083 */
/* 084 */     boolean isNull_1 = i.isNullAt(0);
/* 085 */     int value_1 = isNull_1 ?
/* 086 */     -1 : (i.getInt(0));
/* 087 */     boolean isNull_0 = isNull_1;
/* 088 */     UTF8String value_0 = null;
/* 089 */     if (!isNull_1) {
/* 090 */       value_0 = UTF8String.fromString(String.valueOf(value_1));
/* 091 */     }
/* 092 */     if (isNull_0) {
/* 093 */       mutableStateArray_0[0].setNullAt(0);
/* 094 */     } else {
/* 095 */       mutableStateArray_0[0].write(0, value_0);
/* 096 */     }
/* 097 */
/* 098 */     boolean isNull_2 = i.isNullAt(1);
/* 099 */     UTF8String value_2 = isNull_2 ?
/* 100 */     null : (i.getUTF8String(1));
/* 101 */     if (isNull_2) {
/* 102 */       mutableStateArray_0[0].setNullAt(1);
/* 103 */     } else {
/* 104 */       mutableStateArray_0[0].write(1, value_2);
/* 105 */     }
/* 106 */
/* 107 */     boolean isNull_3 = i.isNullAt(2);
/* 108 */     UTF8String value_3 = isNull_3 ?
/* 109 */     null : (i.getUTF8String(2));
/* 110 */     if (isNull_3) {
/* 111 */       mutableStateArray_0[0].setNullAt(2);
/* 112 */     } else {
/* 113 */       mutableStateArray_0[0].write(2, value_3);
/* 114 */     }
/* 115 */
/* 116 */     boolean isNull_4 = i.isNullAt(3);
/* 117 */     UTF8String value_4 = isNull_4 ?
/* 118 */     null : (i.getUTF8String(3));
/* 119 */     if (isNull_4) {
/* 120 */       mutableStateArray_0[0].setNullAt(3);
/* 121 */     } else {
/* 122 */       mutableStateArray_0[0].write(3, value_4);
/* 123 */     }
/* 124 */
/* 125 */     boolean isNull_5 = i.isNullAt(4);
/* 126 */     UTF8String value_5 = isNull_5 ?
/* 127 */     null : (i.getUTF8String(4));
/* 128 */     if (isNull_5) {
/* 129 */       mutableStateArray_0[0].setNullAt(4);
/* 130 */     } else {
/* 131 */       mutableStateArray_0[0].write(4, value_5);
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */ }

2022-02-10 17:22:43 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_6 = i.isNullAt(5);
/* 039 */     UTF8String value_6 = isNull_6 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_6) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_6);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_7 = i.isNullAt(6);
/* 048 */     UTF8String value_7 = isNull_7 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_7) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_7);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_8 = i.isNullAt(7);
/* 057 */     UTF8String value_8 = isNull_8 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_8) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_8);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_10 = i.isNullAt(8);
/* 066 */     int value_10 = isNull_10 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     boolean isNull_9 = isNull_10;
/* 069 */     UTF8String value_9 = null;
/* 070 */     if (!isNull_10) {
/* 071 */       value_9 = UTF8String.fromString(String.valueOf(value_10));
/* 072 */     }
/* 073 */     if (isNull_9) {
/* 074 */       mutableStateArray_0[0].setNullAt(8);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(8, value_9);
/* 077 */     }
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */
/* 082 */   private void writeFields_0_0(InternalRow i) {
/* 083 */
/* 084 */     boolean isNull_1 = i.isNullAt(0);
/* 085 */     int value_1 = isNull_1 ?
/* 086 */     -1 : (i.getInt(0));
/* 087 */     boolean isNull_0 = isNull_1;
/* 088 */     UTF8String value_0 = null;
/* 089 */     if (!isNull_1) {
/* 090 */       value_0 = UTF8String.fromString(String.valueOf(value_1));
/* 091 */     }
/* 092 */     if (isNull_0) {
/* 093 */       mutableStateArray_0[0].setNullAt(0);
/* 094 */     } else {
/* 095 */       mutableStateArray_0[0].write(0, value_0);
/* 096 */     }
/* 097 */
/* 098 */     boolean isNull_2 = i.isNullAt(1);
/* 099 */     UTF8String value_2 = isNull_2 ?
/* 100 */     null : (i.getUTF8String(1));
/* 101 */     if (isNull_2) {
/* 102 */       mutableStateArray_0[0].setNullAt(1);
/* 103 */     } else {
/* 104 */       mutableStateArray_0[0].write(1, value_2);
/* 105 */     }
/* 106 */
/* 107 */     boolean isNull_3 = i.isNullAt(2);
/* 108 */     UTF8String value_3 = isNull_3 ?
/* 109 */     null : (i.getUTF8String(2));
/* 110 */     if (isNull_3) {
/* 111 */       mutableStateArray_0[0].setNullAt(2);
/* 112 */     } else {
/* 113 */       mutableStateArray_0[0].write(2, value_3);
/* 114 */     }
/* 115 */
/* 116 */     boolean isNull_4 = i.isNullAt(3);
/* 117 */     UTF8String value_4 = isNull_4 ?
/* 118 */     null : (i.getUTF8String(3));
/* 119 */     if (isNull_4) {
/* 120 */       mutableStateArray_0[0].setNullAt(3);
/* 121 */     } else {
/* 122 */       mutableStateArray_0[0].write(3, value_4);
/* 123 */     }
/* 124 */
/* 125 */     boolean isNull_5 = i.isNullAt(4);
/* 126 */     UTF8String value_5 = isNull_5 ?
/* 127 */     null : (i.getUTF8String(4));
/* 128 */     if (isNull_5) {
/* 129 */       mutableStateArray_0[0].setNullAt(4);
/* 130 */     } else {
/* 131 */       mutableStateArray_0[0].write(4, value_5);
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */ }

2022-02-10 17:22:43 INFO  CodeGenerator:54 - Code generated in 15.6047 ms
2022-02-10 17:32:37 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 17:32:37 WARN  HeartbeatReceiver:66 - Removing executor driver with no recent heartbeats: 602012 ms exceeds timeout 120000 ms
2022-02-10 17:32:37 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 17:32:37 ERROR TaskSchedulerImpl:70 - Lost executor driver on localhost: Executor heartbeat timed out after 602012 ms
2022-02-10 17:32:37 DEBUG TaskSchedulerImpl:58 - Cleaning up TaskScheduler state for tasks [] on failed executor driver
2022-02-10 17:32:37 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 17:32:37 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#279
2022-02-10 17:32:37 DEBUG DAGScheduler:58 - Considering removal of executor driver; fileLost: true, currentEpoch: 0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Executor lost: driver (epoch 0)
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 WARN  SparkContext:66 - Killing executors is not supported by current scheduler.
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMasterEndpoint:54 - Trying to remove executor driver from BlockManagerMaster.
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#279]
 +- Relation[value#279] text                +- Relation[value#279] text
          
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_0_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_31_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_28_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_3_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_29_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_32_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_30_piece0 !
2022-02-10 17:32:37 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for broadcast_2_piece0 !
2022-02-10 17:32:37 INFO  BlockManagerMasterEndpoint:54 - Removing block manager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Removed driver successfully in removeExecutor
2022-02-10 17:32:37 INFO  BlockManagerMasterEndpoint:54 - Registering block manager Clairvoyant-324.mshome.net:59502 with 1970.4 MB RAM, BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Shuffle files lost for executor: driver (epoch 0)
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 1
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Host added was in lost list earlier: localhost
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.8 KB, free: 1970.4 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 7.6 KB, free: 1970.4 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#283: java.lang.String   DeserializeToObject cast(value#279 as string).toString, obj#283: java.lang.String
 +- LocalRelation <empty>, [value#279]                                                                                                                                      +- LocalRelation <empty>, [value#279]
          
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#279
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#279, None)) > 0)
 +- Project [value#279]                     +- Project [value#279]
    +- Relation[value#279] text                +- Relation[value#279] text
          
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#284: java.lang.String   DeserializeToObject cast(value#279 as string).toString, obj#284: java.lang.String
 +- LocalRelation <empty>, [value#279]                                                                                                                                      +- LocalRelation <empty>, [value#279]
          
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#285: java.lang.String   DeserializeToObject cast(value#279 as string).toString, obj#285: java.lang.String
 +- LocalRelation <empty>, [value#279]                                                                                                                                      +- LocalRelation <empty>, [value#279]
          
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#279, None)) > 0)      +- Filter (length(trim(value#279, None)) > 0)
!      +- Project [value#279]                             +- Relation[value#279] text
!         +- Relation[value#279] text               
          
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#279, None)) > 0)
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:32:37 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 16 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  MemoryStore:54 - Block broadcast_33 stored as values in memory (estimated size 221.9 KB, free 1968.2 MB)
2022-02-10 17:32:37 DEBUG BlockManager:58 - Put block broadcast_33 locally took  1 ms
2022-02-10 17:32:37 DEBUG BlockManager:58 - Putting block broadcast_33 without replication took  1 ms
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 17 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 17 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  MemoryStore:54 - Block broadcast_33_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.2 MB)
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManager:58 - Told master about block broadcast_33_piece0
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 DEBUG BlockManager:58 - Put block broadcast_33_piece0 locally took  1 ms
2022-02-10 17:32:37 DEBUG BlockManager:58 - Putting block broadcast_33_piece0 without replication took  1 ms
2022-02-10 17:32:37 INFO  SparkContext:54 - Created broadcast 33 from load at UseCase2.java:19
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 18 blocks to the master.
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 18 blocks to the master.
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 17:32:37 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Got job 16 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (load at UseCase2.java:19)
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 17:32:37 DEBUG DAGScheduler:58 - submitStage(ResultStage 16 (name=load at UseCase2.java:19;jobs=16))
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 17:32:37 INFO  Executor:54 - Told to re-register on heartbeat
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Submitting ResultStage 16 (MapPartitionsRDD[78] at load at UseCase2.java:19), which has no missing parents
2022-02-10 17:32:37 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 16)
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None) re-registering with master
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 59502, None)
2022-02-10 17:32:37 INFO  BlockManager:54 - Reporting 18 blocks to the master.
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.8 KB, original size: 7.8 KB, free: 1970.3 MB)
2022-02-10 17:32:37 INFO  MemoryStore:54 - Block broadcast_34 stored as values in memory (estimated size 8.9 KB, free 1968.1 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 17:32:37 DEBUG BlockManager:58 - Put block broadcast_34 locally took  0 ms
2022-02-10 17:32:37 DEBUG BlockManager:58 - Putting block broadcast_34 without replication took  1 ms
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 7.6 KB, original size: 7.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 17:32:37 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 6.3 KB, original size: 6.3 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 17:32:37 INFO  MemoryStore:54 - Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.1 MB)
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Added broadcast_34_piece0 in memory on Clairvoyant-324.mshome.net:59502 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_34_piece0
2022-02-10 17:32:37 DEBUG BlockManager:58 - Told master about block broadcast_34_piece0
2022-02-10 17:32:37 DEBUG BlockManager:58 - Put block broadcast_34_piece0 locally took  1 ms
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManager:58 - Putting block broadcast_34_piece0 without replication took  1 ms
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 17:32:37 INFO  SparkContext:54 - Created broadcast 34 from broadcast at DAGScheduler.scala:1184
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 181.0 B, original size: 181.0 B, free: 1970.3 MB)
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[78] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping Server@4d8286c4{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 17:32:37 INFO  TaskSchedulerImpl:54 - Adding task set 16.0 with 1 tasks
2022-02-10 17:32:37 DEBUG Server:433 - doStop Server@4d8286c4{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 17:32:37 INFO  BlockManagerInfo:54 - Updated broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:59502 (current size: 20.7 KB, original size: 20.7 KB, free: 1970.3 MB)
2022-02-10 17:32:37 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 17:32:37 DEBUG TaskSetManager:58 - Epoch for TaskSet 16.0: 1
2022-02-10 17:32:37 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 16.0: NO_PREF, ANY
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran SparkUI-33-acceptor-0@1f0b3cfe-ServerConnector@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Running task 0.0 in stage 16.0 (TID 16)
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG BlockManager:58 - Getting local block broadcast_34
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG BlockManager:58 - Level for block broadcast_34 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:32:37 DEBUG AbstractHandlerContainer:167 - Graceful shutdown Server@4d8286c4{STOPPING}[9.4.z-SNAPSHOT] by 
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping SelectorManager@Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@4ee6291f{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@2f2d745c on ManagedSelector@4ee6291f{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@4ee6291f{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 17:32:37 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 woken with none selected
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@2f2d745c
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@4ee6291f{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@61c0a594 on ManagedSelector@4ee6291f{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 waiting with 0 keys
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@4ee6291f{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 woken with none selected
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG BlockManager:58 - Getting local block broadcast_33
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG BlockManager:58 - Level for block broadcast_33 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@7a74d03 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@61c0a594
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  Executor:54 - Finished task 0.0 in stage 16.0 (TID 16). 1171 bytes result sent to driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@65c33b92 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 8 ms on localhost (executor driver) (1/1)
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 INFO  TaskSchedulerImpl:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.41+05:30
2022-02-10 17:32:37 INFO  DAGScheduler:54 - ResultStage 16 (load at UseCase2.java:19) finished in 0.015 s
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@5c080ef3/SelectorProducer@188cbcde/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.41+05:30
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@4ee6291f{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 17:32:37 DEBUG DAGScheduler:58 - After removal of stage 16, remaining stages = 0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@24e83d19{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@675b5d28 on ManagedSelector@24e83d19{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 17:32:37 INFO  DAGScheduler:54 - Job 16 finished: load at UseCase2.java:19, took 0.016884 s
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@24e83d19{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 woken with none selected
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@675b5d28
2022-02-10 17:32:37 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@24e83d19{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 waiting with 0 keys
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@3ca0e716 on ManagedSelector@24e83d19{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@24e83d19{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG HeartbeatReceiver:58 - Received heartbeat from unknown executor driver
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 woken with none selected
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@545ad031 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@3ca0e716
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/PRODUCING/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.413+05:30
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@9b21bd3 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@124ac145/SelectorProducer@2def7a7a/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.413+05:30
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@24e83d19{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@4f66ffc8{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@786beb4a on ManagedSelector@4f66ffc8{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@4f66ffc8{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#287: java.lang.String   DeserializeToObject cast(value#279 as string).toString, obj#287: java.lang.String
 +- Project [value#279]                                                                                                                                                     +- Project [value#279]
    +- Relation[value#279] text                                                                                                                                                +- Relation[value#279] text
          
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 woken with none selected
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@786beb4a
2022-02-10 17:32:37 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@4f66ffc8{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 waiting with 0 keys
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@1c631dd6 on ManagedSelector@4f66ffc8{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@4f66ffc8{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 woken with none selected
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@78badc93 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@1c631dd6
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@44e93c1f in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.415+05:30
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@1bc49bc5/SelectorProducer@7b6e5c12/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.415+05:30
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@4f66ffc8{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@7a04fea7{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@216cb16f on ManagedSelector@7a04fea7{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@7a04fea7{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 woken with none selected
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@216cb16f
2022-02-10 17:32:37 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@7a04fea7{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 waiting with 0 keys
2022-02-10 17:32:37 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@1a4f6a03 on ManagedSelector@7a04fea7{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 17:32:37 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@7a04fea7{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 17:32:37 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 woken with none selected
2022-02-10 17:32:37 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 woken up from select, 0/0/0 selected
2022-02-10 17:32:37 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@59f70ad0 processing 0 keys, 1 updates
2022-02-10 17:32:37 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 17:32:37 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@1a4f6a03
2022-02-10 17:32:37 DEBUG ManagedSelector:587 - updates 0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/PRODUCING/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.417+05:30
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/937437482@4af70944 in QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@3f725306/SelectorProducer@a2ddf26/IDLE/p=false/QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T17:32:37.417+05:30
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@7a04fea7{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED SelectorManager@Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping HttpConnectionFactory@3db972d2[HTTP/1.1]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED HttpConnectionFactory@3db972d2[HTTP/1.1]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ScheduledExecutorScheduler@64fe9da7{STARTED}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ScheduledExecutorScheduler@64fe9da7{STOPPED}
2022-02-10 17:32:37 INFO  AbstractConnector:381 - Stopped Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:32:37 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#279 as string).toString, obj#287: java.lang.String   DeserializeToObject value#279.toString, obj#287: java.lang.String
!+- Project [value#279]                                                              +- Relation[value#279] text
!   +- Relation[value#279] text                                                      
          
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED Spark@7eae3764{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 17:32:37 DEBUG AbstractHandler:107 - stopping Server@4d8286c4{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ContextHandlerCollection@fb6097b{STARTED}
2022-02-10 17:32:37 DEBUG AbstractHandler:107 - stopping ContextHandlerCollection@fb6097b{STOPPING}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ContextHandlerCollection@fb6097b{STOPPED}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ErrorHandler@69228e85{STARTED}
2022-02-10 17:32:37 DEBUG AbstractHandler:107 - stopping ErrorHandler@69228e85{STOPPING}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ErrorHandler@69228e85{STOPPED}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping QueuedThreadPool[SparkUI]@5d5160e6{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:224 - Stopping QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:212 - stopping ReservedThreadExecutor@5a2bd7c8{s=0/8,p=0}
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED ReservedThreadExecutor@5a2bd7c8{s=-1/8,p=0}
2022-02-10 17:32:37 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-29,5,main] for 14999
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-30,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-32,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-33,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-34,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-35,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-29,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1665209618@4d8376cc in QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-33,5,] for 14998
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-36,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-31,5,main] for 14998
2022-02-10 17:32:37 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-31,5,main] exited for QueuedThreadPool[SparkUI]@5d5160e6{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED QueuedThreadPool[SparkUI]@5d5160e6{STOPPED,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 17:32:37 DEBUG AbstractLifeCycle:224 - STOPPED Server@4d8286c4{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 17:32:37 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 17:32:37 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 17:32:37 INFO  SparkUI:54 - Stopped Spark web UI at http://Clairvoyant-324.mshome.net:4040
2022-02-10 17:32:37 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 17:32:37 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2022-02-10 17:32:37 INFO  MemoryStore:54 - MemoryStore cleared
2022-02-10 17:32:37 INFO  BlockManager:54 - BlockManager stopped
2022-02-10 17:32:37 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2022-02-10 17:32:37 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2022-02-10 17:32:37 INFO  SparkContext:54 - Successfully stopped SparkContext
2022-02-10 17:32:37 INFO  ShutdownHookManager:54 - Shutdown hook called
2022-02-10 17:32:37 INFO  ShutdownHookManager:54 - Deleting directory C:\Users\Anukul Thalkar\AppData\Local\Temp\spark-c89846e4-a3b4-4476-83d7-c8df3b211bb6
