2022-02-10 15:13:34 INFO  SparkContext:54 - Running Spark version 2.4.8
2022-02-10 15:13:34 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 15:13:34 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 15:13:34 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
2022-02-10 15:13:34 DEBUG MetricsSystemImpl:231 - UgiMetrics, User and group related metrics
2022-02-10 15:13:35 DEBUG KerberosName:88 - Kerberos krb5 configuration not found, setting default realm to empty
2022-02-10 15:13:35 DEBUG Groups:291 -  Creating new Groups object
2022-02-10 15:13:35 DEBUG NativeCodeLoader:46 - Trying to load the custom-built native-hadoop library...
2022-02-10 15:13:35 DEBUG NativeCodeLoader:50 - Loaded the native-hadoop library
2022-02-10 15:13:35 DEBUG JniBasedUnixGroupsMapping:50 - Using JniBasedUnixGroupsMapping for Group resolution
2022-02-10 15:13:35 DEBUG JniBasedUnixGroupsMappingWithFallback:45 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
2022-02-10 15:13:35 ERROR Shell:396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\Users\Anukul Thalkar\hadoop\bin\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:293)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2526)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at util.getSparkSession(util.java:4)
	at UseCase5.getProducts(UseCase5.java:32)
	at UseCase5.getProductsCount(UseCase5.java:47)
	at UseCase5Test.validateProducts(UseCase5Test.java:17)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
2022-02-10 15:13:35 DEBUG Groups:103 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2022-02-10 15:13:35 DEBUG UserGroupInformation:221 - hadoop login
2022-02-10 15:13:35 DEBUG UserGroupInformation:156 - hadoop login commit
2022-02-10 15:13:35 DEBUG UserGroupInformation:186 - using local user:NTUserPrincipal: Anukul Thalkar
2022-02-10 15:13:35 DEBUG UserGroupInformation:192 - Using user: "NTUserPrincipal: Anukul Thalkar" with name Anukul Thalkar
2022-02-10 15:13:35 DEBUG UserGroupInformation:202 - User entry: "Anukul Thalkar"
2022-02-10 15:13:35 DEBUG UserGroupInformation:825 - UGI loginUser:Anukul Thalkar (auth:SIMPLE)
2022-02-10 15:13:35 INFO  SparkContext:54 - Submitted application: e75f2563-3da7-4cc6-aeec-39c9b0a37dbe
2022-02-10 15:13:35 INFO  SecurityManager:54 - Changing view acls to: Anukul Thalkar
2022-02-10 15:13:35 INFO  SecurityManager:54 - Changing modify acls to: Anukul Thalkar
2022-02-10 15:13:35 INFO  SecurityManager:54 - Changing view acls groups to: 
2022-02-10 15:13:35 INFO  SecurityManager:54 - Changing modify acls groups to: 
2022-02-10 15:13:35 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Anukul Thalkar); groups with view permissions: Set(); users  with modify permissions: Set(Anukul Thalkar); groups with modify permissions: Set()
2022-02-10 15:13:35 DEBUG InternalLoggerFactory:45 - Using SLF4J as the default logging framework
2022-02-10 15:13:35 DEBUG InternalThreadLocalMap:56 - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2022-02-10 15:13:35 DEBUG InternalThreadLocalMap:59 - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2022-02-10 15:13:35 DEBUG MultithreadEventLoopGroup:44 - -Dio.netty.eventLoopThreads: 16
2022-02-10 15:13:35 DEBUG NioEventLoop:106 - -Dio.netty.noKeySetOptimization: false
2022-02-10 15:13:35 DEBUG NioEventLoop:107 - -Dio.netty.selectorAutoRebuildThreshold: 512
2022-02-10 15:13:35 DEBUG PlatformDependent:1003 - Platform: Windows
2022-02-10 15:13:35 DEBUG PlatformDependent0:396 - -Dio.netty.noUnsafe: false
2022-02-10 15:13:35 DEBUG PlatformDependent0:852 - Java version: 8
2022-02-10 15:13:35 DEBUG PlatformDependent0:121 - sun.misc.Unsafe.theUnsafe: available
2022-02-10 15:13:35 DEBUG PlatformDependent0:145 - sun.misc.Unsafe.copyMemory: available
2022-02-10 15:13:35 DEBUG PlatformDependent0:183 - java.nio.Buffer.address: available
2022-02-10 15:13:35 DEBUG PlatformDependent0:244 - direct buffer constructor: available
2022-02-10 15:13:35 DEBUG PlatformDependent0:314 - java.nio.Bits.unaligned: available, true
2022-02-10 15:13:35 DEBUG PlatformDependent0:379 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2022-02-10 15:13:35 DEBUG PlatformDependent0:386 - java.nio.DirectByteBuffer.<init>(long, int): available
2022-02-10 15:13:35 DEBUG PlatformDependent:1046 - sun.misc.Unsafe: available
2022-02-10 15:13:35 DEBUG PlatformDependent:1165 - -Dio.netty.tmpdir: C:\Users\ANUKUL~1\AppData\Local\Temp (java.io.tmpdir)
2022-02-10 15:13:35 DEBUG PlatformDependent:1244 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
2022-02-10 15:13:35 DEBUG PlatformDependent:177 - -Dio.netty.maxDirectMemory: 3758096384 bytes
2022-02-10 15:13:35 DEBUG PlatformDependent:184 - -Dio.netty.uninitializedArrayAllocationThreshold: -1
2022-02-10 15:13:35 DEBUG CleanerJava6:92 - java.nio.ByteBuffer.cleaner(): available
2022-02-10 15:13:35 DEBUG PlatformDependent:204 - -Dio.netty.noPreferDirect: false
2022-02-10 15:13:35 DEBUG PlatformDependent:907 - org.jctools-core.MpscChunkedArrayQueue: available
2022-02-10 15:13:35 DEBUG ResourceLeakDetector:130 - -Dio.netty.leakDetection.level: simple
2022-02-10 15:13:35 DEBUG ResourceLeakDetector:131 - -Dio.netty.leakDetection.targetRecords: 4
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:156 - -Dio.netty.allocator.numHeapArenas: 16
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:157 - -Dio.netty.allocator.numDirectArenas: 16
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:159 - -Dio.netty.allocator.pageSize: 8192
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:164 - -Dio.netty.allocator.maxOrder: 11
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:168 - -Dio.netty.allocator.chunkSize: 16777216
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:169 - -Dio.netty.allocator.tinyCacheSize: 512
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:170 - -Dio.netty.allocator.smallCacheSize: 256
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:171 - -Dio.netty.allocator.normalCacheSize: 64
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:172 - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:173 - -Dio.netty.allocator.cacheTrimInterval: 8192
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:174 - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:175 - -Dio.netty.allocator.useCacheForAllThreads: true
2022-02-10 15:13:35 DEBUG PooledByteBufAllocator:176 - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2022-02-10 15:13:35 DEBUG DefaultChannelId:79 - -Dio.netty.processId: 8392 (auto-detected)
2022-02-10 15:13:35 DEBUG NetUtil:139 - -Djava.net.preferIPv4Stack: false
2022-02-10 15:13:35 DEBUG NetUtil:140 - -Djava.net.preferIPv6Addresses: false
2022-02-10 15:13:35 DEBUG NetUtil:224 - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)
2022-02-10 15:13:35 DEBUG NetUtil:289 - Failed to get SOMAXCONN from sysctl and file \proc\sys\net\core\somaxconn. Default: 200
2022-02-10 15:13:35 DEBUG DefaultChannelId:101 - -Dio.netty.machineId: 7c:70:db:ff:fe:41:5e:f6 (auto-detected)
2022-02-10 15:13:35 DEBUG ByteBufUtil:86 - -Dio.netty.allocator.type: pooled
2022-02-10 15:13:35 DEBUG ByteBufUtil:95 - -Dio.netty.threadLocalDirectBufferSize: 0
2022-02-10 15:13:35 DEBUG ByteBufUtil:98 - -Dio.netty.maxThreadLocalCharBufferSize: 16384
2022-02-10 15:13:35 DEBUG TransportServer:141 - Shuffle server started on port: 58884
2022-02-10 15:13:35 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 58884.
2022-02-10 15:13:35 DEBUG SparkEnv:58 - Using serializer: class org.apache.spark.serializer.JavaSerializer
2022-02-10 15:13:35 INFO  SparkEnv:54 - Registering MapOutputTracker
2022-02-10 15:13:35 DEBUG MapOutputTrackerMasterEndpoint:58 - init
2022-02-10 15:13:35 INFO  SparkEnv:54 - Registering BlockManagerMaster
2022-02-10 15:13:35 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2022-02-10 15:13:35 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2022-02-10 15:13:35 INFO  DiskBlockManager:54 - Created local directory at C:\Users\Anukul Thalkar\AppData\Local\Temp\blockmgr-0d23ce56-6732-48eb-b1f0-800ac9f21fb1
2022-02-10 15:13:35 DEBUG DiskBlockManager:58 - Adding shutdown hook
2022-02-10 15:13:35 DEBUG ShutdownHookManager:58 - Adding shutdown hook
2022-02-10 15:13:36 INFO  MemoryStore:54 - MemoryStore started with capacity 1970.4 MB
2022-02-10 15:13:36 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2022-02-10 15:13:36 DEBUG OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:58 - init
2022-02-10 15:13:36 DEBUG SecurityManager:58 - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2022-02-10 15:13:36 DEBUG log:159 - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.spark_project.jetty.util.log) via org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 15:13:36 INFO  log:169 - Logging initialized @2253ms to org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2b58f754
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@66629f63{/,null,STOPPED} added {ServletHandler@6f80fafe{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6f80fafe{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9==org.apache.spark.ui.JettyUtils$$anon$3@b7cf7b4d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6f80fafe{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6fff253c
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6c6357f9{/,null,STOPPED} added {ServletHandler@591e58fa{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@591e58fa{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-3954d008==org.apache.spark.ui.JettyUtils$$anon$3@57891e45{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@591e58fa{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-3954d008,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@493dfb8e
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5d25e6bb{/,null,STOPPED} added {ServletHandler@ce5a68e{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@ce5a68e{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-9d157ff==org.apache.spark.ui.JettyUtils$$anon$3@4f91a6bc{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@ce5a68e{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-9d157ff,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2f162cc0
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5df417a7{/,null,STOPPED} added {ServletHandler@7c041b41{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7c041b41{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7f69d591==org.apache.spark.ui.JettyUtils$$anon$3@45819e27{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7c041b41{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7f69d591,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@664a9613
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5118388b{/,null,STOPPED} added {ServletHandler@15a902e7{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@15a902e7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7876d598==org.apache.spark.ui.JettyUtils$$anon$3@c5ad8a9c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@15a902e7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7876d598,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4a3e3e8b
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5af28b27{/,null,STOPPED} added {ServletHandler@71104a4{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@71104a4{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb==org.apache.spark.ui.JettyUtils$$anon$3@c4973a2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@71104a4{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3c9168dc
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@332a7fce{/,null,STOPPED} added {ServletHandler@549621f3{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@549621f3{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-54361a9==org.apache.spark.ui.JettyUtils$$anon$3@2a2ca7fb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@549621f3{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-54361a9,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@32232e55
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5217f3d0{/,null,STOPPED} added {ServletHandler@37ebc9d8{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@37ebc9d8{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5==org.apache.spark.ui.JettyUtils$$anon$3@eec5b48c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@37ebc9d8{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6fa590ba
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6e9319f{/,null,STOPPED} added {ServletHandler@72e34f77{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@72e34f77{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098==org.apache.spark.ui.JettyUtils$$anon$3@c3d05377{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@72e34f77{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@389adf1d
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@77307458{/,null,STOPPED} added {ServletHandler@1fc0053e{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@1fc0053e{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e==org.apache.spark.ui.JettyUtils$$anon$3@e9917ca9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@1fc0053e{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@18e7143f
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@f9b7332{/,null,STOPPED} added {ServletHandler@74cec793{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@74cec793{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e==org.apache.spark.ui.JettyUtils$$anon$3@304e982b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@74cec793{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4f8969b0
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1bdf8190{/,null,STOPPED} added {ServletHandler@192f2f27{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@192f2f27{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-8a589a2==org.apache.spark.ui.JettyUtils$$anon$3@15c6a372{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@192f2f27{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-8a589a2,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6b5176f2
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@b672aa8{/,null,STOPPED} added {ServletHandler@2fab4aff{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@2fab4aff{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-ec0c838==org.apache.spark.ui.JettyUtils$$anon$3@6575cc40{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@2fab4aff{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-ec0c838,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6e46d9f4
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5cc69cfe{/,null,STOPPED} added {ServletHandler@29cfd92b{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@29cfd92b{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-21c64522==org.apache.spark.ui.JettyUtils$$anon$3@96fb741f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@29cfd92b{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-21c64522,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@55f3c410
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@11acdc30{/,null,STOPPED} added {ServletHandler@770d4269{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@770d4269{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068==org.apache.spark.ui.JettyUtils$$anon$3@afb50d2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@770d4269{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1922e6d
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@76a82f33{/,null,STOPPED} added {ServletHandler@6bab2585{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6bab2585{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-74bdc168==org.apache.spark.ui.JettyUtils$$anon$3@3dfcb5bb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6bab2585{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-74bdc168,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7f811d00
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@62923ee6{/,null,STOPPED} added {ServletHandler@4089713{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4089713{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2==org.apache.spark.ui.JettyUtils$$anon$3@458895a6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4089713{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7807ac2c
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@b91d8c4{/,null,STOPPED} added {ServletHandler@4b6166aa{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4b6166aa{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-a77614d==org.apache.spark.ui.JettyUtils$$anon$3@4715b9c0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4b6166aa{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-a77614d,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4a067c25
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@a1217f9{/,null,STOPPED} added {ServletHandler@3bde62ff{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3bde62ff{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-523424b5==org.apache.spark.ui.JettyUtils$$anon$3@b17f827f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3bde62ff{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-523424b5,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2baa8d82
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@319dead1{/,null,STOPPED} added {ServletHandler@791cbf87{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@791cbf87{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d==org.apache.spark.ui.JettyUtils$$anon$3@499ce180{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@791cbf87{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2b52c0d6
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@372ea2bc{/,null,STOPPED} added {ServletHandler@4cc76301{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG PreEncodedHttpField:61 - HttpField encoders loaded: []
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4cc76301{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-cf65451==org.spark_project.jetty.servlet.DefaultServlet@dcc00e3f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@4cc76301{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-cf65451,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@32fe9d0a
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@c9413d8{/,null,STOPPED} added {ServletHandler@64da2a7{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@64da2a7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-46074492==org.apache.spark.ui.JettyUtils$$anon$4@c8d542d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@64da2a7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-46074492,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@47428937
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3b9d6699{/,null,STOPPED} added {ServletHandler@7caa550{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7caa550{STOPPED} added {org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7caa550{STOPPED} added {[/*]=>org.glassfish.jersey.servlet.ServletContainer-30ed9c6c,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2fd1731c
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5ae76500{/,null,STOPPED} added {ServletHandler@6063d80a{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6063d80a{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e==org.apache.spark.ui.JettyUtils$$anon$4@d167c798{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@6063d80a{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2a2da905
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@24f360b2{/,null,STOPPED} added {ServletHandler@60cf80e7{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@60cf80e7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-302fec27==org.apache.spark.ui.JettyUtils$$anon$4@6d5dbb7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@60cf80e7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-302fec27,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[qtp1251897263]@4a9e6faf{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY] added {org.spark_project.jetty.util.thread.ThreadPoolBudget@a50b09c,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - Server@538613b3{STOPPED}[9.4.z-SNAPSHOT] added {QueuedThreadPool[SparkUI]@4a9e6faf{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY],AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - Server@538613b3{STOPPED}[9.4.z-SNAPSHOT] added {ErrorHandler@5c1bd44c{STOPPED},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - Server@538613b3{STOPPED}[9.4.z-SNAPSHOT] added {ContextHandlerCollection@560cbf1a{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting Server@538613b3{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 15:13:36 INFO  Server:375 - jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_281-b09
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting Server@538613b3{STARTING}[9.4.z-SNAPSHOT]
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting QueuedThreadPool[SparkUI]@4a9e6faf{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:36 DEBUG ReservedThreadExecutor:85 - ReservedThreadExecutor@3d6300e8{s=0/8,p=0}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=0<=200,i=0,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}] added {ReservedThreadExecutor@3d6300e8{s=0/8,p=0},AUTO}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ReservedThreadExecutor@3d6300e8{s=0/8,p=0}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2341ms ReservedThreadExecutor@3d6300e8{s=0/8,p=0}
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-27,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-28,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-29,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=4<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-30,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-31,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-32,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=6<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-33,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=7<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-34,5,main]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=7<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2345ms QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ErrorHandler@5c1bd44c{STOPPED}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ErrorHandler@5c1bd44c{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2346ms ErrorHandler@5c1bd44c{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ContextHandlerCollection@560cbf1a{STOPPED}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ContextHandlerCollection@560cbf1a{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2346ms ContextHandlerCollection@560cbf1a{STARTED}
2022-02-10 15:13:36 INFO  Server:415 - Started @2347ms
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2347ms Server@538613b3{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 15:13:36 DEBUG JettyUtils:58 - Using requestHeaderSize: 8192
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - HttpConnectionFactory@681aad3b[HTTP/1.1] added {HttpConfiguration@1a6f2363{32768/8192,8192/8192,https://:0,[]},POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{null, ()}{0.0.0.0:0} added {Server@538613b3{STARTED}[9.4.z-SNAPSHOT],UNMANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{null, ()}{0.0.0.0:0} added {QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}],UNMANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{null, ()}{0.0.0.0:0} added {ScheduledExecutorScheduler@3a0807b7{STOPPED},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{null, ()}{0.0.0.0:0} added {org.spark_project.jetty.io.ArrayByteBufferPool@21a5fd96,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{null, (http/1.1)}{0.0.0.0:0} added {HttpConnectionFactory@681aad3b[HTTP/1.1],AUTO}
2022-02-10 15:13:36 DEBUG AbstractConnector:484 - ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added HttpConnectionFactory@681aad3b[HTTP/1.1]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added {SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:0},MANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:4040],POJO}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ScheduledExecutorScheduler@3a0807b7{STOPPED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2372ms ScheduledExecutorScheduler@3a0807b7{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting HttpConnectionFactory@681aad3b[HTTP/1.1]
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2372ms HttpConnectionFactory@681aad3b[HTTP/1.1]
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 added {SelectorProducer@e3cee7b,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 added {QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}],UNMANAGED}
2022-02-10 15:13:36 DEBUG EatWhatYouKill:93 - EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 created
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ManagedSelector@f8908f6{STOPPED} id=0 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30,MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@f8908f6{STOPPED} id=0 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 added {SelectorProducer@2ef8a8c3,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 added {QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}],UNMANAGED}
2022-02-10 15:13:36 DEBUG EatWhatYouKill:93 - EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30 created
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ManagedSelector@24f43aa3{STOPPED} id=1 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.201+05:30,MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@24f43aa3{STOPPED} id=1 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 added {SelectorProducer@1e11bc55,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 added {QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}],UNMANAGED}
2022-02-10 15:13:36 DEBUG EatWhatYouKill:93 - EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 created
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ManagedSelector@7544a1e4{STOPPED} id=2 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30,MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@7544a1e4{STOPPED} id=2 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 added {SelectorProducer@7957dc72,POJO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 added {QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}],UNMANAGED}
2022-02-10 15:13:36 DEBUG EatWhatYouKill:93 - EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 created
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ManagedSelector@6ab72419{STOPPED} id=3 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30,MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@6ab72419{STOPPED} id=3 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@f8908f6{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2396ms EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@5be82d43 startThread=0
2022-02-10 15:13:36 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@5be82d43 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@600b0b7 on ManagedSelector@f8908f6{STARTING} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG EatWhatYouKill:141 - EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 tryProduce false
2022-02-10 15:13:36 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:36 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@600b0b7
2022-02-10 15:13:36 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:36 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 waiting with 0 keys
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2401ms ManagedSelector@f8908f6{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@24f43aa3{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2402ms EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@345e5a17 startThread=0
2022-02-10 15:13:36 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@345e5a17 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@5ea502e0 on ManagedSelector@24f43aa3{STARTING} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG EatWhatYouKill:141 - EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 tryProduce false
2022-02-10 15:13:36 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:36 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@5ea502e0
2022-02-10 15:13:36 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:36 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 waiting with 0 keys
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2404ms ManagedSelector@24f43aa3{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@7544a1e4{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2404ms EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@443dbe42 startThread=0
2022-02-10 15:13:36 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@443dbe42 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@473b3b7a on ManagedSelector@7544a1e4{STARTING} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG EatWhatYouKill:141 - EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 tryProduce false
2022-02-10 15:13:36 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:36 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@473b3b7a
2022-02-10 15:13:36 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:36 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 waiting with 0 keys
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2406ms ManagedSelector@7544a1e4{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@6ab72419{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2407ms EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30
2022-02-10 15:13:36 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@77b7ffa4 startThread=0
2022-02-10 15:13:36 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@77b7ffa4 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@5ed190be on ManagedSelector@6ab72419{STARTING} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG EatWhatYouKill:141 - EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:36.217+05:30 tryProduce false
2022-02-10 15:13:36 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:36 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@5ed190be
2022-02-10 15:13:36 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:36 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe waiting with 0 keys
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2409ms ManagedSelector@6ab72419{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2409ms SelectorManager@ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {acceptor-0@133e019b,POJO}
2022-02-10 15:13:36 DEBUG QueuedThreadPool:719 - queue acceptor-0@133e019b startThread=0
2022-02-10 15:13:36 DEBUG QueuedThreadPool:1035 - run acceptor-0@133e019b in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:36 INFO  AbstractConnector:331 - Started ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2411ms ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:36 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - Server@538613b3{STARTED}[9.4.z-SNAPSHOT] added {Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040},UNMANAGED}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@22680f52{STOPPED,min=32,inflate=-1} mime types IncludeExclude@60d84f61{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@22680f52{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@66629f63{/jobs,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@22680f52{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@22680f52{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@22680f52{STARTING,min=32,inflate=-1} added {DeflaterPool@2575f671{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@22680f52{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@66629f63{/jobs,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@66629f63{/jobs,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6f80fafe{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9==org.apache.spark.ui.JettyUtils$$anon$3@b7cf7b4d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9=org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9==org.apache.spark.ui.JettyUtils$$anon$3@b7cf7b4d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@6f80fafe{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2429ms ServletHandler@6f80fafe{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9==org.apache.spark.ui.JettyUtils$$anon$3@b7cf7b4d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2432ms org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9==org.apache.spark.ui.JettyUtils$$anon$3@b7cf7b4d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-5ab14cb9
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2435ms o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2575f671{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2435ms DeflaterPool@2575f671{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2435ms GzipHandler@22680f52{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@5949eba8{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6e0ff644{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@5949eba8{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@5949eba8{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5949eba8{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@5949eba8{STARTING,min=32,inflate=-1} added {DeflaterPool@58dea0a5{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@5949eba8{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@591e58fa{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-3954d008[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-3954d008==org.apache.spark.ui.JettyUtils$$anon$3@57891e45{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3954d008=org.apache.spark.ui.JettyUtils$$anon$3-3954d008==org.apache.spark.ui.JettyUtils$$anon$3@57891e45{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@591e58fa{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2438ms ServletHandler@591e58fa{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-3954d008==org.apache.spark.ui.JettyUtils$$anon$3@57891e45{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2438ms org.apache.spark.ui.JettyUtils$$anon$3-3954d008==org.apache.spark.ui.JettyUtils$$anon$3@57891e45{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-3954d008
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2438ms o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@58dea0a5{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2439ms DeflaterPool@58dea0a5{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2439ms GzipHandler@5949eba8{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@2a2bb0eb{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3c291aad{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@2a2bb0eb{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@2a2bb0eb{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@2a2bb0eb{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@2a2bb0eb{STARTING,min=32,inflate=-1} added {DeflaterPool@2d0566ba{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@2a2bb0eb{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@ce5a68e{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-9d157ff[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-9d157ff==org.apache.spark.ui.JettyUtils$$anon$3@4f91a6bc{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-9d157ff=org.apache.spark.ui.JettyUtils$$anon$3-9d157ff==org.apache.spark.ui.JettyUtils$$anon$3@4f91a6bc{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@ce5a68e{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2441ms ServletHandler@ce5a68e{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-9d157ff==org.apache.spark.ui.JettyUtils$$anon$3@4f91a6bc{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2442ms org.apache.spark.ui.JettyUtils$$anon$3-9d157ff==org.apache.spark.ui.JettyUtils$$anon$3@4f91a6bc{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-9d157ff
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2442ms o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2d0566ba{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2442ms DeflaterPool@2d0566ba{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2442ms GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@7728643a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@320e400{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@7728643a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@7728643a{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@7728643a{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@7728643a{STARTING,min=32,inflate=-1} added {DeflaterPool@5167268{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@7728643a{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7c041b41{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7f69d591[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7f69d591==org.apache.spark.ui.JettyUtils$$anon$3@45819e27{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7f69d591=org.apache.spark.ui.JettyUtils$$anon$3-7f69d591==org.apache.spark.ui.JettyUtils$$anon$3@45819e27{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@7c041b41{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2444ms ServletHandler@7c041b41{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7f69d591==org.apache.spark.ui.JettyUtils$$anon$3@45819e27{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2445ms org.apache.spark.ui.JettyUtils$$anon$3-7f69d591==org.apache.spark.ui.JettyUtils$$anon$3@45819e27{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7f69d591
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2445ms o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@5167268{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2445ms DeflaterPool@5167268{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2445ms GzipHandler@7728643a{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@1cfd1875{STOPPED,min=32,inflate=-1} mime types IncludeExclude@28c0b664{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1cfd1875{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5118388b{/stages,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@1cfd1875{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@1cfd1875{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1cfd1875{STARTING,min=32,inflate=-1} added {DeflaterPool@2c444798{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@1cfd1875{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5118388b{/stages,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5118388b{/stages,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@15a902e7{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7876d598[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7876d598==org.apache.spark.ui.JettyUtils$$anon$3@c5ad8a9c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7876d598=org.apache.spark.ui.JettyUtils$$anon$3-7876d598==org.apache.spark.ui.JettyUtils$$anon$3@c5ad8a9c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@15a902e7{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2449ms ServletHandler@15a902e7{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7876d598==org.apache.spark.ui.JettyUtils$$anon$3@c5ad8a9c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2449ms org.apache.spark.ui.JettyUtils$$anon$3-7876d598==org.apache.spark.ui.JettyUtils$$anon$3@c5ad8a9c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7876d598
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2449ms o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2c444798{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2450ms DeflaterPool@2c444798{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2450ms GzipHandler@1cfd1875{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@1af7f54a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6ebd78d1{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1af7f54a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@1af7f54a{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@1af7f54a{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1af7f54a{STARTING,min=32,inflate=-1} added {DeflaterPool@436390f4{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@1af7f54a{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@71104a4{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb==org.apache.spark.ui.JettyUtils$$anon$3@c4973a2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb=org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb==org.apache.spark.ui.JettyUtils$$anon$3@c4973a2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@71104a4{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2454ms ServletHandler@71104a4{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb==org.apache.spark.ui.JettyUtils$$anon$3@c4973a2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2455ms org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb==org.apache.spark.ui.JettyUtils$$anon$3@c4973a2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4985cbcb
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2455ms o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@436390f4{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2455ms DeflaterPool@436390f4{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2456ms GzipHandler@1af7f54a{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@4d157787{STOPPED,min=32,inflate=-1} mime types IncludeExclude@68ed96ca{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@4d157787{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@4d157787{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4d157787{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@4d157787{STARTING,min=32,inflate=-1} added {DeflaterPool@6d1310f6{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@4d157787{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@549621f3{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-54361a9[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-54361a9==org.apache.spark.ui.JettyUtils$$anon$3@2a2ca7fb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-54361a9=org.apache.spark.ui.JettyUtils$$anon$3-54361a9==org.apache.spark.ui.JettyUtils$$anon$3@2a2ca7fb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@549621f3{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2459ms ServletHandler@549621f3{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-54361a9==org.apache.spark.ui.JettyUtils$$anon$3@2a2ca7fb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2459ms org.apache.spark.ui.JettyUtils$$anon$3-54361a9==org.apache.spark.ui.JettyUtils$$anon$3@2a2ca7fb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-54361a9
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2460ms o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@6d1310f6{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2460ms DeflaterPool@6d1310f6{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2460ms GzipHandler@4d157787{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@50b8ae8d{STOPPED,min=32,inflate=-1} mime types IncludeExclude@255990cc{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@50b8ae8d{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@50b8ae8d{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@50b8ae8d{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@50b8ae8d{STARTING,min=32,inflate=-1} added {DeflaterPool@51c929ae{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@50b8ae8d{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@37ebc9d8{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5==org.apache.spark.ui.JettyUtils$$anon$3@eec5b48c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5=org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5==org.apache.spark.ui.JettyUtils$$anon$3@eec5b48c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@37ebc9d8{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2463ms ServletHandler@37ebc9d8{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5==org.apache.spark.ui.JettyUtils$$anon$3@eec5b48c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2464ms org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5==org.apache.spark.ui.JettyUtils$$anon$3@eec5b48c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-293bb8a5
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2464ms o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@51c929ae{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2464ms DeflaterPool@51c929ae{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2464ms GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@3c8bdd5b{STOPPED,min=32,inflate=-1} mime types IncludeExclude@29d2d081{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@3c8bdd5b{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@3c8bdd5b{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@3c8bdd5b{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@3c8bdd5b{STARTING,min=32,inflate=-1} added {DeflaterPool@40e4ea87{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@3c8bdd5b{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@72e34f77{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098==org.apache.spark.ui.JettyUtils$$anon$3@c3d05377{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098=org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098==org.apache.spark.ui.JettyUtils$$anon$3@c3d05377{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@72e34f77{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2467ms ServletHandler@72e34f77{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098==org.apache.spark.ui.JettyUtils$$anon$3@c3d05377{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2467ms org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098==org.apache.spark.ui.JettyUtils$$anon$3@c3d05377{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7bf9b098
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2468ms o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@40e4ea87{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2468ms DeflaterPool@40e4ea87{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2468ms GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@58783f6c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3a7b503d{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@58783f6c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@58783f6c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@58783f6c{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@58783f6c{STARTING,min=32,inflate=-1} added {DeflaterPool@512d92b{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@58783f6c{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@1fc0053e{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e==org.apache.spark.ui.JettyUtils$$anon$3@e9917ca9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e=org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e==org.apache.spark.ui.JettyUtils$$anon$3@e9917ca9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@1fc0053e{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2471ms ServletHandler@1fc0053e{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e==org.apache.spark.ui.JettyUtils$$anon$3@e9917ca9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2471ms org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e==org.apache.spark.ui.JettyUtils$$anon$3@e9917ca9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-290b1b2e
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2471ms o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@512d92b{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2471ms DeflaterPool@512d92b{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2471ms GzipHandler@58783f6c{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@62c5bbdc{STOPPED,min=32,inflate=-1} mime types IncludeExclude@7bdf6bb7{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@62c5bbdc{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@f9b7332{/storage,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@62c5bbdc{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@62c5bbdc{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@62c5bbdc{STARTING,min=32,inflate=-1} added {DeflaterPool@1bc53649{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@62c5bbdc{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@f9b7332{/storage,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@f9b7332{/storage,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@74cec793{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e==org.apache.spark.ui.JettyUtils$$anon$3@304e982b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e=org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e==org.apache.spark.ui.JettyUtils$$anon$3@304e982b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@74cec793{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2474ms ServletHandler@74cec793{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e==org.apache.spark.ui.JettyUtils$$anon$3@304e982b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2474ms org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e==org.apache.spark.ui.JettyUtils$$anon$3@304e982b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-6fefce9e
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2475ms o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1bc53649{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2475ms DeflaterPool@1bc53649{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2475ms GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@88d6f9b{STOPPED,min=32,inflate=-1} mime types IncludeExclude@47d93e0d{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@88d6f9b{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@88d6f9b{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@88d6f9b{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@88d6f9b{STARTING,min=32,inflate=-1} added {DeflaterPool@475b7792{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@88d6f9b{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@192f2f27{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-8a589a2[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-8a589a2==org.apache.spark.ui.JettyUtils$$anon$3@15c6a372{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-8a589a2=org.apache.spark.ui.JettyUtils$$anon$3-8a589a2==org.apache.spark.ui.JettyUtils$$anon$3@15c6a372{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@192f2f27{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2477ms ServletHandler@192f2f27{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-8a589a2==org.apache.spark.ui.JettyUtils$$anon$3@15c6a372{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2478ms org.apache.spark.ui.JettyUtils$$anon$3-8a589a2==org.apache.spark.ui.JettyUtils$$anon$3@15c6a372{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-8a589a2
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2478ms o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@475b7792{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2478ms DeflaterPool@475b7792{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2478ms GzipHandler@88d6f9b{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@751e664e{STOPPED,min=32,inflate=-1} mime types IncludeExclude@160c3ec1{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@751e664e{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@751e664e{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@751e664e{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@751e664e{STARTING,min=32,inflate=-1} added {DeflaterPool@182b435b{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@751e664e{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2fab4aff{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-ec0c838[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-ec0c838==org.apache.spark.ui.JettyUtils$$anon$3@6575cc40{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-ec0c838=org.apache.spark.ui.JettyUtils$$anon$3-ec0c838==org.apache.spark.ui.JettyUtils$$anon$3@6575cc40{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@2fab4aff{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2481ms ServletHandler@2fab4aff{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-ec0c838==org.apache.spark.ui.JettyUtils$$anon$3@6575cc40{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2481ms org.apache.spark.ui.JettyUtils$$anon$3-ec0c838==org.apache.spark.ui.JettyUtils$$anon$3@6575cc40{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-ec0c838
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2481ms o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@182b435b{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2481ms DeflaterPool@182b435b{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2481ms GzipHandler@751e664e{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@4d0402b{STOPPED,min=32,inflate=-1} mime types IncludeExclude@2fa7ae9{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@4d0402b{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@4d0402b{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4d0402b{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@4d0402b{STARTING,min=32,inflate=-1} added {DeflaterPool@7577b641{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@4d0402b{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@29cfd92b{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-21c64522[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-21c64522==org.apache.spark.ui.JettyUtils$$anon$3@96fb741f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-21c64522=org.apache.spark.ui.JettyUtils$$anon$3-21c64522==org.apache.spark.ui.JettyUtils$$anon$3@96fb741f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@29cfd92b{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2484ms ServletHandler@29cfd92b{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-21c64522==org.apache.spark.ui.JettyUtils$$anon$3@96fb741f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2484ms org.apache.spark.ui.JettyUtils$$anon$3-21c64522==org.apache.spark.ui.JettyUtils$$anon$3@96fb741f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-21c64522
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2484ms o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@7577b641{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2485ms DeflaterPool@7577b641{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2485ms GzipHandler@4d0402b{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@3704122f{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3153ddfc{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@3704122f{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@11acdc30{/environment,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@3704122f{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@3704122f{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@3704122f{STARTING,min=32,inflate=-1} added {DeflaterPool@60afd40d{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@3704122f{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@11acdc30{/environment,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@11acdc30{/environment,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@770d4269{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068==org.apache.spark.ui.JettyUtils$$anon$3@afb50d2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068=org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068==org.apache.spark.ui.JettyUtils$$anon$3@afb50d2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@770d4269{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2487ms ServletHandler@770d4269{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068==org.apache.spark.ui.JettyUtils$$anon$3@afb50d2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2487ms org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068==org.apache.spark.ui.JettyUtils$$anon$3@afb50d2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4a8ab068
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2488ms o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@60afd40d{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2488ms DeflaterPool@60afd40d{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2488ms GzipHandler@3704122f{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@28a2a3e7{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3f2049b6{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@28a2a3e7{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@28a2a3e7{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@28a2a3e7{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@28a2a3e7{STARTING,min=32,inflate=-1} added {DeflaterPool@10b3df93{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@28a2a3e7{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6bab2585{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-74bdc168[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-74bdc168==org.apache.spark.ui.JettyUtils$$anon$3@3dfcb5bb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-74bdc168=org.apache.spark.ui.JettyUtils$$anon$3-74bdc168==org.apache.spark.ui.JettyUtils$$anon$3@3dfcb5bb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@6bab2585{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2493ms ServletHandler@6bab2585{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-74bdc168==org.apache.spark.ui.JettyUtils$$anon$3@3dfcb5bb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2493ms org.apache.spark.ui.JettyUtils$$anon$3-74bdc168==org.apache.spark.ui.JettyUtils$$anon$3@3dfcb5bb{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-74bdc168
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2494ms o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@10b3df93{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2494ms DeflaterPool@10b3df93{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2494ms GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@ea27e34{STOPPED,min=32,inflate=-1} mime types IncludeExclude@33a2499c{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@ea27e34{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@62923ee6{/executors,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@ea27e34{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@ea27e34{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@ea27e34{STARTING,min=32,inflate=-1} added {DeflaterPool@e72dba7{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@ea27e34{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@62923ee6{/executors,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@62923ee6{/executors,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4089713{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2==org.apache.spark.ui.JettyUtils$$anon$3@458895a6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2=org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2==org.apache.spark.ui.JettyUtils$$anon$3@458895a6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@4089713{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2499ms ServletHandler@4089713{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2==org.apache.spark.ui.JettyUtils$$anon$3@458895a6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2500ms org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2==org.apache.spark.ui.JettyUtils$$anon$3@458895a6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-f19c9d2
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2500ms o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@e72dba7{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2500ms DeflaterPool@e72dba7{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2500ms GzipHandler@ea27e34{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@33c2bd{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1dfd5f51{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@33c2bd{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@33c2bd{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@33c2bd{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@33c2bd{STARTING,min=32,inflate=-1} added {DeflaterPool@3c321bdb{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@33c2bd{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4b6166aa{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-a77614d[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-a77614d==org.apache.spark.ui.JettyUtils$$anon$3@4715b9c0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-a77614d=org.apache.spark.ui.JettyUtils$$anon$3-a77614d==org.apache.spark.ui.JettyUtils$$anon$3@4715b9c0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@4b6166aa{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2505ms ServletHandler@4b6166aa{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-a77614d==org.apache.spark.ui.JettyUtils$$anon$3@4715b9c0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2506ms org.apache.spark.ui.JettyUtils$$anon$3-a77614d==org.apache.spark.ui.JettyUtils$$anon$3@4715b9c0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-a77614d
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2506ms o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@3c321bdb{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2506ms DeflaterPool@3c321bdb{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2506ms GzipHandler@33c2bd{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@24855019{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3abd581e{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@24855019{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@24855019{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@24855019{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@24855019{STARTING,min=32,inflate=-1} added {DeflaterPool@4d4d8fcf{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@24855019{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3bde62ff{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-523424b5[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-523424b5==org.apache.spark.ui.JettyUtils$$anon$3@b17f827f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-523424b5=org.apache.spark.ui.JettyUtils$$anon$3-523424b5==org.apache.spark.ui.JettyUtils$$anon$3@b17f827f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@3bde62ff{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2511ms ServletHandler@3bde62ff{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-523424b5==org.apache.spark.ui.JettyUtils$$anon$3@b17f827f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2511ms org.apache.spark.ui.JettyUtils$$anon$3-523424b5==org.apache.spark.ui.JettyUtils$$anon$3@b17f827f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-523424b5
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2512ms o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@4d4d8fcf{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2512ms DeflaterPool@4d4d8fcf{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2512ms GzipHandler@24855019{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@610db97e{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6f0628de{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@610db97e{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@610db97e{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@610db97e{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@610db97e{STARTING,min=32,inflate=-1} added {DeflaterPool@3fabf088{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@610db97e{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@791cbf87{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d==org.apache.spark.ui.JettyUtils$$anon$3@499ce180{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d=org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d==org.apache.spark.ui.JettyUtils$$anon$3@499ce180{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@791cbf87{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2517ms ServletHandler@791cbf87{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d==org.apache.spark.ui.JettyUtils$$anon$3@499ce180{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2517ms org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d==org.apache.spark.ui.JettyUtils$$anon$3@499ce180{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-a7e2d9d
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2518ms o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@3fabf088{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2518ms DeflaterPool@3fabf088{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2518ms GzipHandler@610db97e{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@1e392345{STOPPED,min=32,inflate=-1} mime types IncludeExclude@12f3afb5{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1e392345{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@372ea2bc{/static,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@1e392345{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@1e392345{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@1e392345{STARTING,min=32,inflate=-1} added {DeflaterPool@4ced35ed{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@1e392345{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@372ea2bc{/static,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@372ea2bc{/static,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4cc76301{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-cf65451[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-cf65451==org.spark_project.jetty.servlet.DefaultServlet@dcc00e3f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-cf65451=org.spark_project.jetty.servlet.DefaultServlet-cf65451==org.spark_project.jetty.servlet.DefaultServlet@dcc00e3f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@4cc76301{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2523ms ServletHandler@4cc76301{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-cf65451==org.spark_project.jetty.servlet.DefaultServlet@dcc00e3f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2524ms org.spark_project.jetty.servlet.DefaultServlet-cf65451==org.spark_project.jetty.servlet.DefaultServlet@dcc00e3f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-cf65451
2022-02-10 15:13:36 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-core_2.11/2.4.8/spark-core_2.11-2.4.8.jar!/org/apache/spark/ui/static
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2532ms o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@4ced35ed{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2532ms DeflaterPool@4ced35ed{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2532ms GzipHandler@1e392345{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@323e8306{STOPPED,min=32,inflate=-1} mime types IncludeExclude@a23a01d{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@323e8306{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@c9413d8{/,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@323e8306{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@323e8306{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@323e8306{STARTING,min=32,inflate=-1} added {DeflaterPool@4acf72b6{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@323e8306{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@c9413d8{/,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@c9413d8{/,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@64da2a7{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-46074492[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-46074492==org.apache.spark.ui.JettyUtils$$anon$4@c8d542d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-46074492=org.apache.spark.ui.JettyUtils$$anon$4-46074492==org.apache.spark.ui.JettyUtils$$anon$4@c8d542d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@64da2a7{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2537ms ServletHandler@64da2a7{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-46074492==org.apache.spark.ui.JettyUtils$$anon$4@c8d542d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2537ms org.apache.spark.ui.JettyUtils$$anon$4-46074492==org.apache.spark.ui.JettyUtils$$anon$4@c8d542d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-46074492
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2537ms o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@4acf72b6{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2538ms DeflaterPool@4acf72b6{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2538ms GzipHandler@323e8306{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@7561db12{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3301500b{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@7561db12{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@3b9d6699{/api,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@7561db12{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@7561db12{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@7561db12{STARTING,min=32,inflate=-1} added {DeflaterPool@24b52d3e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@7561db12{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3b9d6699{/api,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3b9d6699{/api,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7caa550{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-30ed9c6c=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG ServletHandler:169 - Adding Default404Servlet to ServletHandler@7caa550{STARTING}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7caa550{STARTING} added {org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@7caa550{STARTING} added {[/]=>org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee,POJO}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-30ed9c6c=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-30ed9c6c=org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@7caa550{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2544ms ServletHandler@7caa550{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2545ms org.glassfish.jersey.servlet.ServletContainer-30ed9c6c==org.glassfish.jersey.servlet.ServletContainer@27f7a201{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2545ms org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-57a4d5ee==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@1b230aca{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2545ms o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@24b52d3e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2545ms DeflaterPool@24b52d3e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2545ms GzipHandler@7561db12{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@5af5def9{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3a45c42a{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@5af5def9{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@5af5def9{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5af5def9{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@5af5def9{STARTING,min=32,inflate=-1} added {DeflaterPool@36dce7ed{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@5af5def9{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6063d80a{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e==org.apache.spark.ui.JettyUtils$$anon$4@d167c798{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e=org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e==org.apache.spark.ui.JettyUtils$$anon$4@d167c798{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@6063d80a{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2549ms ServletHandler@6063d80a{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e==org.apache.spark.ui.JettyUtils$$anon$4@d167c798{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2549ms org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e==org.apache.spark.ui.JettyUtils$$anon$4@d167c798{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-1133ec6e
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2549ms o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@36dce7ed{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2549ms DeflaterPool@36dce7ed{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2549ms GzipHandler@5af5def9{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG GzipHandler:208 - GzipHandler@47a64f7d{STOPPED,min=32,inflate=-1} mime types IncludeExclude@33d05366{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@47a64f7d{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {GzipHandler@47a64f7d{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting GzipHandler@47a64f7d{STOPPED,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - GzipHandler@47a64f7d{STARTING,min=32,inflate=-1} added {DeflaterPool@27a0a5a2{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting GzipHandler@47a64f7d{STARTING,min=32,inflate=-1}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@60cf80e7{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-302fec27[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-302fec27==org.apache.spark.ui.JettyUtils$$anon$4@6d5dbb7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-302fec27=org.apache.spark.ui.JettyUtils$$anon$4-302fec27==org.apache.spark.ui.JettyUtils$$anon$4@6d5dbb7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@60cf80e7{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2554ms ServletHandler@60cf80e7{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-302fec27==org.apache.spark.ui.JettyUtils$$anon$4@6d5dbb7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2555ms org.apache.spark.ui.JettyUtils$$anon$4-302fec27==org.apache.spark.ui.JettyUtils$$anon$4@6d5dbb7f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-302fec27
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2555ms o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@27a0a5a2{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2555ms DeflaterPool@27a0a5a2{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2555ms GzipHandler@47a64f7d{STARTED,min=32,inflate=-1}
2022-02-10 15:13:36 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://Clairvoyant-324.mshome.net:4040
2022-02-10 15:13:36 INFO  Executor:54 - Starting executor ID driver on host localhost
2022-02-10 15:13:36 DEBUG TransportServer:141 - Shuffle server started on port: 58899
2022-02-10 15:13:36 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58899.
2022-02-10 15:13:36 INFO  NettyBlockTransferService:54 - Server created on Clairvoyant-324.mshome.net:58899
2022-02-10 15:13:36 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2022-02-10 15:13:36 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 58899, None)
2022-02-10 15:13:36 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 15:13:36 INFO  BlockManagerMasterEndpoint:54 - Registering block manager Clairvoyant-324.mshome.net:58899 with 1970.4 MB RAM, BlockManagerId(driver, Clairvoyant-324.mshome.net, 58899, None)
2022-02-10 15:13:36 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 58899, None)
2022-02-10 15:13:36 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, Clairvoyant-324.mshome.net, 58899, None)
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2b0b4d53
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@7068f7ca{/,null,STOPPED} added {ServletHandler@38548b19{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@38548b19{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa==org.apache.spark.ui.JettyUtils$$anon$3@65c90bc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@38548b19{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa,POJO}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@38548b19{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa==org.apache.spark.ui.JettyUtils$$anon$3@65c90bc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa=org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa==org.apache.spark.ui.JettyUtils$$anon$3@65c90bc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@38548b19{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2841ms ServletHandler@38548b19{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa==org.apache.spark.ui.JettyUtils$$anon$3@65c90bc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2842ms org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa==org.apache.spark.ui.JettyUtils$$anon$3@65c90bc8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-41aaedaa
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2842ms o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG SparkContext:58 - Adding shutdown hook
2022-02-10 15:13:36 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse').
2022-02-10 15:13:36 INFO  SharedState:54 - Warehouse path is 'file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse'.
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1436a7ab
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3b7b05a8{/,null,STOPPED} added {ServletHandler@3d36dff4{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3d36dff4{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf==org.apache.spark.ui.JettyUtils$$anon$3@8a6d0d82{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3d36dff4{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@5b94ccbc
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@38a1c423{/,null,STOPPED} added {ServletHandler@336365bc{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@336365bc{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0==org.apache.spark.ui.JettyUtils$$anon$3@76fdd030{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@336365bc{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0,POJO}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3d36dff4{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf==org.apache.spark.ui.JettyUtils$$anon$3@8a6d0d82{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf=org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf==org.apache.spark.ui.JettyUtils$$anon$3@8a6d0d82{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@3d36dff4{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2971ms ServletHandler@3d36dff4{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf==org.apache.spark.ui.JettyUtils$$anon$3@8a6d0d82{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2971ms org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf==org.apache.spark.ui.JettyUtils$$anon$3@8a6d0d82{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7abe27bf
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2972ms o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@336365bc{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0==org.apache.spark.ui.JettyUtils$$anon$3@76fdd030{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0=org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0==org.apache.spark.ui.JettyUtils$$anon$3@76fdd030{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@336365bc{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2979ms ServletHandler@336365bc{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0==org.apache.spark.ui.JettyUtils$$anon$3@76fdd030{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2979ms org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0==org.apache.spark.ui.JettyUtils$$anon$3@76fdd030{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-38eb2fb0
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2980ms o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7351a16e
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5bb7643d{/,null,STOPPED} added {ServletHandler@3ac04654{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3ac04654{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4074023c==org.apache.spark.ui.JettyUtils$$anon$3@e6a44bd7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@3ac04654{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4074023c,POJO}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3ed0918d
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5e268ce6{/,null,STOPPED} added {ServletHandler@66ec9390{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@66ec9390{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-6e91893==org.apache.spark.ui.JettyUtils$$anon$3@b302b63f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@66ec9390{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6e91893,POJO}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3ac04654{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4074023c[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4074023c==org.apache.spark.ui.JettyUtils$$anon$3@e6a44bd7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4074023c=org.apache.spark.ui.JettyUtils$$anon$3-4074023c==org.apache.spark.ui.JettyUtils$$anon$3@e6a44bd7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@3ac04654{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2988ms ServletHandler@3ac04654{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4074023c==org.apache.spark.ui.JettyUtils$$anon$3@e6a44bd7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2989ms org.apache.spark.ui.JettyUtils$$anon$3-4074023c==org.apache.spark.ui.JettyUtils$$anon$3@e6a44bd7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4074023c
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2989ms o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@66ec9390{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6e91893[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-6e91893==org.apache.spark.ui.JettyUtils$$anon$3@b302b63f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6e91893=org.apache.spark.ui.JettyUtils$$anon$3-6e91893==org.apache.spark.ui.JettyUtils$$anon$3@b302b63f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@66ec9390{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2995ms ServletHandler@66ec9390{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-6e91893==org.apache.spark.ui.JettyUtils$$anon$3@b302b63f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2995ms org.apache.spark.ui.JettyUtils$$anon$3-6e91893==org.apache.spark.ui.JettyUtils$$anon$3@b302b63f{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-6e91893
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @2995ms o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1a35993f
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5b12012e{/,null,STOPPED} added {ServletHandler@2f7dcef2{STOPPED},MANAGED}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@2f7dcef2{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5==org.spark_project.jetty.servlet.DefaultServlet@9ddd3c30{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ServletHandler@2f7dcef2{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5,POJO}
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@323e8306{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@c9413d8{/,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@751e664e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b672aa8{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@62c5bbdc{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@f9b7332{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@4d0402b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5cc69cfe{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@5e268ce6{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7561db12{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3b9d6699{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@58783f6c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@77307458{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3c8bdd5b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6e9319f{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@5949eba8{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6c6357f9{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1e392345{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@372ea2bc{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@33c2bd{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@b91d8c4{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@50b8ae8d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5217f3d0{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@610db97e{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@319dead1{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@28a2a3e7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@76a82f33{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@7728643a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5df417a7{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@22680f52{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@66629f63{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@1af7f54a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5af28b27{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@4d157787{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@332a7fce{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@88d6f9b{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1bdf8190{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3b7b05a8{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - static/sql->[{o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,STOPPED,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@47a64f7d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@2a2bb0eb{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5d25e6bb{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@3704122f{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@11acdc30{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@1cfd1875{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5118388b{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@ea27e34{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@62923ee6{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@38a1c423{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5af5def9{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5ae76500{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@7068f7ca{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@5bb7643d{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@24855019{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@a1217f9{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 15:13:36 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@560cbf1a{STARTED} added {o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,STOPPED,@Spark}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,STARTING,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2f7dcef2{STOPPED}
2022-02-10 15:13:36 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5[EMBEDDED:null]
2022-02-10 15:13:36 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5==org.spark_project.jetty.servlet.DefaultServlet@9ddd3c30{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 15:13:36 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 15:13:36 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 15:13:36 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 15:13:36 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5=org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5==org.spark_project.jetty.servlet.DefaultServlet@9ddd3c30{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 15:13:36 DEBUG AbstractHandler:94 - starting ServletHandler@2f7dcef2{STARTING}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @3002ms ServletHandler@2f7dcef2{STARTED}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5==org.spark_project.jetty.servlet.DefaultServlet@9ddd3c30{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @3003ms org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5==org.spark_project.jetty.servlet.DefaultServlet@9ddd3c30{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 15:13:36 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-4a3be6a5
2022-02-10 15:13:36 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-sql_2.11/2.4.8/spark-sql_2.11-2.4.8.jar!/org/apache/spark/sql/execution/ui/static
2022-02-10 15:13:36 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 15:13:36 DEBUG AbstractLifeCycle:191 - STARTED @3023ms o.s.j.s.ServletContextHandler@5b12012e{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 15:13:37 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2022-02-10 15:13:37 INFO  InMemoryFileIndex:54 - It took 22 ms to list leaf files for 1 paths.
2022-02-10 15:13:37 INFO  InMemoryFileIndex:54 - It took 3 ms to list leaf files for 2 paths.
2022-02-10 15:13:38 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#0]
 +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#4: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#4: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 15:13:38 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#0, None)) > 0)
 +- Project [value#0]                       +- Project [value#0]
    +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#5: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#5: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#6: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#6: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 15:13:38 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                    GlobalLimit 1
 +- LocalLimit 1                                  +- LocalLimit 1
    +- Filter (length(trim(value#0, None)) > 0)      +- Filter (length(trim(value#0, None)) > 0)
!      +- Project [value#0]                             +- Relation[value#0] text
!         +- Relation[value#0] text               
          
2022-02-10 15:13:38 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:38 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)
2022-02-10 15:13:38 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:38 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:38 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:38 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:38 INFO  CodeGenerator:54 - Code generated in 160.1263 ms
2022-02-10 15:13:39 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 18.4551 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 221.9 KB, free 1970.2 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_0 locally took  31 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_0 without replication took  31 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1970.2 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_0_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_0_piece0 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_0_piece0 without replication took  0 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 0 from load at UseCase5.java:32
2022-02-10 15:13:39 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8737102 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:39 INFO  SparkContext:54 - Starting job: load at UseCase5.java:32
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Got job 0 (load at UseCase5.java:32) with 1 output partitions
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (load at UseCase5.java:32)
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitStage(ResultStage 0 (name=load at UseCase5.java:32;jobs=0))
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at UseCase5.java:32), which has no missing parents
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 0)
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 1970.2 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_1 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_1 without replication took  0 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1970.1 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.4 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_1_piece0 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_1_piece0 without replication took  0 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at UseCase5.java:32) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:39 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Epoch for TaskSet 0.0: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 15:13:39 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 15:13:39 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes)
2022-02-10 15:13:39 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_1
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:39 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:39 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 9.947 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:39 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1234 bytes result sent to driver
2022-02-10 15:13:39 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:39 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 111 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:39 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2022-02-10 15:13:39 INFO  DAGScheduler:54 - ResultStage 0 (load at UseCase5.java:32) finished in 0.191 s
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - After removal of stage 0, remaining stages = 0
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Job 0 finished: load at UseCase5.java:32, took 0.225079 s
2022-02-10 15:13:39 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#8: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String
 +- Project [value#0]                                                                                                                                                     +- Project [value#0]
    +- Relation[value#0] text                                                                                                                                                +- Relation[value#0] text
          
2022-02-10 15:13:39 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String   DeserializeToObject value#0.toString, obj#8: java.lang.String
!+- Project [value#0]                                                            +- Relation[value#0] text
!   +- Relation[value#0] text                                                    
          
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:39 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:39 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 6.6548 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 221.9 KB, free 1969.9 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_2 locally took  16 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_2 without replication took  16 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.9 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_2_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_2_piece0 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_2_piece0 without replication took  0 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 2 from load at UseCase5.java:32
2022-02-10 15:13:39 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8737102 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:39 INFO  SparkContext:54 - Starting job: load at UseCase5.java:32
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Got job 1 (load at UseCase5.java:32) with 1 output partitions
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (load at UseCase5.java:32)
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitStage(ResultStage 1 (name=load at UseCase5.java:32;jobs=1))
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at UseCase5.java:32), which has no missing parents
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 1)
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 14.0 KB, free 1969.9 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_3 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_3 without replication took  0 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.9 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_3_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_3_piece0 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_3_piece0 without replication took  0 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at UseCase5.java:32) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:39 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Epoch for TaskSet 1.0: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
2022-02-10 15:13:39 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 15:13:39 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes)
2022-02-10 15:13:39 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_3
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:39 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:39 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:39 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_2
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:39 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:39 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1559 bytes result sent to driver
2022-02-10 15:13:39 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:39 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 79 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:39 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2022-02-10 15:13:39 INFO  DAGScheduler:54 - ResultStage 1 (load at UseCase5.java:32) finished in 0.079 s
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - After removal of stage 1, remaining stages = 0
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Job 1 finished: load at UseCase5.java:32, took 0.096920 s
2022-02-10 15:13:39 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#29L]                                                                                                Aggregate [count(1) AS count#29L]
 +- Relation[product_id#10,product_category_id#11,product_name#12,product_description#13,product_price#14,product_image#15] csv   +- Relation[product_id#10,product_category_id#11,product_name#12,product_description#13,product_price#14,product_image#15] csv
          
2022-02-10 15:13:39 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#29L]                                                                                                Aggregate [count(1) AS count#29L]
!+- Relation[product_id#10,product_category_id#11,product_name#12,product_description#13,product_price#14,product_image#15] csv   +- Project
!                                                                                                                                    +- Relation[product_id#10,product_category_id#11,product_name#12,product_description#13,product_price#14,product_image#15] csv
          
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:39 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>
2022-02-10 15:13:39 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:39 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 8.7905 ms
2022-02-10 15:13:39 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 10.1193 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 221.8 KB, free 1969.7 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_4 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_4 without replication took  0 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.7 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_4_piece0 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_4_piece0 without replication took  0 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 4 from count at UseCase5.java:47
2022-02-10 15:13:39 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4368551 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[15] at count at UseCase5.java:47
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[15] at count at UseCase5.java:47)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[15] at count at UseCase5.java:47
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[15] at count at UseCase5.java:47)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:39 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:39 INFO  SparkContext:54 - Starting job: count at UseCase5.java:47
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Registering RDD 12 (count at UseCase5.java:47) as input to shuffle 0
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Got job 2 (count at UseCase5.java:47) with 1 output partitions
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (count at UseCase5.java:47)
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 2)
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 2)
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitStage(ResultStage 3 (name=count at UseCase5.java:47;jobs=2))
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 2)
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 2 (name=count at UseCase5.java:47;jobs=2))
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 2 (MapPartitionsRDD[12] at count at UseCase5.java:47), which has no missing parents
2022-02-10 15:13:39 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 2)
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 12.8 KB, free 1969.6 MB)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_5 locally took  0 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_5 without replication took  0 ms
2022-02-10 15:13:39 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.8 KB, free 1969.6 MB)
2022-02-10 15:13:39 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 6.8 KB, free: 1970.3 MB)
2022-02-10 15:13:39 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 15:13:39 DEBUG BlockManager:58 - Put block broadcast_5_piece0 locally took  16 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Putting block broadcast_5_piece0 without replication took  16 ms
2022-02-10 15:13:39 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:39 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[12] at count at UseCase5.java:47) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:39 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Epoch for TaskSet 2.0: 0
2022-02-10 15:13:39 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 2.0: NO_PREF, ANY
2022-02-10 15:13:39 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 15:13:39 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8309 bytes)
2022-02-10 15:13:39 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 2)
2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_5
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:39 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:39 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:39 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:39 INFO  CodeGenerator:54 - Code generated in 6.8982 ms
2022-02-10 15:13:39 DEBUG BlockManager:58 - Getting local block broadcast_4
2022-02-10 15:13:39 DEBUG BlockManager:58 - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 2). 1516 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 63 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ShuffleMapStage 2 (count at UseCase5.java:47) finished in 0.079 s
2022-02-10 15:13:40 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:40 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - waiting: Set(ResultStage 3)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:40 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 1
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 3 (name=count at UseCase5.java:47;jobs=2))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[15] at count at UseCase5.java:47), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 3)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 7.3 KB, free 1969.6 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_6 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_6 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1969.6 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_6_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_6_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at count at UseCase5.java:47) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 3.0: 1
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 3.0: ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7767 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_6
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 0, partitions 0-1
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:40 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:40 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_0_0_0
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 1696 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 32 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 3 (count at UseCase5.java:47) finished in 0.032 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 2, remaining stages = 1
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 3, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 2 finished: count at UseCase5.java:47, took 0.133678 s
2022-02-10 15:13:40 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#33
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#33]
 +- Relation[value#33] text                 +- Relation[value#33] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#37: java.lang.String   DeserializeToObject cast(value#33 as string).toString, obj#37: java.lang.String
 +- LocalRelation <empty>, [value#33]                                                                                                                                      +- LocalRelation <empty>, [value#33]
          
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#33
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#33, None)) > 0)
 +- Project [value#33]                      +- Project [value#33]
    +- Relation[value#33] text                 +- Relation[value#33] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#38: java.lang.String   DeserializeToObject cast(value#33 as string).toString, obj#38: java.lang.String
 +- LocalRelation <empty>, [value#33]                                                                                                                                      +- LocalRelation <empty>, [value#33]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#39: java.lang.String   DeserializeToObject cast(value#33 as string).toString, obj#39: java.lang.String
 +- LocalRelation <empty>, [value#33]                                                                                                                                      +- LocalRelation <empty>, [value#33]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#33, None)) > 0)      +- Filter (length(trim(value#33, None)) > 0)
!      +- Project [value#33]                             +- Relation[value#33] text
!         +- Relation[value#33] text               
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#33, None)) > 0)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_7 locally took  16 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_7 without replication took  16 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_7_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_7_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 7 from load at UseCase5.java:26
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8390764 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:26
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 3 (load at UseCase5.java:26) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (load at UseCase5.java:26)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 4 (name=load at UseCase5.java:26;jobs=3))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 4 (MapPartitionsRDD[19] at load at UseCase5.java:26), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 4)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_8 locally took  16 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_8 without replication took  16 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_8_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_8_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at load at UseCase5.java:26) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 4.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 4.0: 1
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8354 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 4)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_8
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_7
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 4). 1119 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 4 (load at UseCase5.java:26) finished in 0.032 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 4, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 3 finished: load at UseCase5.java:26, took 0.023014 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#41: java.lang.String   DeserializeToObject cast(value#33 as string).toString, obj#41: java.lang.String
 +- Project [value#33]                                                                                                                                                     +- Project [value#33]
    +- Relation[value#33] text                                                                                                                                                +- Relation[value#33] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#33 as string).toString, obj#41: java.lang.String   DeserializeToObject value#33.toString, obj#41: java.lang.String
!+- Project [value#33]                                                             +- Relation[value#33] text
!   +- Relation[value#33] text                                                     
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_9 locally took  16 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_9 without replication took  16 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.1 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_9_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_9_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 9 from load at UseCase5.java:26
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8390764 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:26
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 4 (load at UseCase5.java:26) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (load at UseCase5.java:26)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 5 (name=load at UseCase5.java:26;jobs=4))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[25] at load at UseCase5.java:26), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 5)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 13.9 KB, free 1969.1 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_10 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_10 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1969.1 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_10_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_10_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at load at UseCase5.java:26) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 5.0: 1
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 5.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8354 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 5.0 (TID 5)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_10
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_10 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_9
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_9 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 5.0 (TID 5). 1443 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 5 (load at UseCase5.java:26) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 5, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 4 finished: load at UseCase5.java:26, took 0.024067 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#53L]                                            Aggregate [count(1) AS count#53L]
 +- Relation[category_id#43,category_department_id#44,category_name#45] csv   +- Relation[category_id#43,category_department_id#44,category_name#45] csv
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#53L]                                            Aggregate [count(1) AS count#53L]
!+- Relation[category_id#43,category_department_id#44,category_name#45] csv   +- Project
!                                                                                +- Relation[category_id#43,category_department_id#44,category_name#45] csv
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 221.8 KB, free 1968.9 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_11 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_11 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_11_piece0 locally took  16 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_11_piece0 without replication took  16 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 11 from count at UseCase5.java:42
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4195382 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[31] at count at UseCase5.java:42
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[31] at count at UseCase5.java:42)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[31] at count at UseCase5.java:42
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[31] at count at UseCase5.java:42)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: count at UseCase5.java:42
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Registering RDD 28 (count at UseCase5.java:42) as input to shuffle 1
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 5 (count at UseCase5.java:42) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (count at UseCase5.java:42)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 6)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 6)
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 7 (name=count at UseCase5.java:42;jobs=5))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 6)
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 6 (name=count at UseCase5.java:42;jobs=5))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 6 (MapPartitionsRDD[28] at count at UseCase5.java:42), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 6)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 12.6 KB, free 1968.9 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_12 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_12 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.8 KB, free 1968.9 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 6.8 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_12_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_12_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(125)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[28] at count at UseCase5.java:42) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 6.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 6.0: 1
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 125
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 6.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 125
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(116)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 116
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 116
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8311 bytes)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(150)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 150
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 150
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(163)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 163
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 6.0 (TID 6)
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 163
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(6)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 6
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 6
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(1)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 1
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_12
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 1
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_12 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_11
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_11 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 1
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 1
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_1
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_1 of size 9144 dropped from memory (free 2064510057)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_1_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_1_piece0 of size 4741 dropped from memory (free 2064514798)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 1, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 1
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(7)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 7
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 7
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(78)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 78
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 78
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(9)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 9
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 9
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 9
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 9
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_9_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_9_piece0 of size 21170 dropped from memory (free 2064535968)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_9
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_9 of size 227232 dropped from memory (free 2064763200)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 6.0 (TID 6). 1602 bytes result sent to driver
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 9, response is 0
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 34 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 9
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(52)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 52
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 52
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(91)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 91
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 91
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(84)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 84
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 84
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(18)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 18
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 18
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(108)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 108
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 108
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(128)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 128
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 128
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(167)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 167
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 167
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ShuffleMapStage 6 (count at UseCase5.java:42) finished in 0.050 s
2022-02-10 15:13:40 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:40 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - waiting: Set(ResultStage 7)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:40 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 2
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(146)
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 7 (name=count at UseCase5.java:42;jobs=5))
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 146
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 146
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(140)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 7 (MapPartitionsRDD[31] at count at UseCase5.java:42), which has no missing parents
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 140
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 7)
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 140
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(176)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 176
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 176
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(136)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 136
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 136
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(135)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 135
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 135
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(51)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 51
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 51
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(61)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 61
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 61
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(142)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 142
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 142
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(165)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 165
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 165
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(54)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 54
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 54
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(117)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 117
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 117
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(186)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 186
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 186
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(141)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 141
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 141
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(28)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 28
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 28
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(65)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 7.3 KB, free 1969.1 MB)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 65
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 65
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(6)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 6
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 6
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 6
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 6
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_6_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_13 locally took  0 ms
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_6_piece0 of size 4011 dropped from memory (free 2064759779)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_13 without replication took  0 ms
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_6
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_6 of size 7432 dropped from memory (free 2064767211)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1969.1 MB)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 6, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 6
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(7)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 7
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 7
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 7
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 7
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_7
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_7 of size 227232 dropped from memory (free 2064990435)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_7_piece0
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_7_piece0 of size 21170 dropped from memory (free 2065011605)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_13_piece0 locally took  8 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_13_piece0 without replication took  8 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at count at UseCase5.java:42) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 7.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 7.0: 2
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 7, response is 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 7.0: ANY
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 7
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(72)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 72
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 72
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(127)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 127
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 127
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(56)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 56
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 56
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(160)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 160
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, ANY, 7767 bytes)
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 160
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(148)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 148
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 148
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 7.0 (TID 7)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(39)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 39
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 39
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(66)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 66
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 66
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(177)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 177
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 177
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(182)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 182
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 182
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(8)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 8
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 8
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(10)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 10
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 10
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_13
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_13 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 10
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 10
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_10
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_10 of size 14256 dropped from memory (free 2065025861)
2022-02-10 15:13:40 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 1, partitions 0-1
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_10_piece0
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_10_piece0 of size 7722 dropped from memory (free 2065033583)
2022-02-10 15:13:40 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:40 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_1_0_0
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 15:13:40 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 10, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 7.0 (TID 7). 1696 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 10
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(170)
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 6 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 170
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 170
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(87)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 87
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 87
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(132)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 132
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 132
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(137)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 137
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 137
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(171)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 171
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 171
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(178)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 178
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 178
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(145)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 145
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 145
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(156)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 156
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 156
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 7 (count at UseCase5.java:42) finished in 0.019 s
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(115)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 115
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 115
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(154)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 154
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 154
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(50)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 50
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 50
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(134)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 134
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 7, remaining stages = 1
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 134
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 6, remaining stages = 0
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(36)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 36
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 36
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(67)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 67
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 5 finished: count at UseCase5.java:42, took 0.066515 s
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 67
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(124)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 124
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 124
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(179)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 179
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 179
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(82)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 82
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 82
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(126)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 126
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 126
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(155)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 155
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 155
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(26)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 26
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 26
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(73)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 73
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 73
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(81)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 81
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 81
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(3)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 3
2022-02-10 15:13:40 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 3
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 3
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 3
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_3_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_3_piece0 of size 7747 dropped from memory (free 2065041330)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_3_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_3
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_3 of size 14312 dropped from memory (free 2065055642)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 3, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 3
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(113)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 113
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 113
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(55)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 55
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 55
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(21)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 21
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 21
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(180)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 180
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 180
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(83)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 83
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 83
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(46)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 46
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 46
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(59)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 59
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 59
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(121)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 121
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 121
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(173)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 173
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 173
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(47)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 47
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 47
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(168)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 168
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 168
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(60)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 60
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 60
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(88)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 88
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 88
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(162)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 162
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 162
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(101)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 101
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 101
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(20)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 20
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 20
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(131)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 131
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 131
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(174)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 174
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 174
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(164)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 164
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 164
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(68)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 68
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 68
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(153)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 153
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 153
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(38)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 38
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 38
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(166)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 166
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 166
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(147)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 147
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 147
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(189)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 189
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 189
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(70)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 70
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 70
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(114)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 114
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 114
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(161)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 161
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 161
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(75)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 75
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 75
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(94)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 94
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 94
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(158)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 158
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 158
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(86)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 86
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 86
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(92)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 92
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 92
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(24)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 24
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 24
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(62)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 62
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 62
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(11)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 11
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 11
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(74)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 74
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 74
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(122)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 122
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 122
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(43)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 43
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 43
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(112)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 112
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 112
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(79)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 79
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 79
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(119)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 119
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 119
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(118)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 118
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 118
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(106)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 106
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 106
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(123)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 123
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 123
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(16)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 16
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 16
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(23)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 23
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 23
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(48)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 48
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 48
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(97)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 97
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 97
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(187)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 187
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 187
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(95)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 95
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 95
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(76)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 76
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 76
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(89)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 89
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 89
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(98)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 98
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 98
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(8)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 8
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 8
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#57
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 8
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 8
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_8
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_8 of size 9144 dropped from memory (free 2065064786)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_8_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_8_piece0 of size 4744 dropped from memory (free 2065069530)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#57]
 +- Relation[value#57] text                 +- Relation[value#57] text
          
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 8, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 8
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(0)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning shuffle 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 0
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned shuffle 0
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(42)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 42
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 42
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(15)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 15
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 15
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(25)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 25
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 25
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(12)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 12
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 12
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(14)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 14
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 14
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(85)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 85
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 85
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(5)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 5
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 5
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 5
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 5
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_5
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_5 of size 13072 dropped from memory (free 2065082602)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_5_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_5_piece0 of size 6988 dropped from memory (free 2065089590)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 6.8 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 5, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 5
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(175)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 0, response is true
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 175
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#61: java.lang.String   DeserializeToObject cast(value#57 as string).toString, obj#61: java.lang.String
 +- LocalRelation <empty>, [value#57]                                                                                                                                      +- LocalRelation <empty>, [value#57]
          
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 175
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(93)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 93
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 93
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(159)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 159
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 159
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(80)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 80
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 80
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(49)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 49
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 49
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(103)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 103
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 103
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(57)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 57
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 57
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(133)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 133
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 133
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(27)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 27
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 27
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(105)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 105
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 105
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(64)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 64
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 64
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(100)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 100
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 100
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(188)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 188
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 188
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(29)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 29
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 29
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(190)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 190
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 190
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(152)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 152
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 152
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(30)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 30
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 30
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(71)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 71
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 71
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(120)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 120
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 120
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(144)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 144
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 144
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(149)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 149
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 149
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(143)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 143
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 143
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(172)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 172
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 172
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(53)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 53
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 53
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(58)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 58
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 58
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(130)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 130
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 130
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(99)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 99
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 99
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(157)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 157
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 157
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(138)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 138
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 138
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(41)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 41
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 41
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(109)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 109
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 109
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(110)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 110
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 110
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(13)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 13
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 13
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(183)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 183
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 183
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(102)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 102
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 102
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(90)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 90
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 90
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(111)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 111
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 111
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#57
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(4)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning broadcast 4
2022-02-10 15:13:40 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 4
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 4
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing broadcast 4
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_4_piece0
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_4_piece0 of size 21155 dropped from memory (free 2065110745)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#57, None)) > 0)
 +- Project [value#57]                      +- Project [value#57]
    +- Relation[value#57] text                 +- Relation[value#57] text
          
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Removing block broadcast_4
2022-02-10 15:13:40 DEBUG MemoryStore:58 - Block broadcast_4 of size 227072 dropped from memory (free 2065337817)
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 4, response is 0
2022-02-10 15:13:40 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaned broadcast 4
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(17)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 17
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 17
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(129)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 129
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 129
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(22)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 22
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 22
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(184)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 184
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 184
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(185)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 185
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 185
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(151)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 151
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 151
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(139)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 139
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 139
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(63)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 63
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 63
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(69)
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#62: java.lang.String   DeserializeToObject cast(value#57 as string).toString, obj#62: java.lang.String
 +- LocalRelation <empty>, [value#57]                                                                                                                                      +- LocalRelation <empty>, [value#57]
          
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 69
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 69
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(169)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 169
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 169
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(104)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 104
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 104
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(10)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 10
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 10
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(181)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 181
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 181
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(19)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 19
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 19
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(96)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 96
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 96
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(44)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 44
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 44
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(107)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 107
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 107
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(40)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 40
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 40
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(45)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 45
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 45
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(9)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 9
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 9
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(37)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 37
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 37
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(77)
2022-02-10 15:13:40 DEBUG ContextCleaner:58 - Cleaning accumulator 77
2022-02-10 15:13:40 INFO  ContextCleaner:54 - Cleaned accumulator 77
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#63: java.lang.String   DeserializeToObject cast(value#57 as string).toString, obj#63: java.lang.String
 +- LocalRelation <empty>, [value#57]                                                                                                                                      +- LocalRelation <empty>, [value#57]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#57, None)) > 0)      +- Filter (length(trim(value#57, None)) > 0)
!      +- Project [value#57]                             +- Relation[value#57] text
!         +- Relation[value#57] text               
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#57, None)) > 0)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_14 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_14 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_14_piece0 locally took  15 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_14_piece0 without replication took  15 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 14 from load at UseCase5.java:20
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8388788 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:20
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 6 (load at UseCase5.java:20) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (load at UseCase5.java:20)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 8 (name=load at UseCase5.java:20;jobs=6))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 8 (MapPartitionsRDD[35] at load at UseCase5.java:20), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 8)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_15 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_15 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_15_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_15_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at load at UseCase5.java:20) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 8.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 8.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 8.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8355 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 8.0 (TID 8)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_15
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_15 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_14
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_14 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 8.0 (TID 8). 1111 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 8 (load at UseCase5.java:20) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 8, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 6 finished: load at UseCase5.java:20, took 0.011884 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#65: java.lang.String   DeserializeToObject cast(value#57 as string).toString, obj#65: java.lang.String
 +- Project [value#57]                                                                                                                                                     +- Project [value#57]
    +- Relation[value#57] text                                                                                                                                                +- Relation[value#57] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#57 as string).toString, obj#65: java.lang.String   DeserializeToObject value#57.toString, obj#65: java.lang.String
!+- Project [value#57]                                                             +- Relation[value#57] text
!   +- Relation[value#57] text                                                     
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_16 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_16 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_16_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_16_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 16 from load at UseCase5.java:20
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8388788 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:20
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 7 (load at UseCase5.java:20) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (load at UseCase5.java:20)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 9 (name=load at UseCase5.java:20;jobs=7))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 9 (MapPartitionsRDD[41] at load at UseCase5.java:20), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 9)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 13.9 KB, free 1969.2 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_17 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_17 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1969.2 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_17_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_17_piece0 without replication took  16 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[41] at load at UseCase5.java:20) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 9.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 9.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 9.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8355 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 9.0 (TID 9)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_17
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_17 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_16
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_16 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 9.0 (TID 9). 1352 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 9 (load at UseCase5.java:20) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 9, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 7 finished: load at UseCase5.java:20, took 0.018286 s
2022-02-10 15:13:40 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#71
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#71]
 +- Relation[value#71] text                 +- Relation[value#71] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#75: java.lang.String   DeserializeToObject cast(value#71 as string).toString, obj#75: java.lang.String
 +- LocalRelation <empty>, [value#71]                                                                                                                                      +- LocalRelation <empty>, [value#71]
          
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#71
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#71, None)) > 0)
 +- Project [value#71]                      +- Project [value#71]
    +- Relation[value#71] text                 +- Relation[value#71] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#76: java.lang.String   DeserializeToObject cast(value#71 as string).toString, obj#76: java.lang.String
 +- LocalRelation <empty>, [value#71]                                                                                                                                      +- LocalRelation <empty>, [value#71]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#77: java.lang.String   DeserializeToObject cast(value#71 as string).toString, obj#77: java.lang.String
 +- LocalRelation <empty>, [value#71]                                                                                                                                      +- LocalRelation <empty>, [value#71]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#71, None)) > 0)      +- Filter (length(trim(value#71, None)) > 0)
!      +- Project [value#71]                             +- Relation[value#71] text
!         +- Relation[value#71] text               
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#71, None)) > 0)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 221.9 KB, free 1968.9 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_18 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_18 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_18_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_18_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 18 from load at UseCase5.java:26
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8390764 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:26
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 8 (load at UseCase5.java:26) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (load at UseCase5.java:26)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 10 (name=load at UseCase5.java:26;jobs=8))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 10 (MapPartitionsRDD[45] at load at UseCase5.java:26), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 10)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 8.9 KB, free 1968.9 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_19 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_19 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.9 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_19_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_19_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[45] at load at UseCase5.java:26) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 10.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 10.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 10.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8354 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 10.0 (TID 10)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_19
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_19 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_18
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_18 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 10.0 (TID 10). 1205 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 10 (load at UseCase5.java:26) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 10, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 8 finished: load at UseCase5.java:26, took 0.016689 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#79: java.lang.String   DeserializeToObject cast(value#71 as string).toString, obj#79: java.lang.String
 +- Project [value#71]                                                                                                                                                     +- Project [value#71]
    +- Relation[value#71] text                                                                                                                                                +- Relation[value#71] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#71 as string).toString, obj#79: java.lang.String   DeserializeToObject value#71.toString, obj#79: java.lang.String
!+- Project [value#71]                                                             +- Relation[value#71] text
!   +- Relation[value#71] text                                                     
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 221.9 KB, free 1968.7 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_20 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_20 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.7 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_20_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_20_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 20 from load at UseCase5.java:26
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8390764 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:26
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 9 (load at UseCase5.java:26) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 11 (load at UseCase5.java:26)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 11 (name=load at UseCase5.java:26;jobs=9))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 11 (MapPartitionsRDD[51] at load at UseCase5.java:26), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 11)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 13.9 KB, free 1968.7 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_21 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_21 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1968.6 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.5 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_21_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_21_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at load at UseCase5.java:26) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 11.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 11.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 11.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8354 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 11.0 (TID 11)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_21
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_21 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_20
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_20 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 11.0 (TID 11). 1443 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 11 (load at UseCase5.java:26) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 11, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 9 finished: load at UseCase5.java:26, took 0.018623 s
2022-02-10 15:13:40 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 15:13:40 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#87
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#87]
 +- Relation[value#87] text                 +- Relation[value#87] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#91: java.lang.String   DeserializeToObject cast(value#87 as string).toString, obj#91: java.lang.String
 +- LocalRelation <empty>, [value#87]                                                                                                                                      +- LocalRelation <empty>, [value#87]
          
2022-02-10 15:13:40 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#87
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#87, None)) > 0)
 +- Project [value#87]                      +- Project [value#87]
    +- Relation[value#87] text                 +- Relation[value#87] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#92: java.lang.String   DeserializeToObject cast(value#87 as string).toString, obj#92: java.lang.String
 +- LocalRelation <empty>, [value#87]                                                                                                                                      +- LocalRelation <empty>, [value#87]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#93: java.lang.String   DeserializeToObject cast(value#87 as string).toString, obj#93: java.lang.String
 +- LocalRelation <empty>, [value#87]                                                                                                                                      +- LocalRelation <empty>, [value#87]
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#87, None)) > 0)      +- Filter (length(trim(value#87, None)) > 0)
!      +- Project [value#87]                             +- Relation[value#87] text
!         +- Relation[value#87] text               
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#87, None)) > 0)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 221.9 KB, free 1968.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_22 locally took  15 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_22 without replication took  16 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_22_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_22_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 22 from load at UseCase5.java:32
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8737102 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:32
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 10 (load at UseCase5.java:32) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (load at UseCase5.java:32)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 12 (name=load at UseCase5.java:32;jobs=10))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 12 (MapPartitionsRDD[55] at load at UseCase5.java:32), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 12)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 8.9 KB, free 1968.4 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_23 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_23 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.4 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_23_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_23_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[55] at load at UseCase5.java:32) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 12.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 12.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 12.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 12.0 (TID 12)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_23
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_23 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_22
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_22 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 12.0 (TID 12). 1234 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 12 (load at UseCase5.java:32) finished in 0.016 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 12, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 10 finished: load at UseCase5.java:32, took 0.014316 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#95: java.lang.String   DeserializeToObject cast(value#87 as string).toString, obj#95: java.lang.String
 +- Project [value#87]                                                                                                                                                     +- Project [value#87]
    +- Relation[value#87] text                                                                                                                                                +- Relation[value#87] text
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#87 as string).toString, obj#95: java.lang.String   DeserializeToObject value#87.toString, obj#95: java.lang.String
!+- Project [value#87]                                                             +- Relation[value#87] text
!   +- Relation[value#87] text                                                     
          
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:40 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 221.9 KB, free 1968.2 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_24 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_24 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.2 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_24_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_24_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 24 from load at UseCase5.java:32
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8737102 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:40 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:40 INFO  SparkContext:54 - Starting job: load at UseCase5.java:32
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Got job 11 (load at UseCase5.java:32) with 1 output partitions
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Final stage: ResultStage 13 (load at UseCase5.java:32)
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitStage(ResultStage 13 (name=load at UseCase5.java:32;jobs=11))
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting ResultStage 13 (MapPartitionsRDD[61] at load at UseCase5.java:32), which has no missing parents
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 13)
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_25 stored as values in memory (estimated size 14.0 KB, free 1968.1 MB)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_25 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_25 without replication took  0 ms
2022-02-10 15:13:40 INFO  MemoryStore:54 - Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1968.1 MB)
2022-02-10 15:13:40 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 15:13:40 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 15:13:40 DEBUG BlockManager:58 - Put block broadcast_25_piece0 locally took  0 ms
2022-02-10 15:13:40 DEBUG BlockManager:58 - Putting block broadcast_25_piece0 without replication took  0 ms
2022-02-10 15:13:40 INFO  SparkContext:54 - Created broadcast 25 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[61] at load at UseCase5.java:32) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Adding task set 13.0 with 1 tasks
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Epoch for TaskSet 13.0: 2
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 13.0: NO_PREF, ANY
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes)
2022-02-10 15:13:40 INFO  Executor:54 - Running task 0.0 in stage 13.0 (TID 13)
2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_25
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_25 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:40 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:40 DEBUG BlockManager:58 - Getting local block broadcast_24
2022-02-10 15:13:40 DEBUG BlockManager:58 - Level for block broadcast_24 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:40 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:40 INFO  Executor:54 - Finished task 0.0 in stage 13.0 (TID 13). 1559 bytes result sent to driver
2022-02-10 15:13:40 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 15:13:40 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:40 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 32 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:40 INFO  TaskSchedulerImpl:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2022-02-10 15:13:40 INFO  DAGScheduler:54 - ResultStage 13 (load at UseCase5.java:32) finished in 0.032 s
2022-02-10 15:13:40 DEBUG DAGScheduler:58 - After removal of stage 13, remaining stages = 0
2022-02-10 15:13:40 INFO  DAGScheduler:54 - Job 11 finished: load at UseCase5.java:32, took 0.039051 s
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [department_id#67, department_name#68], [department_id#67, department_name#68, count(product_category_id#98) AS count(product_category_id)#169L]   Aggregate [department_id#67, department_name#68], [department_id#67, department_name#68, count(product_category_id#98) AS count(product_category_id)#169L]
 +- Join Inner, (category_id#81 = product_category_id#98)                                                                                                     +- Join Inner, (category_id#81 = product_category_id#98)
    :- Join Inner, (department_id#67 = category_department_id#82)                                                                                                :- Join Inner, (department_id#67 = category_department_id#82)
    :  :- Relation[department_id#67,department_name#68] csv                                                                                                      :  :- Relation[department_id#67,department_name#68] csv
    :  +- Relation[category_id#81,category_department_id#82,category_name#83] csv                                                                                :  +- Relation[category_id#81,category_department_id#82,category_name#83] csv
    +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv                            +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#177L]                                                                                                                                 Aggregate [count(1) AS count#177L]
 +- Sort [department_id#67 ASC NULLS FIRST], true                                                                                                                   +- Sort [department_id#67 ASC NULLS FIRST], true
    +- Aggregate [department_id#67, department_name#68], [department_id#67, department_name#68, count(product_category_id#98) AS count(product_category_id)#169L]      +- Aggregate [department_id#67, department_name#68], [department_id#67, department_name#68, count(product_category_id#98) AS count(product_category_id)#169L]
       +- Join Inner, (category_id#81 = product_category_id#98)                                                                                                           +- Join Inner, (category_id#81 = product_category_id#98)
          :- Join Inner, (department_id#67 = category_department_id#82)                                                                                                      :- Join Inner, (department_id#67 = category_department_id#82)
          :  :- Relation[department_id#67,department_name#68] csv                                                                                                            :  :- Relation[department_id#67,department_name#68] csv
          :  +- Relation[category_id#81,category_department_id#82,category_name#83] csv                                                                                      :  +- Relation[category_id#81,category_department_id#82,category_name#83] csv
          +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv                                  +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#177L]                                                                                                                                 Aggregate [count(1) AS count#177L]
!+- Sort [department_id#67 ASC NULLS FIRST], true                                                                                                                   +- Project
!   +- Aggregate [department_id#67, department_name#68], [department_id#67, department_name#68, count(product_category_id#98) AS count(product_category_id)#169L]      +- Sort [department_id#67 ASC NULLS FIRST], true
!      +- Join Inner, (category_id#81 = product_category_id#98)                                                                                                           +- Aggregate [department_id#67, department_name#68], [department_id#67]
!         :- Join Inner, (department_id#67 = category_department_id#82)                                                                                                      +- Project [department_id#67, department_name#68]
!         :  :- Relation[department_id#67,department_name#68] csv                                                                                                               +- Join Inner, (category_id#81 = product_category_id#98)
!         :  +- Relation[category_id#81,category_department_id#82,category_name#83] csv                                                                                            :- Project [department_id#67, department_name#68, category_id#81]
!         +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv                                        :  +- Join Inner, (department_id#67 = category_department_id#82)
!                                                                                                                                                                                  :     :- Relation[department_id#67,department_name#68] csv
!                                                                                                                                                                                  :     +- Project [category_id#81, category_department_id#82]
!                                                                                                                                                                                  :        +- Relation[category_id#81,category_department_id#82,category_name#83] csv
!                                                                                                                                                                                  +- Project [product_category_id#98]
!                                                                                                                                                                                     +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Infer Filters ===
 Aggregate [count(1) AS count#177L]                                                                                                                    Aggregate [count(1) AS count#177L]
 +- Project                                                                                                                                            +- Project
    +- Sort [department_id#67 ASC NULLS FIRST], true                                                                                                      +- Sort [department_id#67 ASC NULLS FIRST], true
       +- Aggregate [department_id#67, department_name#68], [department_id#67]                                                                               +- Aggregate [department_id#67, department_name#68], [department_id#67]
          +- Project [department_id#67, department_name#68]                                                                                                     +- Project [department_id#67, department_name#68]
             +- Join Inner, (category_id#81 = product_category_id#98)                                                                                              +- Join Inner, (category_id#81 = product_category_id#98)
!               :- Project [department_id#67, department_name#68, category_id#81]                                                                                     :- Filter isnotnull(category_id#81)
!               :  +- Join Inner, (department_id#67 = category_department_id#82)                                                                                      :  +- Project [department_id#67, department_name#68, category_id#81]
!               :     :- Relation[department_id#67,department_name#68] csv                                                                                            :     +- Join Inner, (department_id#67 = category_department_id#82)
!               :     +- Project [category_id#81, category_department_id#82]                                                                                          :        :- Filter isnotnull(department_id#67)
!               :        +- Relation[category_id#81,category_department_id#82,category_name#83] csv                                                                   :        :  +- Relation[department_id#67,department_name#68] csv
!               +- Project [product_category_id#98]                                                                                                                   :        +- Filter isnotnull(category_department_id#82)
!                  +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv                  :           +- Project [category_id#81, category_department_id#82]
!                                                                                                                                                                     :              +- Relation[category_id#81,category_department_id#82,category_name#83] csv
!                                                                                                                                                                     +- Filter isnotnull(product_category_id#98)
!                                                                                                                                                                        +- Project [product_category_id#98]
!                                                                                                                                                                           +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv
          
2022-02-10 15:13:40 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization after Inferring Filters ===
 Aggregate [count(1) AS count#177L]                                                                                                                       Aggregate [count(1) AS count#177L]
 +- Project                                                                                                                                               +- Project
    +- Sort [department_id#67 ASC NULLS FIRST], true                                                                                                         +- Sort [department_id#67 ASC NULLS FIRST], true
       +- Aggregate [department_id#67, department_name#68], [department_id#67]                                                                                  +- Aggregate [department_id#67, department_name#68], [department_id#67]
          +- Project [department_id#67, department_name#68]                                                                                                        +- Project [department_id#67, department_name#68]
             +- Join Inner, (category_id#81 = product_category_id#98)                                                                                                 +- Join Inner, (category_id#81 = product_category_id#98)
!               :- Filter isnotnull(category_id#81)                                                                                                                      :- Project [department_id#67, department_name#68, category_id#81]
!               :  +- Project [department_id#67, department_name#68, category_id#81]                                                                                     :  +- Join Inner, (department_id#67 = category_department_id#82)
!               :     +- Join Inner, (department_id#67 = category_department_id#82)                                                                                      :     :- Filter isnotnull(department_id#67)
!               :        :- Filter isnotnull(department_id#67)                                                                                                           :     :  +- Relation[department_id#67,department_name#68] csv
!               :        :  +- Relation[department_id#67,department_name#68] csv                                                                                         :     +- Project [category_id#81, category_department_id#82]
!               :        +- Filter isnotnull(category_department_id#82)                                                                                                  :        +- Filter (isnotnull(category_department_id#82) && isnotnull(category_id#81))
!               :           +- Project [category_id#81, category_department_id#82]                                                                                       :           +- Relation[category_id#81,category_department_id#82,category_name#83] csv
!               :              +- Relation[category_id#81,category_department_id#82,category_name#83] csv                                                                +- Project [product_category_id#98]
!               +- Filter isnotnull(product_category_id#98)                                                                                                                 +- Filter isnotnull(product_category_id#98)
!                  +- Project [product_category_id#98]                                                                                                                         +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv
!                     +- Relation[product_id#97,product_category_id#98,product_name#99,product_description#100,product_price#101,product_image#102] csv   
          
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((category_id#81 = product_category_id#98))
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(category_id#81) | rightKeys:List(product_category_id#98)
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((category_id#81 = product_category_id#98))
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(category_id#81) | rightKeys:List(product_category_id#98)
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((department_id#67 = category_department_id#82))
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(department_id#67) | rightKeys:List(category_department_id#82)
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((department_id#67 = category_department_id#82))
2022-02-10 15:13:40 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(department_id#67) | rightKeys:List(category_department_id#82)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(department_id#67)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<department_id: int, department_name: string>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(department_id)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(category_department_id#82),isnotnull(category_id#81)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<category_id: int, category_department_id: int>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(category_department_id),IsNotNull(category_id)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(product_category_id#98)
2022-02-10 15:13:40 INFO  FileSourceStrategy:54 - Output Data Schema: struct<product_category_id: int>
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(product_category_id)
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(none)
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(none),IsNotNull(none)
2022-02-10 15:13:40 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(none)
2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage6(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=6
/* 006 */ final class GeneratedIteratorForCodegenStage6 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage6(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage6(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=6
/* 006 */ final class GeneratedIteratorForCodegenStage6 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage6(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         filter_mutableStateArray_0[1].reset();
/* 039 */
/* 040 */         if (false) {
/* 041 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 042 */         } else {
/* 043 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 044 */         }
/* 045 */         append((filter_mutableStateArray_0[1].getRow()));
/* 046 */
/* 047 */       } while(false);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */   }
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 039 */         UTF8String scan_value_1 = scan_isNull_1 ?
/* 040 */         null : (scan_row_0.getUTF8String(1));
/* 041 */         filter_mutableStateArray_0[1].reset();
/* 042 */
/* 043 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 044 */
/* 045 */         if (false) {
/* 046 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 047 */         } else {
/* 048 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 049 */         }
/* 050 */
/* 051 */         if (scan_isNull_1) {
/* 052 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 053 */         } else {
/* 054 */           filter_mutableStateArray_0[1].write(1, scan_value_1);
/* 055 */         }
/* 056 */         append((filter_mutableStateArray_0[1].getRow()));
/* 057 */
/* 058 */       } while(false);
/* 059 */       if (shouldStop()) return;
/* 060 */     }
/* 061 */   }
/* 062 */
/* 063 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 039 */         UTF8String scan_value_1 = scan_isNull_1 ?
/* 040 */         null : (scan_row_0.getUTF8String(1));
/* 041 */         filter_mutableStateArray_0[1].reset();
/* 042 */
/* 043 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 044 */
/* 045 */         if (false) {
/* 046 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 047 */         } else {
/* 048 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 049 */         }
/* 050 */
/* 051 */         if (scan_isNull_1) {
/* 052 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 053 */         } else {
/* 054 */           filter_mutableStateArray_0[1].write(1, scan_value_1);
/* 055 */         }
/* 056 */         append((filter_mutableStateArray_0[1].getRow()));
/* 057 */
/* 058 */       } while(false);
/* 059 */       if (shouldStop()) return;
/* 060 */     }
/* 061 */   }
/* 062 */
/* 063 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         filter_mutableStateArray_0[1].reset();
/* 039 */
/* 040 */         if (false) {
/* 041 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 042 */         } else {
/* 043 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 044 */         }
/* 045 */         append((filter_mutableStateArray_0[1].getRow()));
/* 046 */
/* 047 */       } while(false);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */   }
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 11.6455 ms
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 8.1577 ms
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 9.8775 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_26 stored as values in memory (estimated size 221.8 KB, free 1967.9 MB)
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_27 stored as values in memory (estimated size 221.8 KB, free 1967.7 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_26 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_26 without replication took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_27 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_27 without replication took  0 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.7 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_26_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_26_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 26 from run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4368551 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.7 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_27_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_27_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 27 from run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194394 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage5(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=5
/* 006 */ final class GeneratedIteratorForCodegenStage5 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private boolean sort_needToSort_0;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 015 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 016 */   private scala.collection.Iterator inputadapter_input_0;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 018 */
/* 019 */   public GeneratedIteratorForCodegenStage5(Object[] references) {
/* 020 */     this.references = references;
/* 021 */   }
/* 022 */
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     sort_needToSort_0 = true;
/* 028 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 029 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 036 */     // initialize aggregation buffer
/* 037 */     agg_bufIsNull_0 = false;
/* 038 */     agg_bufValue_0 = 0L;
/* 039 */
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       agg_doConsume_0();
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */
/* 059 */   }
/* 060 */
/* 061 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 062 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 063 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 064 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 065 */       if (shouldStop()) return;
/* 066 */     }
/* 067 */
/* 068 */   }
/* 069 */
/* 070 */   private void agg_doConsume_0() throws java.io.IOException {
/* 071 */     // do aggregate
/* 072 */     // common sub-expressions
/* 073 */
/* 074 */     // evaluate aggregate function
/* 075 */     long agg_value_1 = -1L;
/* 076 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 077 */     // update aggregation buffer
/* 078 */     agg_bufIsNull_0 = false;
/* 079 */     agg_bufValue_0 = agg_value_1;
/* 080 */
/* 081 */   }
/* 082 */
/* 083 */   protected void processNext() throws java.io.IOException {
/* 084 */     while (!agg_initAgg_0) {
/* 085 */       agg_initAgg_0 = true;
/* 086 */       long agg_beforeAgg_0 = System.nanoTime();
/* 087 */       agg_doAggregateWithoutKey_0();
/* 088 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 089 */
/* 090 */       // output the result
/* 091 */
/* 092 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 093 */       agg_mutableStateArray_0[0].reset();
/* 094 */
/* 095 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 096 */
/* 097 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 098 */       append((agg_mutableStateArray_0[0].getRow()));
/* 099 */     }
/* 100 */   }
/* 101 */
/* 102 */ }

2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage5(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=5
/* 006 */ final class GeneratedIteratorForCodegenStage5 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private boolean sort_needToSort_0;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 015 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 016 */   private scala.collection.Iterator inputadapter_input_0;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 018 */
/* 019 */   public GeneratedIteratorForCodegenStage5(Object[] references) {
/* 020 */     this.references = references;
/* 021 */   }
/* 022 */
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     sort_needToSort_0 = true;
/* 028 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 029 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 036 */     // initialize aggregation buffer
/* 037 */     agg_bufIsNull_0 = false;
/* 038 */     agg_bufValue_0 = 0L;
/* 039 */
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       agg_doConsume_0();
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */
/* 059 */   }
/* 060 */
/* 061 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 062 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 063 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 064 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 065 */       if (shouldStop()) return;
/* 066 */     }
/* 067 */
/* 068 */   }
/* 069 */
/* 070 */   private void agg_doConsume_0() throws java.io.IOException {
/* 071 */     // do aggregate
/* 072 */     // common sub-expressions
/* 073 */
/* 074 */     // evaluate aggregate function
/* 075 */     long agg_value_1 = -1L;
/* 076 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 077 */     // update aggregation buffer
/* 078 */     agg_bufIsNull_0 = false;
/* 079 */     agg_bufValue_0 = agg_value_1;
/* 080 */
/* 081 */   }
/* 082 */
/* 083 */   protected void processNext() throws java.io.IOException {
/* 084 */     while (!agg_initAgg_0) {
/* 085 */       agg_initAgg_0 = true;
/* 086 */       long agg_beforeAgg_0 = System.nanoTime();
/* 087 */       agg_doAggregateWithoutKey_0();
/* 088 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 089 */
/* 090 */       // output the result
/* 091 */
/* 092 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 093 */       agg_mutableStateArray_0[0].reset();
/* 094 */
/* 095 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 096 */
/* 097 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 098 */       append((agg_mutableStateArray_0[0].getRow()));
/* 099 */     }
/* 100 */   }
/* 101 */
/* 102 */ }

2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Got job 12 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:41 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitStage(ResultStage 14 (name=run at ThreadPoolExecutor.java:1149;jobs=12))
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting ResultStage 14 (MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 14)
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_28 stored as values in memory (estimated size 11.0 KB, free 1967.7 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_28 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_28 without replication took  0 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1967.6 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 6.0 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_28_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_28_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_28_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 28 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 19.1062 ms
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[65] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Adding task set 14.0 with 1 tasks
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Epoch for TaskSet 14.0: 2
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 14.0: NO_PREF, ANY
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Got job 13 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (run at ThreadPoolExecutor.java:1149)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8320 bytes)
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitStage(ResultStage 15 (name=run at ThreadPoolExecutor.java:1149;jobs=13))
2022-02-10 15:13:41 INFO  Executor:54 - Running task 0.0 in stage 14.0 (TID 14)
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting ResultStage 15 (MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 15)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_28
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_28 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 INFO  HashAggregateExec:54 - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_29 stored as values in memory (estimated size 11.2 KB, free 1967.6 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_29 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_29 without replication took  0 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1967.6 MB)
2022-02-10 15:13:41 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/products/part-00000, range: 0-174247, partition values: [empty row]
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 6.0 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_29_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_29_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_29_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 29 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[67] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Adding task set 15.0 with 1 tasks
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Epoch for TaskSet 15.0: 2
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 15.0: NO_PREF, ANY
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 1
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 6.7644 ms
2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     inputadapter_input_0 = inputs[0];
/* 025 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 026 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 031 */   throws java.io.IOException {
/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 033 */
/* 034 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);
/* 035 */     int agg_value_7 = agg_isNull_7 ?
/* 036 */     -1 : (agg_keyTerm_0.getInt(0));
/* 037 */     agg_mutableStateArray_0[1].reset();
/* 038 */
/* 039 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 040 */
/* 041 */     if (agg_isNull_7) {
/* 042 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 043 */     } else {
/* 044 */       agg_mutableStateArray_0[1].write(0, agg_value_7);
/* 045 */     }
/* 046 */     append((agg_mutableStateArray_0[1].getRow()));
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, int agg_expr_0_0, boolean agg_exprIsNull_0_0, UTF8String agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 051 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 052 */
/* 053 */     // generate grouping key
/* 054 */     agg_mutableStateArray_0[0].reset();
/* 055 */
/* 056 */     agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 057 */
/* 058 */     if (agg_exprIsNull_0_0) {
/* 059 */       agg_mutableStateArray_0[0].setNullAt(0);
/* 060 */     } else {
/* 061 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 062 */     }
/* 063 */
/* 064 */     if (agg_exprIsNull_1_0) {
/* 065 */       agg_mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       agg_mutableStateArray_0[0].write(1, agg_expr_1_0);
/* 068 */     }
/* 069 */     int agg_value_4 = 48;
/* 070 */
/* 071 */     if (!agg_exprIsNull_0_0) {
/* 072 */       agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_expr_0_0, agg_value_4);
/* 073 */     }
/* 074 */
/* 075 */     if (!agg_exprIsNull_1_0) {
/* 076 */       agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_1_0.getBaseObject(), agg_expr_1_0.getBaseOffset(), agg_expr_1_0.numBytes(), agg_value_4);
/* 077 */     }
/* 078 */     if (true) {
/* 079 */       // try to get the buffer from hash map
/* 080 */       agg_unsafeRowAggBuffer_0 =
/* 081 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 082 */     }
/* 083 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 084 */     // aggregation after processing all input rows.
/* 085 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 086 */       if (agg_sorter_0 == null) {
/* 087 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 088 */       } else {
/* 089 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 090 */       }
/* 091 */
/* 092 */       // the hash map had be spilled, it should have enough memory now,
/* 093 */       // try to allocate buffer again.
/* 094 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 095 */         (agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 096 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 097 */         // failed to allocate the first page
/* 098 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 099 */       }
/* 100 */     }
/* 101 */
/* 102 */     // common sub-expressions
/* 103 */
/* 104 */     // evaluate aggregate function
/* 105 */
/* 106 */     // update unsafe row buffer
/* 107 */
/* 108 */   }
/* 109 */
/* 110 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 111 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 112 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 113 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 114 */       int inputadapter_value_0 = inputadapter_isNull_0 ?
/* 115 */       -1 : (inputadapter_row_0.getInt(0));
/* 116 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 117 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 118 */       null : (inputadapter_row_0.getUTF8String(1));
/* 119 */
/* 120 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);
/* 121 */       if (shouldStop()) return;
/* 122 */     }
/* 123 */
/* 124 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 125 */   }
/* 126 */
/* 127 */   protected void processNext() throws java.io.IOException {
/* 128 */     if (!agg_initAgg_0) {
/* 129 */       agg_initAgg_0 = true;
/* 130 */
/* 131 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 132 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 133 */       agg_doAggregateWithKeys_0();
/* 134 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 135 */     }
/* 136 */
/* 137 */     // output the result
/* 138 */
/* 139 */     while (agg_mapIter_0.next()) {
/* 140 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 141 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 142 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 143 */
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */
/* 147 */     agg_mapIter_0.close();
/* 148 */     if (agg_sorter_0 == null) {
/* 149 */       agg_hashMap_0.free();
/* 150 */     }
/* 151 */   }
/* 152 */
/* 153 */ }

2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_26
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_26 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     inputadapter_input_0 = inputs[0];
/* 025 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 026 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 031 */   throws java.io.IOException {
/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 033 */
/* 034 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);
/* 035 */     int agg_value_7 = agg_isNull_7 ?
/* 036 */     -1 : (agg_keyTerm_0.getInt(0));
/* 037 */     agg_mutableStateArray_0[1].reset();
/* 038 */
/* 039 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 040 */
/* 041 */     if (agg_isNull_7) {
/* 042 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 043 */     } else {
/* 044 */       agg_mutableStateArray_0[1].write(0, agg_value_7);
/* 045 */     }
/* 046 */     append((agg_mutableStateArray_0[1].getRow()));
/* 047 */
/* 048 */   }
/* 049 */
/* 050 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, int agg_expr_0_0, boolean agg_exprIsNull_0_0, UTF8String agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 051 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 052 */
/* 053 */     // generate grouping key
/* 054 */     agg_mutableStateArray_0[0].reset();
/* 055 */
/* 056 */     agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 057 */
/* 058 */     if (agg_exprIsNull_0_0) {
/* 059 */       agg_mutableStateArray_0[0].setNullAt(0);
/* 060 */     } else {
/* 061 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 062 */     }
/* 063 */
/* 064 */     if (agg_exprIsNull_1_0) {
/* 065 */       agg_mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       agg_mutableStateArray_0[0].write(1, agg_expr_1_0);
/* 068 */     }
/* 069 */     int agg_value_4 = 48;
/* 070 */
/* 071 */     if (!agg_exprIsNull_0_0) {
/* 072 */       agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_expr_0_0, agg_value_4);
/* 073 */     }
/* 074 */
/* 075 */     if (!agg_exprIsNull_1_0) {
/* 076 */       agg_value_4 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_1_0.getBaseObject(), agg_expr_1_0.getBaseOffset(), agg_expr_1_0.numBytes(), agg_value_4);
/* 077 */     }
/* 078 */     if (true) {
/* 079 */       // try to get the buffer from hash map
/* 080 */       agg_unsafeRowAggBuffer_0 =
/* 081 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 082 */     }
/* 083 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 084 */     // aggregation after processing all input rows.
/* 085 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 086 */       if (agg_sorter_0 == null) {
/* 087 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 088 */       } else {
/* 089 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 090 */       }
/* 091 */
/* 092 */       // the hash map had be spilled, it should have enough memory now,
/* 093 */       // try to allocate buffer again.
/* 094 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 095 */         (agg_mutableStateArray_0[0].getRow()), agg_value_4);
/* 096 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 097 */         // failed to allocate the first page
/* 098 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 099 */       }
/* 100 */     }
/* 101 */
/* 102 */     // common sub-expressions
/* 103 */
/* 104 */     // evaluate aggregate function
/* 105 */
/* 106 */     // update unsafe row buffer
/* 107 */
/* 108 */   }
/* 109 */
/* 110 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 111 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 112 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 113 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 114 */       int inputadapter_value_0 = inputadapter_isNull_0 ?
/* 115 */       -1 : (inputadapter_row_0.getInt(0));
/* 116 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 117 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 118 */       null : (inputadapter_row_0.getUTF8String(1));
/* 119 */
/* 120 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);
/* 121 */       if (shouldStop()) return;
/* 122 */     }
/* 123 */
/* 124 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 125 */   }
/* 126 */
/* 127 */   protected void processNext() throws java.io.IOException {
/* 128 */     if (!agg_initAgg_0) {
/* 129 */       agg_initAgg_0 = true;
/* 130 */
/* 131 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 132 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 133 */       agg_doAggregateWithKeys_0();
/* 134 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 135 */     }
/* 136 */
/* 137 */     // output the result
/* 138 */
/* 139 */     while (agg_mapIter_0.next()) {
/* 140 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 141 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 142 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 143 */
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */
/* 147 */     agg_mapIter_0.close();
/* 148 */     if (agg_sorter_0 == null) {
/* 149 */       agg_hashMap_0.free();
/* 150 */     }
/* 151 */   }
/* 152 */
/* 153 */ }

2022-02-10 15:13:41 INFO  Executor:54 - Finished task 0.0 in stage 14.0 (TID 14). 2181 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8323 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 0.0 in stage 15.0 (TID 15)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 35 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2022-02-10 15:13:41 INFO  DAGScheduler:54 - ResultStage 14 (run at ThreadPoolExecutor.java:1149) finished in 0.051 s
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - After removal of stage 14, remaining stages = 1
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_29
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_29 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Job 12 finished: run at ThreadPoolExecutor.java:1149, took 0.043319 s
2022-02-10 15:13:41 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 0 acquired 1056.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@32c5acc8
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 27.3629 ms
2022-02-10 15:13:41 INFO  HashAggregateExec:54 - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 5.8488 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_27
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_27 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:41 INFO  Executor:54 - Finished task 0.0 in stage 15.0 (TID 15). 1469 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2022-02-10 15:13:41 INFO  DAGScheduler:54 - ResultStage 15 (run at ThreadPoolExecutor.java:1149) finished in 0.051 s
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - After removal of stage 15, remaining stages = 0
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Job 13 finished: run at ThreadPoolExecutor.java:1149, took 0.061519 s
2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 0 acquired 1024.1 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@1fe66cdd
2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 8.8986 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 0 acquired 256.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@1fe66cdd
2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 0 release 128.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@1fe66cdd
2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 0 acquired 48.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@1fe66cdd
2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 0 release 256.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@1fe66cdd
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_30 stored as values in memory (estimated size 1024.1 KB, free 1966.6 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_30 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_30 without replication took  0 ms
2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 0 acquired 464.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@32c5acc8
2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 0 release 32.0 KB from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@32c5acc8
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_31 stored as values in memory (estimated size 1024.5 KB, free 1965.6 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_31 locally took  16 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_31 without replication took  16 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_30_piece0 stored as bytes in memory (estimated size 357.0 B, free 1965.6 MB)
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_31_piece0 stored as bytes in memory (estimated size 9.1 KB, free 1965.6 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 357.0 B, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_30_piece0
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 9.1 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_30_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_30_piece0 without replication took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_31_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_31_piece0 locally took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 30 from run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_31_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 31 from run at ThreadPoolExecutor.java:1149
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_30
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_30 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_31
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_31 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 014 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_1;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[9];
/* 016 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */     wholestagecodegen_init_0_0();
/* 026 */     wholestagecodegen_init_0_1();
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 031 */   throws java.io.IOException {
/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);
/* 033 */
/* 034 */     boolean agg_isNull_9 = agg_keyTerm_0.isNullAt(0);
/* 035 */     int agg_value_9 = agg_isNull_9 ?
/* 036 */     -1 : (agg_keyTerm_0.getInt(0));
/* 037 */     boolean agg_isNull_10 = agg_keyTerm_0.isNullAt(1);
/* 038 */     UTF8String agg_value_10 = agg_isNull_10 ?
/* 039 */     null : (agg_keyTerm_0.getUTF8String(1));
/* 040 */     filter_mutableStateArray_0[8].reset();
/* 041 */
/* 042 */     filter_mutableStateArray_0[8].zeroOutNullBytes();
/* 043 */
/* 044 */     if (agg_isNull_9) {
/* 045 */       filter_mutableStateArray_0[8].setNullAt(0);
/* 046 */     } else {
/* 047 */       filter_mutableStateArray_0[8].write(0, agg_value_9);
/* 048 */     }
/* 049 */
/* 050 */     if (agg_isNull_10) {
/* 051 */       filter_mutableStateArray_0[8].setNullAt(1);
/* 052 */     } else {
/* 053 */       filter_mutableStateArray_0[8].write(1, agg_value_10);
/* 054 */     }
/* 055 */     append((filter_mutableStateArray_0[8].getRow()));
/* 056 */
/* 057 */   }
/* 058 */
/* 059 */   private void wholestagecodegen_init_0_1() {
/* 060 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 061 */     filter_mutableStateArray_0[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 062 */     filter_mutableStateArray_0[7] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 063 */     filter_mutableStateArray_0[8] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 064 */
/* 065 */   }
/* 066 */
/* 067 */   private void agg_doConsume_0(int agg_expr_0_0, boolean agg_exprIsNull_0_0, UTF8String agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 068 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 069 */
/* 070 */     // generate grouping key
/* 071 */     filter_mutableStateArray_0[7].reset();
/* 072 */
/* 073 */     filter_mutableStateArray_0[7].zeroOutNullBytes();
/* 074 */
/* 075 */     if (agg_exprIsNull_0_0) {
/* 076 */       filter_mutableStateArray_0[7].setNullAt(0);
/* 077 */     } else {
/* 078 */       filter_mutableStateArray_0[7].write(0, agg_expr_0_0);
/* 079 */     }
/* 080 */
/* 081 */     if (agg_exprIsNull_1_0) {
/* 082 */       filter_mutableStateArray_0[7].setNullAt(1);
/* 083 */     } else {
/* 084 */       filter_mutableStateArray_0[7].write(1, agg_expr_1_0);
/* 085 */     }
/* 086 */     int agg_value_6 = 48;
/* 087 */
/* 088 */     if (!agg_exprIsNull_0_0) {
/* 089 */       agg_value_6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_expr_0_0, agg_value_6);
/* 090 */     }
/* 091 */
/* 092 */     if (!agg_exprIsNull_1_0) {
/* 093 */       agg_value_6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_1_0.getBaseObject(), agg_expr_1_0.getBaseOffset(), agg_expr_1_0.numBytes(), agg_value_6);
/* 094 */     }
/* 095 */     if (true) {
/* 096 */       // try to get the buffer from hash map
/* 097 */       agg_unsafeRowAggBuffer_0 =
/* 098 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[7].getRow()), agg_value_6);
/* 099 */     }
/* 100 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 101 */     // aggregation after processing all input rows.
/* 102 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 103 */       if (agg_sorter_0 == null) {
/* 104 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 105 */       } else {
/* 106 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 107 */       }
/* 108 */
/* 109 */       // the hash map had be spilled, it should have enough memory now,
/* 110 */       // try to allocate buffer again.
/* 111 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 112 */         (filter_mutableStateArray_0[7].getRow()), agg_value_6);
/* 113 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 114 */         // failed to allocate the first page
/* 115 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 116 */       }
/* 117 */     }
/* 118 */
/* 119 */     // common sub-expressions
/* 120 */
/* 121 */     // evaluate aggregate function
/* 122 */
/* 123 */     // update unsafe row buffer
/* 124 */
/* 125 */   }
/* 126 */
/* 127 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 128 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 129 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 130 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 131 */       do {
/* 132 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 133 */         int scan_value_1 = scan_isNull_1 ?
/* 134 */         -1 : (scan_row_0.getInt(1));
/* 135 */
/* 136 */         if (!(!scan_isNull_1)) continue;
/* 137 */
/* 138 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 139 */         int scan_value_0 = scan_isNull_0 ?
/* 140 */         -1 : (scan_row_0.getInt(0));
/* 141 */
/* 142 */         if (!(!scan_isNull_0)) continue;
/* 143 */
/* 144 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 145 */
/* 146 */         // generate join key for stream side
/* 147 */         boolean bhj_isNull_0 = false;
/* 148 */         long bhj_value_0 = -1L;
/* 149 */         if (!false) {
/* 150 */           bhj_value_0 = (long) scan_value_1;
/* 151 */         }
/* 152 */         // find matches from HashedRelation
/* 153 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 154 */         if (bhj_matched_0 != null) {
/* 155 */           {
/* 156 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);
/* 157 */
/* 158 */             // generate join key for stream side
/* 159 */             boolean bhj_isNull_8 = false;
/* 160 */             long bhj_value_8 = -1L;
/* 161 */             if (!false) {
/* 162 */               bhj_value_8 = (long) scan_value_0;
/* 163 */             }
/* 164 */             // find matches from HashRelation
/* 165 */             scala.collection.Iterator bhj_matches_0 = bhj_isNull_8 ? null : (scala.collection.Iterator)bhj_relation_1.get(bhj_value_8);
/* 166 */             if (bhj_matches_0 != null) {
/* 167 */               while (bhj_matches_0.hasNext()) {
/* 168 */                 UnsafeRow bhj_matched_1 = (UnsafeRow) bhj_matches_0.next();
/* 169 */                 {
/* 170 */                   ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);
/* 171 */
/* 172 */                   boolean bhj_isNull_2 = bhj_matched_0.isNullAt(0);
/* 173 */                   int bhj_value_2 = bhj_isNull_2 ?
/* 174 */                   -1 : (bhj_matched_0.getInt(0));
/* 175 */                   boolean bhj_isNull_3 = bhj_matched_0.isNullAt(1);
/* 176 */                   UTF8String bhj_value_3 = bhj_isNull_3 ?
/* 177 */                   null : (bhj_matched_0.getUTF8String(1));
/* 178 */
/* 179 */                   agg_doConsume_0(bhj_value_2, bhj_isNull_2, bhj_value_3, bhj_isNull_3);
/* 180 */
/* 181 */                 }
/* 182 */               }
/* 183 */             }
/* 184 */
/* 185 */           }
/* 186 */         }
/* 187 */
/* 188 */       } while(false);
/* 189 */       if (shouldStop()) return;
/* 190 */     }
/* 191 */
/* 192 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 193 */   }
/* 194 */
/* 195 */   protected void processNext() throws java.io.IOException {
/* 196 */     if (!agg_initAgg_0) {
/* 197 */       agg_initAgg_0 = true;
/* 198 */
/* 199 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 200 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 201 */       agg_doAggregateWithKeys_0();
/* 202 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 203 */     }
/* 204 */
/* 205 */     // output the result
/* 206 */
/* 207 */     while (agg_mapIter_0.next()) {
/* 208 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 209 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 210 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 211 */
/* 212 */       if (shouldStop()) return;
/* 213 */     }
/* 214 */
/* 215 */     agg_mapIter_0.close();
/* 216 */     if (agg_sorter_0 == null) {
/* 217 */       agg_hashMap_0.free();
/* 218 */     }
/* 219 */   }
/* 220 */
/* 221 */   private void wholestagecodegen_init_0_0() {
/* 222 */     scan_mutableStateArray_0[0] = inputs[0];
/* 223 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 224 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 225 */
/* 226 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[6] /* broadcast */).value()).asReadOnlyCopy();
/* 227 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 228 */
/* 229 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 230 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 231 */
/* 232 */     bhj_relation_1 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();
/* 233 */     incPeakExecutionMemory(bhj_relation_1.estimatedSize());
/* 234 */
/* 235 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 236 */
/* 237 */   }
/* 238 */
/* 239 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 013 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 014 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_1;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[9];
/* 016 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 017 */
/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 019 */     this.references = references;
/* 020 */   }
/* 021 */
/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 023 */     partitionIndex = index;
/* 024 */     this.inputs = inputs;
/* 025 */     wholestagecodegen_init_0_0();
/* 026 */     wholestagecodegen_init_0_1();
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 031 */   throws java.io.IOException {
/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);
/* 033 */
/* 034 */     boolean agg_isNull_9 = agg_keyTerm_0.isNullAt(0);
/* 035 */     int agg_value_9 = agg_isNull_9 ?
/* 036 */     -1 : (agg_keyTerm_0.getInt(0));
/* 037 */     boolean agg_isNull_10 = agg_keyTerm_0.isNullAt(1);
/* 038 */     UTF8String agg_value_10 = agg_isNull_10 ?
/* 039 */     null : (agg_keyTerm_0.getUTF8String(1));
/* 040 */     filter_mutableStateArray_0[8].reset();
/* 041 */
/* 042 */     filter_mutableStateArray_0[8].zeroOutNullBytes();
/* 043 */
/* 044 */     if (agg_isNull_9) {
/* 045 */       filter_mutableStateArray_0[8].setNullAt(0);
/* 046 */     } else {
/* 047 */       filter_mutableStateArray_0[8].write(0, agg_value_9);
/* 048 */     }
/* 049 */
/* 050 */     if (agg_isNull_10) {
/* 051 */       filter_mutableStateArray_0[8].setNullAt(1);
/* 052 */     } else {
/* 053 */       filter_mutableStateArray_0[8].write(1, agg_value_10);
/* 054 */     }
/* 055 */     append((filter_mutableStateArray_0[8].getRow()));
/* 056 */
/* 057 */   }
/* 058 */
/* 059 */   private void wholestagecodegen_init_0_1() {
/* 060 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 061 */     filter_mutableStateArray_0[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 062 */     filter_mutableStateArray_0[7] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 063 */     filter_mutableStateArray_0[8] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 064 */
/* 065 */   }
/* 066 */
/* 067 */   private void agg_doConsume_0(int agg_expr_0_0, boolean agg_exprIsNull_0_0, UTF8String agg_expr_1_0, boolean agg_exprIsNull_1_0) throws java.io.IOException {
/* 068 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 069 */
/* 070 */     // generate grouping key
/* 071 */     filter_mutableStateArray_0[7].reset();
/* 072 */
/* 073 */     filter_mutableStateArray_0[7].zeroOutNullBytes();
/* 074 */
/* 075 */     if (agg_exprIsNull_0_0) {
/* 076 */       filter_mutableStateArray_0[7].setNullAt(0);
/* 077 */     } else {
/* 078 */       filter_mutableStateArray_0[7].write(0, agg_expr_0_0);
/* 079 */     }
/* 080 */
/* 081 */     if (agg_exprIsNull_1_0) {
/* 082 */       filter_mutableStateArray_0[7].setNullAt(1);
/* 083 */     } else {
/* 084 */       filter_mutableStateArray_0[7].write(1, agg_expr_1_0);
/* 085 */     }
/* 086 */     int agg_value_6 = 48;
/* 087 */
/* 088 */     if (!agg_exprIsNull_0_0) {
/* 089 */       agg_value_6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_expr_0_0, agg_value_6);
/* 090 */     }
/* 091 */
/* 092 */     if (!agg_exprIsNull_1_0) {
/* 093 */       agg_value_6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_1_0.getBaseObject(), agg_expr_1_0.getBaseOffset(), agg_expr_1_0.numBytes(), agg_value_6);
/* 094 */     }
/* 095 */     if (true) {
/* 096 */       // try to get the buffer from hash map
/* 097 */       agg_unsafeRowAggBuffer_0 =
/* 098 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[7].getRow()), agg_value_6);
/* 099 */     }
/* 100 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 101 */     // aggregation after processing all input rows.
/* 102 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 103 */       if (agg_sorter_0 == null) {
/* 104 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 105 */       } else {
/* 106 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 107 */       }
/* 108 */
/* 109 */       // the hash map had be spilled, it should have enough memory now,
/* 110 */       // try to allocate buffer again.
/* 111 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 112 */         (filter_mutableStateArray_0[7].getRow()), agg_value_6);
/* 113 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 114 */         // failed to allocate the first page
/* 115 */         throw new OutOfMemoryError("No enough memory for aggregation");
/* 116 */       }
/* 117 */     }
/* 118 */
/* 119 */     // common sub-expressions
/* 120 */
/* 121 */     // evaluate aggregate function
/* 122 */
/* 123 */     // update unsafe row buffer
/* 124 */
/* 125 */   }
/* 126 */
/* 127 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 128 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 129 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 130 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 131 */       do {
/* 132 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 133 */         int scan_value_1 = scan_isNull_1 ?
/* 134 */         -1 : (scan_row_0.getInt(1));
/* 135 */
/* 136 */         if (!(!scan_isNull_1)) continue;
/* 137 */
/* 138 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 139 */         int scan_value_0 = scan_isNull_0 ?
/* 140 */         -1 : (scan_row_0.getInt(0));
/* 141 */
/* 142 */         if (!(!scan_isNull_0)) continue;
/* 143 */
/* 144 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 145 */
/* 146 */         // generate join key for stream side
/* 147 */         boolean bhj_isNull_0 = false;
/* 148 */         long bhj_value_0 = -1L;
/* 149 */         if (!false) {
/* 150 */           bhj_value_0 = (long) scan_value_1;
/* 151 */         }
/* 152 */         // find matches from HashedRelation
/* 153 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 154 */         if (bhj_matched_0 != null) {
/* 155 */           {
/* 156 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);
/* 157 */
/* 158 */             // generate join key for stream side
/* 159 */             boolean bhj_isNull_8 = false;
/* 160 */             long bhj_value_8 = -1L;
/* 161 */             if (!false) {
/* 162 */               bhj_value_8 = (long) scan_value_0;
/* 163 */             }
/* 164 */             // find matches from HashRelation
/* 165 */             scala.collection.Iterator bhj_matches_0 = bhj_isNull_8 ? null : (scala.collection.Iterator)bhj_relation_1.get(bhj_value_8);
/* 166 */             if (bhj_matches_0 != null) {
/* 167 */               while (bhj_matches_0.hasNext()) {
/* 168 */                 UnsafeRow bhj_matched_1 = (UnsafeRow) bhj_matches_0.next();
/* 169 */                 {
/* 170 */                   ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);
/* 171 */
/* 172 */                   boolean bhj_isNull_2 = bhj_matched_0.isNullAt(0);
/* 173 */                   int bhj_value_2 = bhj_isNull_2 ?
/* 174 */                   -1 : (bhj_matched_0.getInt(0));
/* 175 */                   boolean bhj_isNull_3 = bhj_matched_0.isNullAt(1);
/* 176 */                   UTF8String bhj_value_3 = bhj_isNull_3 ?
/* 177 */                   null : (bhj_matched_0.getUTF8String(1));
/* 178 */
/* 179 */                   agg_doConsume_0(bhj_value_2, bhj_isNull_2, bhj_value_3, bhj_isNull_3);
/* 180 */
/* 181 */                 }
/* 182 */               }
/* 183 */             }
/* 184 */
/* 185 */           }
/* 186 */         }
/* 187 */
/* 188 */       } while(false);
/* 189 */       if (shouldStop()) return;
/* 190 */     }
/* 191 */
/* 192 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 193 */   }
/* 194 */
/* 195 */   protected void processNext() throws java.io.IOException {
/* 196 */     if (!agg_initAgg_0) {
/* 197 */       agg_initAgg_0 = true;
/* 198 */
/* 199 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 200 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 201 */       agg_doAggregateWithKeys_0();
/* 202 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 203 */     }
/* 204 */
/* 205 */     // output the result
/* 206 */
/* 207 */     while (agg_mapIter_0.next()) {
/* 208 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 209 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 210 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 211 */
/* 212 */       if (shouldStop()) return;
/* 213 */     }
/* 214 */
/* 215 */     agg_mapIter_0.close();
/* 216 */     if (agg_sorter_0 == null) {
/* 217 */       agg_hashMap_0.free();
/* 218 */     }
/* 219 */   }
/* 220 */
/* 221 */   private void wholestagecodegen_init_0_0() {
/* 222 */     scan_mutableStateArray_0[0] = inputs[0];
/* 223 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 224 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 225 */
/* 226 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[6] /* broadcast */).value()).asReadOnlyCopy();
/* 227 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 228 */
/* 229 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 230 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 231 */
/* 232 */     bhj_relation_1 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();
/* 233 */     incPeakExecutionMemory(bhj_relation_1.estimatedSize());
/* 234 */
/* 235 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 236 */
/* 237 */   }
/* 238 */
/* 239 */ }

2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 19.9882 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_32 stored as values in memory (estimated size 221.8 KB, free 1965.4 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_32 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_32 without replication took  0 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_32_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1965.4 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_32_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_32_piece0 locally took  16 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_32_piece0 without replication took  16 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 32 from count at UseCase5.java:65
2022-02-10 15:13:41 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4195382 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:41 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 9.2306 ms
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$9.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(scala.Product2)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$13.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.sampleSizePerPartition$2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final scala.reflect.ClassTag org.apache.spark.RangePartitioner$$anonfun$13.evidence$5$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.shift$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.RangePartitioner$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[75] at count at UseCase5.java:65
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[75] at count at UseCase5.java:65)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[75] at count at UseCase5.java:65
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[75] at count at UseCase5.java:65)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:41 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:41 INFO  SparkContext:54 - Starting job: count at UseCase5.java:65
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Registering RDD 70 (count at UseCase5.java:65) as input to shuffle 2
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Got job 14 (count at UseCase5.java:65) with 200 output partitions
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Final stage: ResultStage 17 (count at UseCase5.java:65)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 16)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 16)
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitStage(ResultStage 17 (name=count at UseCase5.java:65;jobs=14))
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 16)
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 16 (name=count at UseCase5.java:65;jobs=14))
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 16 (MapPartitionsRDD[70] at count at UseCase5.java:65), which has no missing parents
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 16)
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_33 stored as values in memory (estimated size 31.0 KB, free 1965.4 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_33 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_33 without replication took  0 ms
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_33_piece0 stored as bytes in memory (estimated size 13.7 KB, free 1965.3 MB)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 13.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_33_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_33_piece0 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_33_piece0 without replication took  0 ms
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 33 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[70] at count at UseCase5.java:65) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Adding task set 16.0 with 1 tasks
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Epoch for TaskSet 16.0: 2
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 16.0: NO_PREF, ANY
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8311 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 0.0 in stage 16.0 (TID 16)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_33
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_33 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for pmod(hash(input[0, int, true], input[1, string, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       int value_2 = isNull_2 ?
/* 039 */       -1 : (i.getInt(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 042 */       }
/* 043 */       boolean isNull_3 = i.isNullAt(1);
/* 044 */       UTF8String value_3 = isNull_3 ?
/* 045 */       null : (i.getUTF8String(1));
/* 046 */       if (!isNull_3) {
/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_3.getBaseObject(), value_3.getBaseOffset(), value_3.numBytes(), value_1);
/* 048 */       }
/* 049 */
/* 050 */       int remainder_0 = value_1 % 200;
/* 051 */       if (remainder_0 < 0) {
/* 052 */         value_0=(remainder_0 + 200) % 200;
/* 053 */       } else {
/* 054 */         value_0=remainder_0;
/* 055 */       }
/* 056 */
/* 057 */     }
/* 058 */     if (isNull_0) {
/* 059 */       mutableStateArray_0[0].setNullAt(0);
/* 060 */     } else {
/* 061 */       mutableStateArray_0[0].write(0, value_0);
/* 062 */     }
/* 063 */     return (mutableStateArray_0[0].getRow());
/* 064 */   }
/* 065 */
/* 066 */
/* 067 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       int value_2 = isNull_2 ?
/* 039 */       -1 : (i.getInt(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 042 */       }
/* 043 */       boolean isNull_3 = i.isNullAt(1);
/* 044 */       UTF8String value_3 = isNull_3 ?
/* 045 */       null : (i.getUTF8String(1));
/* 046 */       if (!isNull_3) {
/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_3.getBaseObject(), value_3.getBaseOffset(), value_3.numBytes(), value_1);
/* 048 */       }
/* 049 */
/* 050 */       int remainder_0 = value_1 % 200;
/* 051 */       if (remainder_0 < 0) {
/* 052 */         value_0=(remainder_0 + 200) % 200;
/* 053 */       } else {
/* 054 */         value_0=remainder_0;
/* 055 */       }
/* 056 */
/* 057 */     }
/* 058 */     if (isNull_0) {
/* 059 */       mutableStateArray_0[0].setNullAt(0);
/* 060 */     } else {
/* 061 */       mutableStateArray_0[0].write(0, value_0);
/* 062 */     }
/* 063 */     return (mutableStateArray_0[0].getRow());
/* 064 */   }
/* 065 */
/* 066 */
/* 067 */ }

2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 11.5034 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 16 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4baecb4e
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/categories/part-00000, range: 0-1078, partition values: [empty row]
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     int value_1 = isNull_1 ?
/* 042 */     -1 : (i.getInt(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     int value_1 = isNull_1 ?
/* 042 */     -1 : (i.getInt(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 INFO  CodeGenerator:54 - Code generated in 5.9547 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_32
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_32 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 16 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@4baecb4e
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(309)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 309
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 309
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(253)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 253
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 253
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(248)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 248
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 248
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(308)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 308
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 308
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(524)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 524
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 524
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(520)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 520
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 520
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(280)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 280
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 280
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(350)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 350
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 350
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(277)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 277
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 277
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(333)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 333
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 333
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(260)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 260
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 260
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(269)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 269
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 269
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(233)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 233
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 233
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(347)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 347
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 347
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(549)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 549
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 549
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(552)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 552
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 552
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(318)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 318
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 318
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(378)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 378
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 378
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(412)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 412
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 412
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(385)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 385
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 385
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(516)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 516
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 516
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(246)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 246
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 246
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(386)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 386
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 386
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(402)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 402
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 402
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(222)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 222
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 222
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(255)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 255
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 255
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(292)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 292
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 292
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(334)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 334
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 334
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(252)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 252
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 252
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(249)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 249
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 249
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(407)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 407
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 407
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(415)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 415
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 415
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(389)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 389
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 389
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(256)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 256
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 256
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(295)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 295
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 295
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(262)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 262
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 262
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(368)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 368
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 368
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(335)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 335
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 335
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(322)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 322
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 322
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(426)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 426
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 426
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(398)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 398
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 398
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(330)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 330
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 330
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(200)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 200
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 200
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(536)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 536
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 536
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(196)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 196
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 196
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(230)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 230
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 230
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(436)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 436
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 436
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(510)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 510
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 510
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(236)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 236
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 236
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(359)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 359
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 359
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(509)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 509
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 509
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(416)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 416
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 416
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(238)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 238
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 238
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(348)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 348
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 348
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(237)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 237
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 237
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(301)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 301
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 301
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(423)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 423
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 423
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(375)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 375
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 375
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(383)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 383
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 383
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(372)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 372
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 372
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(329)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 329
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 329
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(228)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 228
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 228
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(226)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 226
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 226
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(298)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 298
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 298
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(214)
2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 16 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4baecb4e
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 214
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 214
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(266)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 266
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 266
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(364)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 364
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 364
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(545)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 545
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 545
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(257)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 257
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 257
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(418)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 418
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 418
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(23)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 23
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 23
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 23
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 23
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_23
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_23 of size 9144 dropped from memory (free 1993707244)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_23_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_23_piece0 of size 4742 dropped from memory (free 1993711986)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 23, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 23
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(517)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 517
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 517
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(551)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 551
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 551
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(547)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 547
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 547
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(390)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 390
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 390
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(208)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 208
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 208
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(539)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 539
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 539
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(310)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 310
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 310
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(299)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 299
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 299
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(293)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 293
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 293
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(503)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 503
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 503
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(374)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 374
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 374
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(297)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 297
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 297
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(542)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 542
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 542
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(288)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 288
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 288
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(537)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 537
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 537
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(343)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 343
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 343
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(29)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 29
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 29
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 29
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 29
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_29_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_29_piece0 of size 6145 dropped from memory (free 1993718131)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_29_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 6.0 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_29_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_29
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_29 of size 11432 dropped from memory (free 1993729563)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 29, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 29
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(273)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 273
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 273
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(518)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 518
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 518
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(284)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 284
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 284
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(409)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 409
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 409
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(394)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 394
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 394
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(285)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 285
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 285
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(370)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 370
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 370
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(328)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 328
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 328
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(315)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 315
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 315
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(534)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 534
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 534
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(235)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 235
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 235
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(11)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 11
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 11
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 11
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 11
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_11
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_11 of size 227072 dropped from memory (free 1993956635)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_11_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_11_piece0 of size 21155 dropped from memory (free 1993977790)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 11, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 11
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(336)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 336
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 336
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(271)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 271
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 271
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(14)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 14
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 14
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 14
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 14
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_14
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_14 of size 227232 dropped from memory (free 1994205022)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_14_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_14_piece0 of size 21170 dropped from memory (free 1994226192)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 14, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 14
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(355)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 355
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 355
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(404)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 404
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 404
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(363)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 363
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 363
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(513)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 513
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 513
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(366)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 366
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 366
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(354)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 354
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 354
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(216)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 216
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 216
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(289)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 289
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 289
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(326)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 326
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 326
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(251)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 251
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 251
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(396)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 396
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 396
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(535)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 535
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 535
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(215)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 215
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 215
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(229)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 229
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 229
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(24)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 24
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 24
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 24
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 24
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_24
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_24 of size 227232 dropped from memory (free 1994453424)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_24_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_24_piece0 of size 21170 dropped from memory (free 1994474594)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_24_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 24, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 24
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(268)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 268
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 268
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(346)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 346
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 346
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(441)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 441
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 441
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(201)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 201
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 201
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(194)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 194
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 194
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(395)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 395
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 395
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(244)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 244
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 244
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(382)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 382
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 382
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(434)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 434
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 434
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(362)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 362
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 362
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(282)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 282
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 282
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(433)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 433
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 433
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(519)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 519
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 519
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(19)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 19
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 19
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 19
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 19
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_19_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_19_piece0 of size 4742 dropped from memory (free 1994479336)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_19
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_19 of size 9144 dropped from memory (free 1994488480)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 19, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 19
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(403)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 403
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 403
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(345)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 345
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 345
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(430)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 430
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 430
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(205)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 205
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 205
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(411)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 411
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 411
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(528)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 528
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 528
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(327)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 327
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 327
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(342)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 342
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 342
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(25)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 25
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 25
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 25
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 25
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_25
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_25 of size 14312 dropped from memory (free 1994502792)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_25_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_25_piece0 of size 7754 dropped from memory (free 1994510546)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_25_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 25, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 25
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(20)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 20
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 20
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 20
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 20
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_20_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_20_piece0 of size 21170 dropped from memory (free 1994531716)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_20
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_20 of size 227232 dropped from memory (free 1994758948)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 20, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 20
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(332)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 332
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 332
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(259)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 259
2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 16 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@4baecb4e
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 259
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(371)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 371
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 371
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(369)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 369
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 369
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(353)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 353
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 353
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(387)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 387
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 387
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(527)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 527
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 527
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(283)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 283
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 283
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(325)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 325
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 325
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(442)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 442
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 442
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(358)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 358
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 358
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(217)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 217
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 217
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(272)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 272
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 272
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(313)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 313
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 313
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(429)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 429
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 429
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(339)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 339
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 339
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(508)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 508
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 508
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(192)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 192
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 192
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(515)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 515
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 515
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(209)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 209
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 209
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(306)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 306
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 306
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(543)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 543
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 543
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(421)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 421
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 421
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(414)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 414
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 414
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(356)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 356
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 356
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(258)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 258
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 258
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(302)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 302
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 302
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(546)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 546
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 546
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(286)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 286
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 286
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(199)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 199
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 199
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(316)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 316
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 316
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(241)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 241
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 241
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(377)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 377
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 377
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(278)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 278
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 278
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(532)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 532
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 532
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(399)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 399
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 399
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(367)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 367
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 367
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(420)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 420
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 420
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(16)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 16
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 16
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 16
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 16
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_16
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_16 of size 227232 dropped from memory (free 2062095044)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_16_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_16_piece0 of size 21170 dropped from memory (free 2062116214)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 16, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 16
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(419)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 419
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 419
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(197)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 197
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 197
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(548)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 548
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 548
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(17)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 17
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 17
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 17
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 17
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_17
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_17 of size 14232 dropped from memory (free 2062130446)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_17_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_17_piece0 of size 7715 dropped from memory (free 2062138161)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 7.5 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 17, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 17
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(443)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 443
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 443
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(425)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 425
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 425
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(18)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 18
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 18
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 18
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 18
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_18_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_18_piece0 of size 21170 dropped from memory (free 2062159331)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_18
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_18 of size 227232 dropped from memory (free 2062386563)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 18, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 18
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(538)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 538
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 538
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(220)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 220
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 220
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(533)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 533
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 533
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(1)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning shuffle 1
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned shuffle 1
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 1
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(410)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 410
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 410
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(427)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 427
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 427
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(530)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 530
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 530
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(279)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 279
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 279
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(305)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 305
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 305
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(191)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 191
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 191
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(290)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 290
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 290
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(397)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 397
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 397
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(247)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 247
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 247
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(428)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 428
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 428
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(349)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 349
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 349
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(22)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 22
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 22
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 22
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 22
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_22
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_22 of size 227232 dropped from memory (free 2062613795)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_22_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_22_piece0 of size 21170 dropped from memory (free 2062634965)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 22, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 22
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(320)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 320
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 320
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(202)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 202
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 202
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(234)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 234
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 234
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(521)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 521
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 521
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(540)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 540
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 540
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(544)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 544
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 544
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(263)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 263
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 263
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(365)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 365
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 365
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(307)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 307
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 307
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(296)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 296
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 296
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(265)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 265
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 1, response is true
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 265
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(526)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 526
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 526
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(281)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 281
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 281
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(240)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 240
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 240
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(245)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 245
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 245
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(338)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 338
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 338
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(400)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 400
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 400
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(405)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 405
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 405
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(232)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 232
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 232
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(311)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 311
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 311
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(379)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 379
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 379
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(392)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 392
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 392
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(224)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 224
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 224
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(505)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 505
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 505
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(203)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 203
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 203
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(439)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 439
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 439
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(207)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 207
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 207
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(225)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 225
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 225
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(275)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 275
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 275
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(300)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 300
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 300
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(514)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 514
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 514
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(221)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 221
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 221
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(337)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 337
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 337
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(267)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 267
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 267
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(218)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 218
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 218
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(223)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 223
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 223
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(384)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 384
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 384
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(324)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 324
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 324
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(211)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 211
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 211
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(393)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 393
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 393
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(504)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 504
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 504
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(210)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 210
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 210
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(243)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 243
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 243
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(264)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 264
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 264
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(373)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 373
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 373
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(512)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 512
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 512
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(261)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 261
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 261
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(239)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 239
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 239
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(507)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 507
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 507
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(391)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 391
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 391
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(206)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 206
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 206
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(388)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 388
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 388
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(314)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 314
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 314
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(424)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 424
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 424
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(204)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 204
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 204
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(360)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 360
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 360
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(437)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 437
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 437
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(357)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 357
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 357
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(304)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 304
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 304
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(406)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 406
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 406
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(28)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 28
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 28
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 28
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 28
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_28_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_28_piece0 of size 6107 dropped from memory (free 2062641072)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_28_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 6.0 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_28_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_28
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_28 of size 11272 dropped from memory (free 2062652344)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 28, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 0.0 in stage 16.0 (TID 16). 3937 bytes result sent to driver
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 28
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(231)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 231
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 231
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(435)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 435
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 435
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(276)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 276
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 276
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(287)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 287
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 287
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(511)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 511
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 511
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(303)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 303
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 303
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(431)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 431
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 431
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(291)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 291
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 291
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(531)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 531
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 531
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(212)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 212
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 156 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 212
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(447)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 447
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 447
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(15)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 15
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 15
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:41 INFO  DAGScheduler:54 - ShuffleMapStage 16 (count at UseCase5.java:65) finished in 0.172 s
2022-02-10 15:13:41 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:41 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - waiting: Set(ResultStage 17)
2022-02-10 15:13:41 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 3
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitStage(ResultStage 17 (name=count at UseCase5.java:65;jobs=14))
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting ResultStage 17 (MapPartitionsRDD[75] at count at UseCase5.java:65), which has no missing parents
2022-02-10 15:13:41 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 17)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 15
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 15
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_15
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_15 of size 9144 dropped from memory (free 2062661488)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_15_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_15_piece0 of size 4742 dropped from memory (free 2062666230)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 15, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 15
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(321)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 321
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 321
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(274)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 274
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 274
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(506)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 506
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 506
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(541)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 541
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 541
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(422)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 422
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 422
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(12)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 12
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 12
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 12
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 12
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_12_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_12_piece0 of size 6934 dropped from memory (free 2062673164)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 6.8 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_12
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_12 of size 12928 dropped from memory (free 2062686092)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 12, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 12
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(317)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 317
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 317
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(213)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 213
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 213
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(341)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 341
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 341
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(195)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 195
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 195
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(401)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 401
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 401
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(413)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 413
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 413
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(417)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 417
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 417
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(522)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 522
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 522
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(227)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 227
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 227
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(21)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 21
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 21
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 21
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 21
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_21_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_21_piece0 of size 7727 dropped from memory (free 2062693819)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_21
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_21 of size 14256 dropped from memory (free 2062708075)
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 21, response is 0
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 21
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(525)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 525
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 525
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(438)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 438
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 438
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(523)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 523
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 523
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(432)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 432
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 432
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(361)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 361
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 361
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(351)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 351
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 351
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(312)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 312
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 312
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(270)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 270
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 270
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(352)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 352
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 352
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(550)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 550
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 550
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(219)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 219
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 219
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(408)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 408
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 408
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(440)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 440
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 440
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(319)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 319
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 319
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(323)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 323
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 323
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(340)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 340
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 340
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(254)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 254
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 254
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(331)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 331
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 331
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(529)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 529
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 529
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(344)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 344
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 344
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(193)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 193
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 193
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(242)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 242
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 242
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(381)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 381
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 381
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(376)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 376
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 376
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(198)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 198
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 198
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(294)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 294
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 294
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_34 stored as values in memory (estimated size 30.6 KB, free 1967.1 MB)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(13)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning broadcast 13
2022-02-10 15:13:41 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 13
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_34 locally took  0 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_34 without replication took  0 ms
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 13
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing broadcast 13
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_13_piece0
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_13_piece0 of size 4008 dropped from memory (free 2062680763)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 15:13:41 INFO  MemoryStore:54 - Block broadcast_34_piece0 stored as bytes in memory (estimated size 14.2 KB, free 1967.1 MB)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Removing block broadcast_13
2022-02-10 15:13:41 DEBUG MemoryStore:58 - Block broadcast_13 of size 7432 dropped from memory (free 2062673621)
2022-02-10 15:13:41 INFO  BlockManagerInfo:54 - Added broadcast_34_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 14.2 KB, free: 1970.3 MB)
2022-02-10 15:13:41 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_34_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Told master about block broadcast_34_piece0
2022-02-10 15:13:41 DEBUG BlockManager:58 - Put block broadcast_34_piece0 locally took  1 ms
2022-02-10 15:13:41 DEBUG BlockManager:58 - Putting block broadcast_34_piece0 without replication took  1 ms
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 13, response is 0
2022-02-10 15:13:41 INFO  SparkContext:54 - Created broadcast 34 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:41 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaned broadcast 13
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(250)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 250
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 250
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(380)
2022-02-10 15:13:41 DEBUG ContextCleaner:58 - Cleaning accumulator 380
2022-02-10 15:13:41 INFO  ContextCleaner:54 - Cleaned accumulator 380
2022-02-10 15:13:41 INFO  DAGScheduler:54 - Submitting 200 missing tasks from ResultStage 17 (MapPartitionsRDD[75] at count at UseCase5.java:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2022-02-10 15:13:41 INFO  TaskSchedulerImpl:54 - Adding task set 17.0 with 200 tasks
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Epoch for TaskSet 17.0: 3
2022-02-10 15:13:41 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 17.0: NO_PREF, ANY
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 0.0 in stage 17.0 (TID 17)
2022-02-10 15:13:41 DEBUG BlockManager:58 - Getting local block broadcast_34
2022-02-10 15:13:41 DEBUG BlockManager:58 - Level for block broadcast_34 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 0-1
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 17 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@49a8b6d5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 17 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@49a8b6d5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 0.0 in stage 17.0 (TID 17). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 1.0 in stage 17.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 1.0 in stage 17.0 (TID 18)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 16 ms on localhost (executor driver) (1/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 1-2
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 18 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2c1ac0db
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 18 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2c1ac0db
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 1.0 in stage 17.0 (TID 18). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 2.0 in stage 17.0 (TID 19, localhost, executor driver, partition 2, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 2.0 in stage 17.0 (TID 19)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 1.0 in stage 17.0 (TID 18) in 16 ms on localhost (executor driver) (2/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 2-3
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 19 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7060b3ef
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 19 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7060b3ef
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 2.0 in stage 17.0 (TID 19). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 3.0 in stage 17.0 (TID 20, localhost, executor driver, partition 3, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 3.0 in stage 17.0 (TID 20)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 2.0 in stage 17.0 (TID 19) in 0 ms on localhost (executor driver) (3/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 3-4
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 20 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7dacf286
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 20 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7dacf286
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 3.0 in stage 17.0 (TID 20). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 4.0 in stage 17.0 (TID 21, localhost, executor driver, partition 4, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 4.0 in stage 17.0 (TID 21)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 3.0 in stage 17.0 (TID 20) in 16 ms on localhost (executor driver) (4/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 4-5
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 21 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5a51724d
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 21 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5a51724d
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 4.0 in stage 17.0 (TID 21). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 5.0 in stage 17.0 (TID 22, localhost, executor driver, partition 5, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 5.0 in stage 17.0 (TID 22)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 4.0 in stage 17.0 (TID 21) in 0 ms on localhost (executor driver) (5/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 5-6
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 22 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5d532f42
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 22 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5d532f42
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 5.0 in stage 17.0 (TID 22). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 6.0 in stage 17.0 (TID 23, localhost, executor driver, partition 6, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 6.0 in stage 17.0 (TID 23)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 5.0 in stage 17.0 (TID 22) in 16 ms on localhost (executor driver) (6/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 6-7
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 23 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2d9ef3fd
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 23 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2d9ef3fd
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 6.0 in stage 17.0 (TID 23). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 7.0 in stage 17.0 (TID 24, localhost, executor driver, partition 7, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 7.0 in stage 17.0 (TID 24)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 6.0 in stage 17.0 (TID 23) in 0 ms on localhost (executor driver) (7/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 7-8
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 24 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30dd264d
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 24 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30dd264d
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 7.0 in stage 17.0 (TID 24). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 8.0 in stage 17.0 (TID 25, localhost, executor driver, partition 8, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 8.0 in stage 17.0 (TID 25)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 7.0 in stage 17.0 (TID 24) in 16 ms on localhost (executor driver) (8/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 8-9
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 25 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5df59722
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 25 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5df59722
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 8.0 in stage 17.0 (TID 25). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 9.0 in stage 17.0 (TID 26, localhost, executor driver, partition 9, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 9.0 in stage 17.0 (TID 26)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 8.0 in stage 17.0 (TID 25) in 0 ms on localhost (executor driver) (9/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 9-10
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 26 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2f364ff5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 26 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2f364ff5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 9.0 in stage 17.0 (TID 26). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 10.0 in stage 17.0 (TID 27, localhost, executor driver, partition 10, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 10.0 in stage 17.0 (TID 27)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 9.0 in stage 17.0 (TID 26) in 16 ms on localhost (executor driver) (10/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 10-11
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 27 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@58dfd6f2
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 27 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@58dfd6f2
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 10.0 in stage 17.0 (TID 27). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 11.0 in stage 17.0 (TID 28, localhost, executor driver, partition 11, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 11.0 in stage 17.0 (TID 28)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 10.0 in stage 17.0 (TID 27) in 0 ms on localhost (executor driver) (11/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 11-12
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 28 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@474cf79c
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 28 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@474cf79c
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 11.0 in stage 17.0 (TID 28). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 12.0 in stage 17.0 (TID 29, localhost, executor driver, partition 12, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 12.0 in stage 17.0 (TID 29)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 11.0 in stage 17.0 (TID 28) in 16 ms on localhost (executor driver) (12/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 12-13
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 29 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2bad8b00
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 29 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2bad8b00
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 12.0 in stage 17.0 (TID 29). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 13.0 in stage 17.0 (TID 30, localhost, executor driver, partition 13, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 12.0 in stage 17.0 (TID 29) in 0 ms on localhost (executor driver) (13/200)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 13.0 in stage 17.0 (TID 30)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 13-14
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 30 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7316007d
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 30 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7316007d
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 13.0 in stage 17.0 (TID 30). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 14.0 in stage 17.0 (TID 31, localhost, executor driver, partition 14, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 14.0 in stage 17.0 (TID 31)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 13.0 in stage 17.0 (TID 30) in 16 ms on localhost (executor driver) (14/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 14-15
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 31 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@481e81d7
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 31 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@481e81d7
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 14.0 in stage 17.0 (TID 31). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 15.0 in stage 17.0 (TID 32, localhost, executor driver, partition 15, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 15.0 in stage 17.0 (TID 32)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 14.0 in stage 17.0 (TID 31) in 0 ms on localhost (executor driver) (15/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 15-16
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 32 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6fed8fbb
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 32 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6fed8fbb
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 15.0 in stage 17.0 (TID 32). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 16.0 in stage 17.0 (TID 33, localhost, executor driver, partition 16, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 16.0 in stage 17.0 (TID 33)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 15.0 in stage 17.0 (TID 32) in 17 ms on localhost (executor driver) (16/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 16-17
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 33 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@776fd858
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 33 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@776fd858
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 16.0 in stage 17.0 (TID 33). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 17.0 in stage 17.0 (TID 34, localhost, executor driver, partition 17, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 17.0 in stage 17.0 (TID 34)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 16.0 in stage 17.0 (TID 33) in 0 ms on localhost (executor driver) (17/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 17-18
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 34 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4519d6ac
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 34 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4519d6ac
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 17.0 in stage 17.0 (TID 34). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 18.0 in stage 17.0 (TID 35, localhost, executor driver, partition 18, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 18.0 in stage 17.0 (TID 35)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 17.0 in stage 17.0 (TID 34) in 15 ms on localhost (executor driver) (18/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 18-19
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 35 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3de06de2
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 35 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3de06de2
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 18.0 in stage 17.0 (TID 35). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 19.0 in stage 17.0 (TID 36, localhost, executor driver, partition 19, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 19.0 in stage 17.0 (TID 36)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 18.0 in stage 17.0 (TID 35) in 0 ms on localhost (executor driver) (19/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 19-20
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 36 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7921c7f5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 36 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7921c7f5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 19.0 in stage 17.0 (TID 36). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 20.0 in stage 17.0 (TID 37, localhost, executor driver, partition 20, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 20.0 in stage 17.0 (TID 37)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 19.0 in stage 17.0 (TID 36) in 18 ms on localhost (executor driver) (20/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 20-21
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 37 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@69bd1ef
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 37 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@69bd1ef
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 20.0 in stage 17.0 (TID 37). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 21.0 in stage 17.0 (TID 38, localhost, executor driver, partition 21, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 21.0 in stage 17.0 (TID 38)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 20.0 in stage 17.0 (TID 37) in 3 ms on localhost (executor driver) (21/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 21-22
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 38 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@675aabe1
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 38 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@675aabe1
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 21.0 in stage 17.0 (TID 38). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 22.0 in stage 17.0 (TID 39, localhost, executor driver, partition 22, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 22.0 in stage 17.0 (TID 39)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 21.0 in stage 17.0 (TID 38) in 0 ms on localhost (executor driver) (22/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 22-23
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 39 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@381d5ba3
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 39 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@381d5ba3
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 22.0 in stage 17.0 (TID 39). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 23.0 in stage 17.0 (TID 40, localhost, executor driver, partition 23, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 23.0 in stage 17.0 (TID 40)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 22.0 in stage 17.0 (TID 39) in 16 ms on localhost (executor driver) (23/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 23-24
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 40 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@63a3609c
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 40 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@63a3609c
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 23.0 in stage 17.0 (TID 40). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 24.0 in stage 17.0 (TID 41, localhost, executor driver, partition 24, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 24.0 in stage 17.0 (TID 41)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 23.0 in stage 17.0 (TID 40) in 0 ms on localhost (executor driver) (24/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 24-25
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 41 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@72b8783a
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 41 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@72b8783a
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 24.0 in stage 17.0 (TID 41). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 25.0 in stage 17.0 (TID 42, localhost, executor driver, partition 25, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 25.0 in stage 17.0 (TID 42)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 24.0 in stage 17.0 (TID 41) in 0 ms on localhost (executor driver) (25/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 25-26
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 2 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  2 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 42 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@130e25ab
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 42 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@130e25ab
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 25.0 in stage 17.0 (TID 42). 4539 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 26.0 in stage 17.0 (TID 43, localhost, executor driver, partition 26, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 26.0 in stage 17.0 (TID 43)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 25.0 in stage 17.0 (TID 42) in 18 ms on localhost (executor driver) (26/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 26-27
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 43 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5bf2f878
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 43 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5bf2f878
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 26.0 in stage 17.0 (TID 43). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 27.0 in stage 17.0 (TID 44, localhost, executor driver, partition 27, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 27.0 in stage 17.0 (TID 44)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 26.0 in stage 17.0 (TID 43) in 0 ms on localhost (executor driver) (27/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 27-28
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 44 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@368a02de
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 44 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@368a02de
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 27.0 in stage 17.0 (TID 44). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 28.0 in stage 17.0 (TID 45, localhost, executor driver, partition 28, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 28.0 in stage 17.0 (TID 45)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 27.0 in stage 17.0 (TID 44) in 16 ms on localhost (executor driver) (28/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 28-29
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 45 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54f09e88
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 45 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54f09e88
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 28.0 in stage 17.0 (TID 45). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 29.0 in stage 17.0 (TID 46, localhost, executor driver, partition 29, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 29.0 in stage 17.0 (TID 46)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 28.0 in stage 17.0 (TID 45) in 0 ms on localhost (executor driver) (29/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 29-30
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 46 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6a3e987
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 46 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6a3e987
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 29.0 in stage 17.0 (TID 46). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 30.0 in stage 17.0 (TID 47, localhost, executor driver, partition 30, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 30.0 in stage 17.0 (TID 47)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 29.0 in stage 17.0 (TID 46) in 16 ms on localhost (executor driver) (30/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 30-31
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 47 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@74afa75a
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 47 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@74afa75a
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 30.0 in stage 17.0 (TID 47). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 31.0 in stage 17.0 (TID 48, localhost, executor driver, partition 31, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 31.0 in stage 17.0 (TID 48)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 30.0 in stage 17.0 (TID 47) in 0 ms on localhost (executor driver) (31/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 31-32
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 48 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@56083d6
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 48 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@56083d6
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 31.0 in stage 17.0 (TID 48). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 32.0 in stage 17.0 (TID 49, localhost, executor driver, partition 32, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 32.0 in stage 17.0 (TID 49)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 31.0 in stage 17.0 (TID 48) in 16 ms on localhost (executor driver) (32/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 32-33
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 49 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4c37f9d9
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 49 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4c37f9d9
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 32.0 in stage 17.0 (TID 49). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 33.0 in stage 17.0 (TID 50, localhost, executor driver, partition 33, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 33.0 in stage 17.0 (TID 50)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 32.0 in stage 17.0 (TID 49) in 0 ms on localhost (executor driver) (33/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 33-34
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 50 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5e24f3dc
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 50 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5e24f3dc
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 33.0 in stage 17.0 (TID 50). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 34.0 in stage 17.0 (TID 51, localhost, executor driver, partition 34, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 33.0 in stage 17.0 (TID 50) in 15 ms on localhost (executor driver) (34/200)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 34.0 in stage 17.0 (TID 51)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 34-35
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 51 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1579da94
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 51 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1579da94
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 34.0 in stage 17.0 (TID 51). 4453 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 35.0 in stage 17.0 (TID 52, localhost, executor driver, partition 35, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 35.0 in stage 17.0 (TID 52)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 34.0 in stage 17.0 (TID 51) in 17 ms on localhost (executor driver) (35/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 35-36
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 52 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@117e8e82
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 52 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@117e8e82
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 35.0 in stage 17.0 (TID 52). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 36.0 in stage 17.0 (TID 53, localhost, executor driver, partition 36, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 36.0 in stage 17.0 (TID 53)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 35.0 in stage 17.0 (TID 52) in 17 ms on localhost (executor driver) (36/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 36-37
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 53 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6b6d13ae
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 53 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6b6d13ae
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 36.0 in stage 17.0 (TID 53). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 37.0 in stage 17.0 (TID 54, localhost, executor driver, partition 37, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 37.0 in stage 17.0 (TID 54)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 36.0 in stage 17.0 (TID 53) in 1 ms on localhost (executor driver) (37/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 37-38
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 54 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3db2835d
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 54 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3db2835d
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 37.0 in stage 17.0 (TID 54). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 38.0 in stage 17.0 (TID 55, localhost, executor driver, partition 38, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 37.0 in stage 17.0 (TID 54) in 16 ms on localhost (executor driver) (38/200)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 38.0 in stage 17.0 (TID 55)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 38-39
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 55 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@51471b0c
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 55 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@51471b0c
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 38.0 in stage 17.0 (TID 55). 4453 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 39.0 in stage 17.0 (TID 56, localhost, executor driver, partition 39, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 39.0 in stage 17.0 (TID 56)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 38.0 in stage 17.0 (TID 55) in 17 ms on localhost (executor driver) (39/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 39-40
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 56 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@37037c9b
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 56 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@37037c9b
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 39.0 in stage 17.0 (TID 56). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 40.0 in stage 17.0 (TID 57, localhost, executor driver, partition 40, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 40.0 in stage 17.0 (TID 57)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 39.0 in stage 17.0 (TID 56) in 0 ms on localhost (executor driver) (40/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 40-41
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 57 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@bc53514
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 57 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@bc53514
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 40.0 in stage 17.0 (TID 57). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 41.0 in stage 17.0 (TID 58, localhost, executor driver, partition 41, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 41.0 in stage 17.0 (TID 58)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 40.0 in stage 17.0 (TID 57) in 17 ms on localhost (executor driver) (41/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 41-42
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 58 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@573771a6
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 58 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@573771a6
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 41.0 in stage 17.0 (TID 58). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 42.0 in stage 17.0 (TID 59, localhost, executor driver, partition 42, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 42.0 in stage 17.0 (TID 59)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 41.0 in stage 17.0 (TID 58) in 0 ms on localhost (executor driver) (42/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 42-43
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 59 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@177c571c
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 59 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@177c571c
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 42.0 in stage 17.0 (TID 59). 4539 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 43.0 in stage 17.0 (TID 60, localhost, executor driver, partition 43, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 43.0 in stage 17.0 (TID 60)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 42.0 in stage 17.0 (TID 59) in 16 ms on localhost (executor driver) (43/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 43-44
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 60 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30f4725a
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 60 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30f4725a
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 43.0 in stage 17.0 (TID 60). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 44.0 in stage 17.0 (TID 61, localhost, executor driver, partition 44, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 44.0 in stage 17.0 (TID 61)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 43.0 in stage 17.0 (TID 60) in 0 ms on localhost (executor driver) (44/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 44-45
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 61 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7c76ff68
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 61 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7c76ff68
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 44.0 in stage 17.0 (TID 61). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 45.0 in stage 17.0 (TID 62, localhost, executor driver, partition 45, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 45.0 in stage 17.0 (TID 62)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 44.0 in stage 17.0 (TID 61) in 16 ms on localhost (executor driver) (45/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 45-46
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 62 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4cde50ed
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 62 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4cde50ed
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 45.0 in stage 17.0 (TID 62). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 46.0 in stage 17.0 (TID 63, localhost, executor driver, partition 46, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 46.0 in stage 17.0 (TID 63)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 45.0 in stage 17.0 (TID 62) in 0 ms on localhost (executor driver) (46/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 46-47
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 63 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@39f9c7c3
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 63 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@39f9c7c3
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 46.0 in stage 17.0 (TID 63). 4539 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 47.0 in stage 17.0 (TID 64, localhost, executor driver, partition 47, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 47.0 in stage 17.0 (TID 64)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 46.0 in stage 17.0 (TID 63) in 16 ms on localhost (executor driver) (47/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 47-48
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 64 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3e7854f4
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 64 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3e7854f4
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 47.0 in stage 17.0 (TID 64). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 48.0 in stage 17.0 (TID 65, localhost, executor driver, partition 48, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 48.0 in stage 17.0 (TID 65)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 47.0 in stage 17.0 (TID 64) in 0 ms on localhost (executor driver) (48/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 48-49
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 65 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7bc257a7
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 65 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7bc257a7
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 48.0 in stage 17.0 (TID 65). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 49.0 in stage 17.0 (TID 66, localhost, executor driver, partition 49, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 49.0 in stage 17.0 (TID 66)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 48.0 in stage 17.0 (TID 65) in 16 ms on localhost (executor driver) (49/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 49-50
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 66 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2891b5ad
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 66 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2891b5ad
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 49.0 in stage 17.0 (TID 66). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 50.0 in stage 17.0 (TID 67, localhost, executor driver, partition 50, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 50.0 in stage 17.0 (TID 67)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 49.0 in stage 17.0 (TID 66) in 0 ms on localhost (executor driver) (50/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 50-51
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 67 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2a5b31e5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 67 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2a5b31e5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 50.0 in stage 17.0 (TID 67). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 51.0 in stage 17.0 (TID 68, localhost, executor driver, partition 51, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 51.0 in stage 17.0 (TID 68)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 50.0 in stage 17.0 (TID 67) in 15 ms on localhost (executor driver) (51/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 51-52
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 68 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4ea26974
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 68 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4ea26974
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 51.0 in stage 17.0 (TID 68). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 52.0 in stage 17.0 (TID 69, localhost, executor driver, partition 52, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 52.0 in stage 17.0 (TID 69)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 51.0 in stage 17.0 (TID 68) in 0 ms on localhost (executor driver) (52/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 52-53
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 69 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@9debd
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 69 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@9debd
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 52.0 in stage 17.0 (TID 69). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 53.0 in stage 17.0 (TID 70, localhost, executor driver, partition 53, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 53.0 in stage 17.0 (TID 70)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 52.0 in stage 17.0 (TID 69) in 16 ms on localhost (executor driver) (53/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 53-54
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 70 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4e3f541c
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 70 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4e3f541c
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 53.0 in stage 17.0 (TID 70). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 54.0 in stage 17.0 (TID 71, localhost, executor driver, partition 54, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 54.0 in stage 17.0 (TID 71)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 53.0 in stage 17.0 (TID 70) in 0 ms on localhost (executor driver) (54/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 54-55
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 71 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@22a92cca
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 71 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@22a92cca
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 54.0 in stage 17.0 (TID 71). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 55.0 in stage 17.0 (TID 72, localhost, executor driver, partition 55, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 55.0 in stage 17.0 (TID 72)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 54.0 in stage 17.0 (TID 71) in 16 ms on localhost (executor driver) (55/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 55-56
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 72 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@37f477d2
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 72 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@37f477d2
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 55.0 in stage 17.0 (TID 72). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 56.0 in stage 17.0 (TID 73, localhost, executor driver, partition 56, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 56.0 in stage 17.0 (TID 73)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 55.0 in stage 17.0 (TID 72) in 0 ms on localhost (executor driver) (56/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 56-57
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 73 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5ddd131d
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 73 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5ddd131d
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 56.0 in stage 17.0 (TID 73). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 57.0 in stage 17.0 (TID 74, localhost, executor driver, partition 57, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 57.0 in stage 17.0 (TID 74)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 56.0 in stage 17.0 (TID 73) in 0 ms on localhost (executor driver) (57/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 57-58
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 74 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@9da13ac
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 74 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@9da13ac
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 57.0 in stage 17.0 (TID 74). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 58.0 in stage 17.0 (TID 75, localhost, executor driver, partition 58, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 58.0 in stage 17.0 (TID 75)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 57.0 in stage 17.0 (TID 74) in 16 ms on localhost (executor driver) (58/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 58-59
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 75 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@72303c11
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 75 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@72303c11
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 58.0 in stage 17.0 (TID 75). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 59.0 in stage 17.0 (TID 76, localhost, executor driver, partition 59, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 59.0 in stage 17.0 (TID 76)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 58.0 in stage 17.0 (TID 75) in 0 ms on localhost (executor driver) (59/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 59-60
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 76 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@68a0da0
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 76 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@68a0da0
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 59.0 in stage 17.0 (TID 76). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 60.0 in stage 17.0 (TID 77, localhost, executor driver, partition 60, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 60.0 in stage 17.0 (TID 77)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 59.0 in stage 17.0 (TID 76) in 16 ms on localhost (executor driver) (60/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 60-61
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 77 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@b00a4c5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 77 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@b00a4c5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 60.0 in stage 17.0 (TID 77). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 61.0 in stage 17.0 (TID 78, localhost, executor driver, partition 61, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 61.0 in stage 17.0 (TID 78)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 60.0 in stage 17.0 (TID 77) in 0 ms on localhost (executor driver) (61/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 61-62
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 78 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3188afda
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 78 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3188afda
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 61.0 in stage 17.0 (TID 78). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 62.0 in stage 17.0 (TID 79, localhost, executor driver, partition 62, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 62.0 in stage 17.0 (TID 79)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 61.0 in stage 17.0 (TID 78) in 0 ms on localhost (executor driver) (62/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 62-63
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 79 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5ac62c07
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 79 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5ac62c07
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 62.0 in stage 17.0 (TID 79). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 63.0 in stage 17.0 (TID 80, localhost, executor driver, partition 63, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 63.0 in stage 17.0 (TID 80)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 62.0 in stage 17.0 (TID 79) in 16 ms on localhost (executor driver) (63/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 63-64
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 80 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@14fcba97
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 80 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@14fcba97
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 63.0 in stage 17.0 (TID 80). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 64.0 in stage 17.0 (TID 81, localhost, executor driver, partition 64, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 64.0 in stage 17.0 (TID 81)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 63.0 in stage 17.0 (TID 80) in 0 ms on localhost (executor driver) (64/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 64-65
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 81 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2d33a72a
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 81 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2d33a72a
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 64.0 in stage 17.0 (TID 81). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 65.0 in stage 17.0 (TID 82, localhost, executor driver, partition 65, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 65.0 in stage 17.0 (TID 82)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 64.0 in stage 17.0 (TID 81) in 16 ms on localhost (executor driver) (65/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 65-66
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 82 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@115337bd
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 82 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@115337bd
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 65.0 in stage 17.0 (TID 82). 4453 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 66.0 in stage 17.0 (TID 83, localhost, executor driver, partition 66, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 66.0 in stage 17.0 (TID 83)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 65.0 in stage 17.0 (TID 82) in 3 ms on localhost (executor driver) (66/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 66-67
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 83 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@772b831a
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 83 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@772b831a
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 66.0 in stage 17.0 (TID 83). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 67.0 in stage 17.0 (TID 84, localhost, executor driver, partition 67, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 67.0 in stage 17.0 (TID 84)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 66.0 in stage 17.0 (TID 83) in 0 ms on localhost (executor driver) (67/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 67-68
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 84 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@75dbacf5
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 84 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@75dbacf5
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 67.0 in stage 17.0 (TID 84). 4496 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 68.0 in stage 17.0 (TID 85, localhost, executor driver, partition 68, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 68.0 in stage 17.0 (TID 85)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 67.0 in stage 17.0 (TID 84) in 16 ms on localhost (executor driver) (68/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 68-69
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 85 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@35a08a3
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 85 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@35a08a3
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 68.0 in stage 17.0 (TID 85). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 69.0 in stage 17.0 (TID 86, localhost, executor driver, partition 69, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 69.0 in stage 17.0 (TID 86)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 68.0 in stage 17.0 (TID 85) in 0 ms on localhost (executor driver) (69/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 69-70
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 86 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@33b1a93
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 86 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@33b1a93
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 69.0 in stage 17.0 (TID 86). 4410 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 70.0 in stage 17.0 (TID 87, localhost, executor driver, partition 70, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 70.0 in stage 17.0 (TID 87)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 69.0 in stage 17.0 (TID 86) in 16 ms on localhost (executor driver) (70/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 70-71
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 87 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@33d7c9b2
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 87 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@33d7c9b2
2022-02-10 15:13:41 INFO  Executor:54 - Finished task 70.0 in stage 17.0 (TID 87). 4453 bytes result sent to driver
2022-02-10 15:13:41 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Starting task 71.0 in stage 17.0 (TID 88, localhost, executor driver, partition 71, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:41 INFO  Executor:54 - Running task 71.0 in stage 17.0 (TID 88)
2022-02-10 15:13:41 INFO  TaskSetManager:54 - Finished task 70.0 in stage 17.0 (TID 87) in 2 ms on localhost (executor driver) (71/200)
2022-02-10 15:13:41 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 71-72
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:41 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:41 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:224 - Task 88 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5d482555
2022-02-10 15:13:41 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:41 DEBUG TaskMemoryManager:233 - Task 88 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5d482555
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 71.0 in stage 17.0 (TID 88). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 72.0 in stage 17.0 (TID 89, localhost, executor driver, partition 72, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 72.0 in stage 17.0 (TID 89)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 71.0 in stage 17.0 (TID 88) in 16 ms on localhost (executor driver) (72/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 72-73
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 89 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@74011e2
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 89 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@74011e2
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 72.0 in stage 17.0 (TID 89). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 73.0 in stage 17.0 (TID 90, localhost, executor driver, partition 73, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 73.0 in stage 17.0 (TID 90)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 72.0 in stage 17.0 (TID 89) in 0 ms on localhost (executor driver) (73/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 73-74
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 90 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@61aa86a
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 90 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@61aa86a
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 73.0 in stage 17.0 (TID 90). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 74.0 in stage 17.0 (TID 91, localhost, executor driver, partition 74, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 74.0 in stage 17.0 (TID 91)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 73.0 in stage 17.0 (TID 90) in 16 ms on localhost (executor driver) (74/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 74-75
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 91 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4d92285b
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 91 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4d92285b
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 74.0 in stage 17.0 (TID 91). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 75.0 in stage 17.0 (TID 92, localhost, executor driver, partition 75, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 75.0 in stage 17.0 (TID 92)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 74.0 in stage 17.0 (TID 91) in 16 ms on localhost (executor driver) (75/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 75-76
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 92 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5312b88
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 92 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5312b88
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 75.0 in stage 17.0 (TID 92). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 76.0 in stage 17.0 (TID 93, localhost, executor driver, partition 76, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 76.0 in stage 17.0 (TID 93)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 75.0 in stage 17.0 (TID 92) in 16 ms on localhost (executor driver) (76/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 76-77
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 93 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@71f5a2dc
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 93 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@71f5a2dc
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 76.0 in stage 17.0 (TID 93). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 77.0 in stage 17.0 (TID 94, localhost, executor driver, partition 77, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 77.0 in stage 17.0 (TID 94)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 76.0 in stage 17.0 (TID 93) in 0 ms on localhost (executor driver) (77/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 77-78
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 94 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7a172e91
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 94 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7a172e91
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 77.0 in stage 17.0 (TID 94). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 78.0 in stage 17.0 (TID 95, localhost, executor driver, partition 78, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 78.0 in stage 17.0 (TID 95)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 77.0 in stage 17.0 (TID 94) in 0 ms on localhost (executor driver) (78/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 78-79
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 95 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@557a5731
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 95 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@557a5731
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 78.0 in stage 17.0 (TID 95). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 79.0 in stage 17.0 (TID 96, localhost, executor driver, partition 79, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 79.0 in stage 17.0 (TID 96)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 78.0 in stage 17.0 (TID 95) in 16 ms on localhost (executor driver) (79/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 79-80
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 96 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@8ae737c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 96 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@8ae737c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 79.0 in stage 17.0 (TID 96). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 80.0 in stage 17.0 (TID 97, localhost, executor driver, partition 80, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 80.0 in stage 17.0 (TID 97)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 79.0 in stage 17.0 (TID 96) in 0 ms on localhost (executor driver) (80/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 80-81
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 97 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6d5b6bbb
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 97 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6d5b6bbb
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 80.0 in stage 17.0 (TID 97). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 81.0 in stage 17.0 (TID 98, localhost, executor driver, partition 81, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 81.0 in stage 17.0 (TID 98)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 80.0 in stage 17.0 (TID 97) in 0 ms on localhost (executor driver) (81/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 81-82
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 98 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6de43197
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 98 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6de43197
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 81.0 in stage 17.0 (TID 98). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 82.0 in stage 17.0 (TID 99, localhost, executor driver, partition 82, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 82.0 in stage 17.0 (TID 99)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 81.0 in stage 17.0 (TID 98) in 16 ms on localhost (executor driver) (82/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 82-83
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 99 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6d10fa74
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 99 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6d10fa74
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 82.0 in stage 17.0 (TID 99). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 83.0 in stage 17.0 (TID 100, localhost, executor driver, partition 83, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 83.0 in stage 17.0 (TID 100)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 82.0 in stage 17.0 (TID 99) in 0 ms on localhost (executor driver) (83/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 83-84
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 100 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7526b7bf
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 100 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7526b7bf
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 83.0 in stage 17.0 (TID 100). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 84.0 in stage 17.0 (TID 101, localhost, executor driver, partition 84, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 84.0 in stage 17.0 (TID 101)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 83.0 in stage 17.0 (TID 100) in 0 ms on localhost (executor driver) (84/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 84-85
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 101 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6774bdce
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 101 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6774bdce
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 84.0 in stage 17.0 (TID 101). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 85.0 in stage 17.0 (TID 102, localhost, executor driver, partition 85, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 85.0 in stage 17.0 (TID 102)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 84.0 in stage 17.0 (TID 101) in 16 ms on localhost (executor driver) (85/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 85-86
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 102 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@19c04679
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 102 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@19c04679
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 85.0 in stage 17.0 (TID 102). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 86.0 in stage 17.0 (TID 103, localhost, executor driver, partition 86, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 86.0 in stage 17.0 (TID 103)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 85.0 in stage 17.0 (TID 102) in 0 ms on localhost (executor driver) (86/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 86-87
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 103 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4f0cd632
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 103 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4f0cd632
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 86.0 in stage 17.0 (TID 103). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 87.0 in stage 17.0 (TID 104, localhost, executor driver, partition 87, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 87.0 in stage 17.0 (TID 104)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 86.0 in stage 17.0 (TID 103) in 0 ms on localhost (executor driver) (87/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 87-88
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 104 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4dcc241b
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 104 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4dcc241b
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 87.0 in stage 17.0 (TID 104). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 88.0 in stage 17.0 (TID 105, localhost, executor driver, partition 88, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 88.0 in stage 17.0 (TID 105)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 87.0 in stage 17.0 (TID 104) in 16 ms on localhost (executor driver) (88/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 88-89
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 105 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@784eeebc
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 105 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@784eeebc
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 88.0 in stage 17.0 (TID 105). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 89.0 in stage 17.0 (TID 106, localhost, executor driver, partition 89, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 89.0 in stage 17.0 (TID 106)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 88.0 in stage 17.0 (TID 105) in 0 ms on localhost (executor driver) (89/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 89-90
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 106 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@32d1b240
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 106 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@32d1b240
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 89.0 in stage 17.0 (TID 106). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 90.0 in stage 17.0 (TID 107, localhost, executor driver, partition 90, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 90.0 in stage 17.0 (TID 107)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 89.0 in stage 17.0 (TID 106) in 16 ms on localhost (executor driver) (90/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 90-91
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 107 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7cfda9a7
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 107 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7cfda9a7
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 90.0 in stage 17.0 (TID 107). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 91.0 in stage 17.0 (TID 108, localhost, executor driver, partition 91, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 91.0 in stage 17.0 (TID 108)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 90.0 in stage 17.0 (TID 107) in 0 ms on localhost (executor driver) (91/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 91-92
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 108 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1a330206
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 108 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1a330206
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 91.0 in stage 17.0 (TID 108). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 92.0 in stage 17.0 (TID 109, localhost, executor driver, partition 92, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 92.0 in stage 17.0 (TID 109)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 91.0 in stage 17.0 (TID 108) in 0 ms on localhost (executor driver) (92/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 92-93
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 109 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6f41335f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 109 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6f41335f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 92.0 in stage 17.0 (TID 109). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 93.0 in stage 17.0 (TID 110, localhost, executor driver, partition 93, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 93.0 in stage 17.0 (TID 110)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 92.0 in stage 17.0 (TID 109) in 16 ms on localhost (executor driver) (93/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 93-94
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 110 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@21bf1e18
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 110 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@21bf1e18
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 93.0 in stage 17.0 (TID 110). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 94.0 in stage 17.0 (TID 111, localhost, executor driver, partition 94, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 94.0 in stage 17.0 (TID 111)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 93.0 in stage 17.0 (TID 110) in 1 ms on localhost (executor driver) (94/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 94-95
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 111 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1ef29b9e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 111 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1ef29b9e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 94.0 in stage 17.0 (TID 111). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 95.0 in stage 17.0 (TID 112, localhost, executor driver, partition 95, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 95.0 in stage 17.0 (TID 112)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 94.0 in stage 17.0 (TID 111) in 0 ms on localhost (executor driver) (95/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 95-96
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 112 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@540b8e58
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 112 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@540b8e58
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 95.0 in stage 17.0 (TID 112). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 96.0 in stage 17.0 (TID 113, localhost, executor driver, partition 96, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 96.0 in stage 17.0 (TID 113)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 95.0 in stage 17.0 (TID 112) in 15 ms on localhost (executor driver) (96/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 96-97
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 113 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5f63ddd
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 113 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5f63ddd
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 96.0 in stage 17.0 (TID 113). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 98.0 in stage 17.0 (TID 114, localhost, executor driver, partition 98, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 98.0 in stage 17.0 (TID 114)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 96.0 in stage 17.0 (TID 113) in 0 ms on localhost (executor driver) (97/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 98-99
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 114 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3833e1ac
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 114 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3833e1ac
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 98.0 in stage 17.0 (TID 114). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 99.0 in stage 17.0 (TID 115, localhost, executor driver, partition 99, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 99.0 in stage 17.0 (TID 115)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 98.0 in stage 17.0 (TID 114) in 0 ms on localhost (executor driver) (98/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 99-100
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 115 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@123a92fc
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 115 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@123a92fc
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 99.0 in stage 17.0 (TID 115). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 100.0 in stage 17.0 (TID 116, localhost, executor driver, partition 100, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 100.0 in stage 17.0 (TID 116)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 99.0 in stage 17.0 (TID 115) in 16 ms on localhost (executor driver) (99/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 100-101
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 116 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3e96028e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 116 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3e96028e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 100.0 in stage 17.0 (TID 116). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 101.0 in stage 17.0 (TID 117, localhost, executor driver, partition 101, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 101.0 in stage 17.0 (TID 117)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 100.0 in stage 17.0 (TID 116) in 0 ms on localhost (executor driver) (100/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 101-102
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 117 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5e82e83d
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 117 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5e82e83d
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 101.0 in stage 17.0 (TID 117). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 102.0 in stage 17.0 (TID 118, localhost, executor driver, partition 102, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 102.0 in stage 17.0 (TID 118)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 101.0 in stage 17.0 (TID 117) in 16 ms on localhost (executor driver) (101/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 102-103
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 118 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1cc53bcf
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 118 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1cc53bcf
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 102.0 in stage 17.0 (TID 118). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 103.0 in stage 17.0 (TID 119, localhost, executor driver, partition 103, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 103.0 in stage 17.0 (TID 119)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 102.0 in stage 17.0 (TID 118) in 0 ms on localhost (executor driver) (102/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 103-104
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 119 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@432d6b7
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 119 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@432d6b7
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 103.0 in stage 17.0 (TID 119). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 104.0 in stage 17.0 (TID 120, localhost, executor driver, partition 104, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 103.0 in stage 17.0 (TID 119) in 0 ms on localhost (executor driver) (103/200)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 104.0 in stage 17.0 (TID 120)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 104-105
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 120 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@14a7f35
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 120 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@14a7f35
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 104.0 in stage 17.0 (TID 120). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 105.0 in stage 17.0 (TID 121, localhost, executor driver, partition 105, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 105.0 in stage 17.0 (TID 121)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 104.0 in stage 17.0 (TID 120) in 16 ms on localhost (executor driver) (104/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 105-106
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 121 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@711ae033
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 121 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@711ae033
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 105.0 in stage 17.0 (TID 121). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 106.0 in stage 17.0 (TID 122, localhost, executor driver, partition 106, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 106.0 in stage 17.0 (TID 122)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 105.0 in stage 17.0 (TID 121) in 0 ms on localhost (executor driver) (105/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 106-107
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 122 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@276aeea1
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 122 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@276aeea1
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 106.0 in stage 17.0 (TID 122). 4539 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 107.0 in stage 17.0 (TID 123, localhost, executor driver, partition 107, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 107.0 in stage 17.0 (TID 123)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 106.0 in stage 17.0 (TID 122) in 16 ms on localhost (executor driver) (106/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 107-108
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 123 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@77327193
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 123 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@77327193
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 107.0 in stage 17.0 (TID 123). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 109.0 in stage 17.0 (TID 124, localhost, executor driver, partition 109, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 109.0 in stage 17.0 (TID 124)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 107.0 in stage 17.0 (TID 123) in 0 ms on localhost (executor driver) (107/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 109-110
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 124 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@40ba92ef
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 124 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@40ba92ef
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 109.0 in stage 17.0 (TID 124). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 110.0 in stage 17.0 (TID 125, localhost, executor driver, partition 110, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 110.0 in stage 17.0 (TID 125)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 109.0 in stage 17.0 (TID 124) in 21 ms on localhost (executor driver) (108/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 110-111
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 125 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3c65026c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 125 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3c65026c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 110.0 in stage 17.0 (TID 125). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 111.0 in stage 17.0 (TID 126, localhost, executor driver, partition 111, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 111.0 in stage 17.0 (TID 126)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 110.0 in stage 17.0 (TID 125) in 1 ms on localhost (executor driver) (109/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 111-112
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 126 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@39322030
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 126 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@39322030
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 111.0 in stage 17.0 (TID 126). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 112.0 in stage 17.0 (TID 127, localhost, executor driver, partition 112, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 112.0 in stage 17.0 (TID 127)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 111.0 in stage 17.0 (TID 126) in 7 ms on localhost (executor driver) (110/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 112-113
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 127 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@46ad215c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 127 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@46ad215c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 112.0 in stage 17.0 (TID 127). 4539 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 113.0 in stage 17.0 (TID 128, localhost, executor driver, partition 113, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 113.0 in stage 17.0 (TID 128)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 112.0 in stage 17.0 (TID 127) in 10 ms on localhost (executor driver) (111/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 113-114
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 128 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1b50252b
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 128 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1b50252b
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 113.0 in stage 17.0 (TID 128). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 114.0 in stage 17.0 (TID 129, localhost, executor driver, partition 114, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 114.0 in stage 17.0 (TID 129)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 113.0 in stage 17.0 (TID 128) in 3 ms on localhost (executor driver) (112/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 114-115
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 129 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@341fc807
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 129 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@341fc807
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 114.0 in stage 17.0 (TID 129). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 115.0 in stage 17.0 (TID 130, localhost, executor driver, partition 115, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 115.0 in stage 17.0 (TID 130)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 114.0 in stage 17.0 (TID 129) in 0 ms on localhost (executor driver) (113/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 115-116
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 130 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3269cdc0
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 130 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3269cdc0
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 115.0 in stage 17.0 (TID 130). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 117.0 in stage 17.0 (TID 131, localhost, executor driver, partition 117, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 115.0 in stage 17.0 (TID 130) in 17 ms on localhost (executor driver) (114/200)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 117.0 in stage 17.0 (TID 131)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 117-118
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 131 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2c112a84
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 131 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2c112a84
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 117.0 in stage 17.0 (TID 131). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 118.0 in stage 17.0 (TID 132, localhost, executor driver, partition 118, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 118.0 in stage 17.0 (TID 132)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 117.0 in stage 17.0 (TID 131) in 2 ms on localhost (executor driver) (115/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 118-119
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 132 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@28b4d1ed
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 132 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@28b4d1ed
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 118.0 in stage 17.0 (TID 132). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 119.0 in stage 17.0 (TID 133, localhost, executor driver, partition 119, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 119.0 in stage 17.0 (TID 133)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 118.0 in stage 17.0 (TID 132) in 6 ms on localhost (executor driver) (116/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 119-120
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 133 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3d91fdec
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 133 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3d91fdec
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 119.0 in stage 17.0 (TID 133). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 120.0 in stage 17.0 (TID 134, localhost, executor driver, partition 120, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 120.0 in stage 17.0 (TID 134)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 119.0 in stage 17.0 (TID 133) in 0 ms on localhost (executor driver) (117/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 120-121
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 134 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@15ac17f7
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 134 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@15ac17f7
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 120.0 in stage 17.0 (TID 134). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 121.0 in stage 17.0 (TID 135, localhost, executor driver, partition 121, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 121.0 in stage 17.0 (TID 135)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 120.0 in stage 17.0 (TID 134) in 0 ms on localhost (executor driver) (118/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 121-122
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 135 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4358019a
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 135 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4358019a
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 121.0 in stage 17.0 (TID 135). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 122.0 in stage 17.0 (TID 136, localhost, executor driver, partition 122, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 122.0 in stage 17.0 (TID 136)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 121.0 in stage 17.0 (TID 135) in 16 ms on localhost (executor driver) (119/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 122-123
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 136 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5a8b7e84
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 136 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5a8b7e84
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 122.0 in stage 17.0 (TID 136). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 123.0 in stage 17.0 (TID 137, localhost, executor driver, partition 123, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 123.0 in stage 17.0 (TID 137)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 122.0 in stage 17.0 (TID 136) in 3 ms on localhost (executor driver) (120/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 123-124
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 137 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@28416ff0
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 137 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@28416ff0
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 123.0 in stage 17.0 (TID 137). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 124.0 in stage 17.0 (TID 138, localhost, executor driver, partition 124, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 124.0 in stage 17.0 (TID 138)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 123.0 in stage 17.0 (TID 137) in 0 ms on localhost (executor driver) (121/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 124-125
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 138 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5c2dd2a2
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 138 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5c2dd2a2
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 124.0 in stage 17.0 (TID 138). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 125.0 in stage 17.0 (TID 139, localhost, executor driver, partition 125, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 125.0 in stage 17.0 (TID 139)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 124.0 in stage 17.0 (TID 138) in 0 ms on localhost (executor driver) (122/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 125-126
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 139 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4c0d22d2
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 139 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4c0d22d2
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 125.0 in stage 17.0 (TID 139). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 126.0 in stage 17.0 (TID 140, localhost, executor driver, partition 126, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 126.0 in stage 17.0 (TID 140)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 125.0 in stage 17.0 (TID 139) in 16 ms on localhost (executor driver) (123/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 126-127
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 140 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5506f827
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 140 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5506f827
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 126.0 in stage 17.0 (TID 140). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 127.0 in stage 17.0 (TID 141, localhost, executor driver, partition 127, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 127.0 in stage 17.0 (TID 141)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 126.0 in stage 17.0 (TID 140) in 0 ms on localhost (executor driver) (124/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 127-128
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 141 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@790b89e9
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 141 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@790b89e9
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 127.0 in stage 17.0 (TID 141). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 128.0 in stage 17.0 (TID 142, localhost, executor driver, partition 128, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 128.0 in stage 17.0 (TID 142)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 127.0 in stage 17.0 (TID 141) in 0 ms on localhost (executor driver) (125/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 128-129
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 142 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@79bed76c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 142 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@79bed76c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 128.0 in stage 17.0 (TID 142). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 129.0 in stage 17.0 (TID 143, localhost, executor driver, partition 129, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 129.0 in stage 17.0 (TID 143)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 128.0 in stage 17.0 (TID 142) in 16 ms on localhost (executor driver) (126/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 129-130
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 143 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2860e0d6
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 143 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2860e0d6
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 129.0 in stage 17.0 (TID 143). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 130.0 in stage 17.0 (TID 144, localhost, executor driver, partition 130, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 130.0 in stage 17.0 (TID 144)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 129.0 in stage 17.0 (TID 143) in 0 ms on localhost (executor driver) (127/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 130-131
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 144 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@11131995
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 144 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@11131995
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 130.0 in stage 17.0 (TID 144). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 131.0 in stage 17.0 (TID 145, localhost, executor driver, partition 131, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 131.0 in stage 17.0 (TID 145)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 130.0 in stage 17.0 (TID 144) in 0 ms on localhost (executor driver) (128/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 131-132
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 145 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@74db386f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 145 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@74db386f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 131.0 in stage 17.0 (TID 145). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 132.0 in stage 17.0 (TID 146, localhost, executor driver, partition 132, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 132.0 in stage 17.0 (TID 146)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 131.0 in stage 17.0 (TID 145) in 16 ms on localhost (executor driver) (129/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 132-133
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 146 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2a8c1ec5
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 146 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2a8c1ec5
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 132.0 in stage 17.0 (TID 146). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 133.0 in stage 17.0 (TID 147, localhost, executor driver, partition 133, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 133.0 in stage 17.0 (TID 147)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 132.0 in stage 17.0 (TID 146) in 0 ms on localhost (executor driver) (130/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 133-134
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 147 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54fde8e0
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 147 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54fde8e0
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 133.0 in stage 17.0 (TID 147). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 135.0 in stage 17.0 (TID 148, localhost, executor driver, partition 135, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 135.0 in stage 17.0 (TID 148)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 133.0 in stage 17.0 (TID 147) in 0 ms on localhost (executor driver) (131/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 135-136
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 148 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@24b21771
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 148 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@24b21771
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 135.0 in stage 17.0 (TID 148). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 136.0 in stage 17.0 (TID 149, localhost, executor driver, partition 136, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 136.0 in stage 17.0 (TID 149)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 135.0 in stage 17.0 (TID 148) in 16 ms on localhost (executor driver) (132/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 136-137
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 149 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@718d2599
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 149 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@718d2599
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 136.0 in stage 17.0 (TID 149). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 137.0 in stage 17.0 (TID 150, localhost, executor driver, partition 137, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 137.0 in stage 17.0 (TID 150)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 136.0 in stage 17.0 (TID 149) in 0 ms on localhost (executor driver) (133/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 137-138
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 150 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@70e64b92
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 150 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@70e64b92
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 137.0 in stage 17.0 (TID 150). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 138.0 in stage 17.0 (TID 151, localhost, executor driver, partition 138, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 138.0 in stage 17.0 (TID 151)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 137.0 in stage 17.0 (TID 150) in 0 ms on localhost (executor driver) (134/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 138-139
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 151 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@24ae0ae6
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 151 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@24ae0ae6
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 138.0 in stage 17.0 (TID 151). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 139.0 in stage 17.0 (TID 152, localhost, executor driver, partition 139, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 139.0 in stage 17.0 (TID 152)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 138.0 in stage 17.0 (TID 151) in 15 ms on localhost (executor driver) (135/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 139-140
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 152 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3d15e87e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 152 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3d15e87e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 139.0 in stage 17.0 (TID 152). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 140.0 in stage 17.0 (TID 153, localhost, executor driver, partition 140, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 140.0 in stage 17.0 (TID 153)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 139.0 in stage 17.0 (TID 152) in 0 ms on localhost (executor driver) (136/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 140-141
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 153 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6fd46ac4
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 153 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6fd46ac4
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 140.0 in stage 17.0 (TID 153). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 141.0 in stage 17.0 (TID 154, localhost, executor driver, partition 141, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 141.0 in stage 17.0 (TID 154)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 140.0 in stage 17.0 (TID 153) in 0 ms on localhost (executor driver) (137/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 141-142
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 154 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@47b0b326
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 154 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@47b0b326
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 141.0 in stage 17.0 (TID 154). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 142.0 in stage 17.0 (TID 155, localhost, executor driver, partition 142, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 142.0 in stage 17.0 (TID 155)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 141.0 in stage 17.0 (TID 154) in 17 ms on localhost (executor driver) (138/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 142-143
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 155 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@293e2834
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 155 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@293e2834
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 142.0 in stage 17.0 (TID 155). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 143.0 in stage 17.0 (TID 156, localhost, executor driver, partition 143, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 143.0 in stage 17.0 (TID 156)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 142.0 in stage 17.0 (TID 155) in 0 ms on localhost (executor driver) (139/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 143-144
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 156 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@46f9fa9b
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 156 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@46f9fa9b
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 143.0 in stage 17.0 (TID 156). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 144.0 in stage 17.0 (TID 157, localhost, executor driver, partition 144, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 144.0 in stage 17.0 (TID 157)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 143.0 in stage 17.0 (TID 156) in 0 ms on localhost (executor driver) (140/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 144-145
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 157 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@15c68a23
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 157 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@15c68a23
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 144.0 in stage 17.0 (TID 157). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 145.0 in stage 17.0 (TID 158, localhost, executor driver, partition 145, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 145.0 in stage 17.0 (TID 158)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 144.0 in stage 17.0 (TID 157) in 16 ms on localhost (executor driver) (141/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 145-146
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 158 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3b9b3cd9
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 158 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3b9b3cd9
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 145.0 in stage 17.0 (TID 158). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 146.0 in stage 17.0 (TID 159, localhost, executor driver, partition 146, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 146.0 in stage 17.0 (TID 159)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 145.0 in stage 17.0 (TID 158) in 0 ms on localhost (executor driver) (142/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 146-147
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 159 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@41f90dd2
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 159 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@41f90dd2
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 146.0 in stage 17.0 (TID 159). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 147.0 in stage 17.0 (TID 160, localhost, executor driver, partition 147, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 147.0 in stage 17.0 (TID 160)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 146.0 in stage 17.0 (TID 159) in 0 ms on localhost (executor driver) (143/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 147-148
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 160 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@249a2488
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 160 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@249a2488
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 147.0 in stage 17.0 (TID 160). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 148.0 in stage 17.0 (TID 161, localhost, executor driver, partition 148, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 148.0 in stage 17.0 (TID 161)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 147.0 in stage 17.0 (TID 160) in 16 ms on localhost (executor driver) (144/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 148-149
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 161 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@76d7b64e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 161 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@76d7b64e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 148.0 in stage 17.0 (TID 161). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 149.0 in stage 17.0 (TID 162, localhost, executor driver, partition 149, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 149.0 in stage 17.0 (TID 162)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 148.0 in stage 17.0 (TID 161) in 1 ms on localhost (executor driver) (145/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 149-150
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 162 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@725751d5
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 162 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@725751d5
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 149.0 in stage 17.0 (TID 162). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 150.0 in stage 17.0 (TID 163, localhost, executor driver, partition 150, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 150.0 in stage 17.0 (TID 163)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 149.0 in stage 17.0 (TID 162) in 0 ms on localhost (executor driver) (146/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 150-151
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 163 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1beb5c48
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 163 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1beb5c48
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 150.0 in stage 17.0 (TID 163). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 151.0 in stage 17.0 (TID 164, localhost, executor driver, partition 151, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 151.0 in stage 17.0 (TID 164)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 150.0 in stage 17.0 (TID 163) in 0 ms on localhost (executor driver) (147/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 151-152
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 164 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@509c08f7
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 164 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@509c08f7
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 151.0 in stage 17.0 (TID 164). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 152.0 in stage 17.0 (TID 165, localhost, executor driver, partition 152, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 152.0 in stage 17.0 (TID 165)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 151.0 in stage 17.0 (TID 164) in 16 ms on localhost (executor driver) (148/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 152-153
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 165 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@326d32cd
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 165 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@326d32cd
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 152.0 in stage 17.0 (TID 165). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 153.0 in stage 17.0 (TID 166, localhost, executor driver, partition 153, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 153.0 in stage 17.0 (TID 166)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 152.0 in stage 17.0 (TID 165) in 0 ms on localhost (executor driver) (149/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 153-154
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 166 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@66085926
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 166 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@66085926
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 153.0 in stage 17.0 (TID 166). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 154.0 in stage 17.0 (TID 167, localhost, executor driver, partition 154, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 154.0 in stage 17.0 (TID 167)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 153.0 in stage 17.0 (TID 166) in 15 ms on localhost (executor driver) (150/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 154-155
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 167 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@e2b3ae4
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 167 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@e2b3ae4
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 154.0 in stage 17.0 (TID 167). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 155.0 in stage 17.0 (TID 168, localhost, executor driver, partition 155, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 155.0 in stage 17.0 (TID 168)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 154.0 in stage 17.0 (TID 167) in 0 ms on localhost (executor driver) (151/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 155-156
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 168 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5d0b7b27
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 168 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5d0b7b27
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 155.0 in stage 17.0 (TID 168). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 157.0 in stage 17.0 (TID 169, localhost, executor driver, partition 157, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 157.0 in stage 17.0 (TID 169)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 155.0 in stage 17.0 (TID 168) in 0 ms on localhost (executor driver) (152/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 157-158
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 169 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5e296f37
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 169 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5e296f37
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 157.0 in stage 17.0 (TID 169). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 158.0 in stage 17.0 (TID 170, localhost, executor driver, partition 158, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 158.0 in stage 17.0 (TID 170)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 157.0 in stage 17.0 (TID 169) in 16 ms on localhost (executor driver) (153/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 158-159
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 170 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7c43b875
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 170 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7c43b875
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 158.0 in stage 17.0 (TID 170). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 159.0 in stage 17.0 (TID 171, localhost, executor driver, partition 159, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 159.0 in stage 17.0 (TID 171)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 158.0 in stage 17.0 (TID 170) in 16 ms on localhost (executor driver) (154/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 159-160
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 171 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@28c1950c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 171 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@28c1950c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 159.0 in stage 17.0 (TID 171). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 160.0 in stage 17.0 (TID 172, localhost, executor driver, partition 160, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 160.0 in stage 17.0 (TID 172)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 159.0 in stage 17.0 (TID 171) in 0 ms on localhost (executor driver) (155/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 160-161
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 172 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2759101e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 172 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2759101e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 160.0 in stage 17.0 (TID 172). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 161.0 in stage 17.0 (TID 173, localhost, executor driver, partition 161, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 161.0 in stage 17.0 (TID 173)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 160.0 in stage 17.0 (TID 172) in 0 ms on localhost (executor driver) (156/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 161-162
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 173 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@29f3c94d
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 173 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@29f3c94d
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 161.0 in stage 17.0 (TID 173). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 162.0 in stage 17.0 (TID 174, localhost, executor driver, partition 162, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 162.0 in stage 17.0 (TID 174)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 161.0 in stage 17.0 (TID 173) in 16 ms on localhost (executor driver) (157/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 162-163
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 174 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@492cffba
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 174 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@492cffba
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 162.0 in stage 17.0 (TID 174). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 163.0 in stage 17.0 (TID 175, localhost, executor driver, partition 163, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 163.0 in stage 17.0 (TID 175)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 162.0 in stage 17.0 (TID 174) in 0 ms on localhost (executor driver) (158/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 163-164
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 175 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1e45493f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 175 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1e45493f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 163.0 in stage 17.0 (TID 175). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 164.0 in stage 17.0 (TID 176, localhost, executor driver, partition 164, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 164.0 in stage 17.0 (TID 176)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 163.0 in stage 17.0 (TID 175) in 0 ms on localhost (executor driver) (159/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 164-165
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 176 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1b2fb86c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 176 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1b2fb86c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 164.0 in stage 17.0 (TID 176). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 165.0 in stage 17.0 (TID 177, localhost, executor driver, partition 165, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 165.0 in stage 17.0 (TID 177)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 164.0 in stage 17.0 (TID 176) in 0 ms on localhost (executor driver) (160/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 165-166
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 177 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@12e69302
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 177 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@12e69302
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 165.0 in stage 17.0 (TID 177). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 166.0 in stage 17.0 (TID 178, localhost, executor driver, partition 166, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 166.0 in stage 17.0 (TID 178)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 165.0 in stage 17.0 (TID 177) in 16 ms on localhost (executor driver) (161/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 166-167
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 178 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@397a5f2f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 178 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@397a5f2f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 166.0 in stage 17.0 (TID 178). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 167.0 in stage 17.0 (TID 179, localhost, executor driver, partition 167, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 167.0 in stage 17.0 (TID 179)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 166.0 in stage 17.0 (TID 178) in 0 ms on localhost (executor driver) (162/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 167-168
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 179 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1b3c03d2
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 179 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1b3c03d2
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 167.0 in stage 17.0 (TID 179). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 168.0 in stage 17.0 (TID 180, localhost, executor driver, partition 168, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 168.0 in stage 17.0 (TID 180)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 167.0 in stage 17.0 (TID 179) in 0 ms on localhost (executor driver) (163/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 168-169
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 180 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6844d86c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 180 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6844d86c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 168.0 in stage 17.0 (TID 180). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 169.0 in stage 17.0 (TID 181, localhost, executor driver, partition 169, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 169.0 in stage 17.0 (TID 181)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 168.0 in stage 17.0 (TID 180) in 0 ms on localhost (executor driver) (164/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 169-170
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 181 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@502b0506
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 181 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@502b0506
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 169.0 in stage 17.0 (TID 181). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 170.0 in stage 17.0 (TID 182, localhost, executor driver, partition 170, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 170.0 in stage 17.0 (TID 182)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 169.0 in stage 17.0 (TID 181) in 16 ms on localhost (executor driver) (165/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 170-171
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 182 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@19561067
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 182 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@19561067
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 170.0 in stage 17.0 (TID 182). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 171.0 in stage 17.0 (TID 183, localhost, executor driver, partition 171, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 171.0 in stage 17.0 (TID 183)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 170.0 in stage 17.0 (TID 182) in 0 ms on localhost (executor driver) (166/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 171-172
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 183 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6009c003
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 183 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6009c003
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 171.0 in stage 17.0 (TID 183). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 172.0 in stage 17.0 (TID 184, localhost, executor driver, partition 172, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 172.0 in stage 17.0 (TID 184)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 171.0 in stage 17.0 (TID 183) in 0 ms on localhost (executor driver) (167/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 172-173
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 184 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@13d56f1f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 184 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@13d56f1f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 172.0 in stage 17.0 (TID 184). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 173.0 in stage 17.0 (TID 185, localhost, executor driver, partition 173, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 173.0 in stage 17.0 (TID 185)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 172.0 in stage 17.0 (TID 184) in 16 ms on localhost (executor driver) (168/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 173-174
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 185 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@21b451ce
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 185 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@21b451ce
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 173.0 in stage 17.0 (TID 185). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 174.0 in stage 17.0 (TID 186, localhost, executor driver, partition 174, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 174.0 in stage 17.0 (TID 186)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 173.0 in stage 17.0 (TID 185) in 0 ms on localhost (executor driver) (169/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 174-175
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 186 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6bb09b07
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 186 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6bb09b07
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 174.0 in stage 17.0 (TID 186). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 175.0 in stage 17.0 (TID 187, localhost, executor driver, partition 175, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 175.0 in stage 17.0 (TID 187)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 174.0 in stage 17.0 (TID 186) in 0 ms on localhost (executor driver) (170/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 175-176
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 187 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@226e519e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 187 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@226e519e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 175.0 in stage 17.0 (TID 187). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 176.0 in stage 17.0 (TID 188, localhost, executor driver, partition 176, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 176.0 in stage 17.0 (TID 188)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 175.0 in stage 17.0 (TID 187) in 16 ms on localhost (executor driver) (171/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 176-177
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 188 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@55fe37d7
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 188 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@55fe37d7
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 176.0 in stage 17.0 (TID 188). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 177.0 in stage 17.0 (TID 189, localhost, executor driver, partition 177, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 177.0 in stage 17.0 (TID 189)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 176.0 in stage 17.0 (TID 188) in 0 ms on localhost (executor driver) (172/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 177-178
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 189 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6297b479
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 189 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6297b479
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 177.0 in stage 17.0 (TID 189). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 178.0 in stage 17.0 (TID 190, localhost, executor driver, partition 178, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 178.0 in stage 17.0 (TID 190)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 177.0 in stage 17.0 (TID 189) in 0 ms on localhost (executor driver) (173/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 178-179
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 190 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@24370a71
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 190 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@24370a71
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 178.0 in stage 17.0 (TID 190). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 179.0 in stage 17.0 (TID 191, localhost, executor driver, partition 179, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 179.0 in stage 17.0 (TID 191)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 178.0 in stage 17.0 (TID 190) in 16 ms on localhost (executor driver) (174/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 179-180
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 191 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@315112b9
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 191 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@315112b9
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 179.0 in stage 17.0 (TID 191). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 180.0 in stage 17.0 (TID 192, localhost, executor driver, partition 180, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 180.0 in stage 17.0 (TID 192)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 179.0 in stage 17.0 (TID 191) in 0 ms on localhost (executor driver) (175/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 180-181
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 192 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@c4325bb
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 192 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@c4325bb
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 180.0 in stage 17.0 (TID 192). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 181.0 in stage 17.0 (TID 193, localhost, executor driver, partition 181, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 181.0 in stage 17.0 (TID 193)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 180.0 in stage 17.0 (TID 192) in 0 ms on localhost (executor driver) (176/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 181-182
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 193 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@751b4d68
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 193 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@751b4d68
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 181.0 in stage 17.0 (TID 193). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 182.0 in stage 17.0 (TID 194, localhost, executor driver, partition 182, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 182.0 in stage 17.0 (TID 194)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 181.0 in stage 17.0 (TID 193) in 16 ms on localhost (executor driver) (177/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 182-183
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 194 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1d2662da
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 194 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1d2662da
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 182.0 in stage 17.0 (TID 194). 4453 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 183.0 in stage 17.0 (TID 195, localhost, executor driver, partition 183, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 183.0 in stage 17.0 (TID 195)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 182.0 in stage 17.0 (TID 194) in 3 ms on localhost (executor driver) (178/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 183-184
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 195 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@24de0f2e
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 195 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@24de0f2e
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 183.0 in stage 17.0 (TID 195). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 184.0 in stage 17.0 (TID 196, localhost, executor driver, partition 184, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 184.0 in stage 17.0 (TID 196)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 183.0 in stage 17.0 (TID 195) in 0 ms on localhost (executor driver) (179/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 184-185
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 196 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@55720819
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 196 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@55720819
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 184.0 in stage 17.0 (TID 196). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 185.0 in stage 17.0 (TID 197, localhost, executor driver, partition 185, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 185.0 in stage 17.0 (TID 197)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 184.0 in stage 17.0 (TID 196) in 0 ms on localhost (executor driver) (180/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 185-186
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 197 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@29261932
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 197 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@29261932
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 185.0 in stage 17.0 (TID 197). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 186.0 in stage 17.0 (TID 198, localhost, executor driver, partition 186, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 186.0 in stage 17.0 (TID 198)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 185.0 in stage 17.0 (TID 197) in 16 ms on localhost (executor driver) (181/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 186-187
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 198 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@787081f6
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 198 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@787081f6
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 186.0 in stage 17.0 (TID 198). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 187.0 in stage 17.0 (TID 199, localhost, executor driver, partition 187, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 187.0 in stage 17.0 (TID 199)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 186.0 in stage 17.0 (TID 198) in 0 ms on localhost (executor driver) (182/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 187-188
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 199 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7621117a
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 199 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7621117a
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 187.0 in stage 17.0 (TID 199). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 188.0 in stage 17.0 (TID 200, localhost, executor driver, partition 188, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 188.0 in stage 17.0 (TID 200)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 187.0 in stage 17.0 (TID 199) in 0 ms on localhost (executor driver) (183/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 188-189
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 200 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@689c0cd1
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 200 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@689c0cd1
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 188.0 in stage 17.0 (TID 200). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 189.0 in stage 17.0 (TID 201, localhost, executor driver, partition 189, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 189.0 in stage 17.0 (TID 201)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 188.0 in stage 17.0 (TID 200) in 0 ms on localhost (executor driver) (184/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 189-190
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 201 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@171688bd
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 201 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@171688bd
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 189.0 in stage 17.0 (TID 201). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 190.0 in stage 17.0 (TID 202, localhost, executor driver, partition 190, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 190.0 in stage 17.0 (TID 202)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 189.0 in stage 17.0 (TID 201) in 15 ms on localhost (executor driver) (185/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 190-191
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 202 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3fbff4ad
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 202 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3fbff4ad
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 190.0 in stage 17.0 (TID 202). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 191.0 in stage 17.0 (TID 203, localhost, executor driver, partition 191, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 191.0 in stage 17.0 (TID 203)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 190.0 in stage 17.0 (TID 202) in 0 ms on localhost (executor driver) (186/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 191-192
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 203 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5becf8ae
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 203 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5becf8ae
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 191.0 in stage 17.0 (TID 203). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 192.0 in stage 17.0 (TID 204, localhost, executor driver, partition 192, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 192.0 in stage 17.0 (TID 204)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 191.0 in stage 17.0 (TID 203) in 0 ms on localhost (executor driver) (187/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 192-193
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 204 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5a9804cc
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 204 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5a9804cc
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 192.0 in stage 17.0 (TID 204). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 193.0 in stage 17.0 (TID 205, localhost, executor driver, partition 193, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 193.0 in stage 17.0 (TID 205)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 192.0 in stage 17.0 (TID 204) in 17 ms on localhost (executor driver) (188/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 193-194
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 205 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6fc71ff4
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 205 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6fc71ff4
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 193.0 in stage 17.0 (TID 205). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 194.0 in stage 17.0 (TID 206, localhost, executor driver, partition 194, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 194.0 in stage 17.0 (TID 206)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 193.0 in stage 17.0 (TID 205) in 0 ms on localhost (executor driver) (189/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 194-195
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 206 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3fc5a1e9
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 206 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3fc5a1e9
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 194.0 in stage 17.0 (TID 206). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 195.0 in stage 17.0 (TID 207, localhost, executor driver, partition 195, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 195.0 in stage 17.0 (TID 207)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 194.0 in stage 17.0 (TID 206) in 0 ms on localhost (executor driver) (190/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 195-196
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 207 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6fd8b97a
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 207 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6fd8b97a
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 195.0 in stage 17.0 (TID 207). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 196.0 in stage 17.0 (TID 208, localhost, executor driver, partition 196, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 196.0 in stage 17.0 (TID 208)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 195.0 in stage 17.0 (TID 207) in 0 ms on localhost (executor driver) (191/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 196-197
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 208 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@14257153
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 208 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@14257153
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 196.0 in stage 17.0 (TID 208). 4496 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 198.0 in stage 17.0 (TID 209, localhost, executor driver, partition 198, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 198.0 in stage 17.0 (TID 209)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 196.0 in stage 17.0 (TID 208) in 15 ms on localhost (executor driver) (192/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 198-199
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 209 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5c52a2a6
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 209 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5c52a2a6
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 198.0 in stage 17.0 (TID 209). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 199.0 in stage 17.0 (TID 210, localhost, executor driver, partition 199, PROCESS_LOCAL, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 199.0 in stage 17.0 (TID 210)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 198.0 in stage 17.0 (TID 209) in 0 ms on localhost (executor driver) (193/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 199-200
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 210 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2b971302
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 210 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2b971302
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 199.0 in stage 17.0 (TID 210). 4410 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 97.0 in stage 17.0 (TID 211, localhost, executor driver, partition 97, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 97.0 in stage 17.0 (TID 211)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 199.0 in stage 17.0 (TID 210) in 16 ms on localhost (executor driver) (194/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 97-98
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_97
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 211 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4107d2ac
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 211 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@4107d2ac
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 211 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4107d2ac
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 211 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@4107d2ac
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 97.0 in stage 17.0 (TID 211). 4561 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 108.0 in stage 17.0 (TID 212, localhost, executor driver, partition 108, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 108.0 in stage 17.0 (TID 212)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 97.0 in stage 17.0 (TID 211) in 16 ms on localhost (executor driver) (195/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 108-109
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_108
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 212 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1077e920
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 212 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@1077e920
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 212 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1077e920
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 212 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@1077e920
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 108.0 in stage 17.0 (TID 212). 4561 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 116.0 in stage 17.0 (TID 213, localhost, executor driver, partition 116, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 116.0 in stage 17.0 (TID 213)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 108.0 in stage 17.0 (TID 212) in 0 ms on localhost (executor driver) (196/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 116-117
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_116
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 213 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@42948229
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 213 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@42948229
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 213 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@42948229
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 213 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@42948229
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 116.0 in stage 17.0 (TID 213). 4647 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 134.0 in stage 17.0 (TID 214, localhost, executor driver, partition 134, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 134.0 in stage 17.0 (TID 214)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 116.0 in stage 17.0 (TID 213) in 16 ms on localhost (executor driver) (197/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 134-135
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_134
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 214 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@63b8f6f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 214 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@63b8f6f
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 214 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@63b8f6f
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 214 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@63b8f6f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 134.0 in stage 17.0 (TID 214). 4561 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 156.0 in stage 17.0 (TID 215, localhost, executor driver, partition 156, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 156.0 in stage 17.0 (TID 215)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 134.0 in stage 17.0 (TID 214) in 0 ms on localhost (executor driver) (198/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 156-157
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_156
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 215 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4a1f6709
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 215 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@4a1f6709
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 215 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4a1f6709
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 215 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@4a1f6709
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 156.0 in stage 17.0 (TID 215). 4561 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 197.0 in stage 17.0 (TID 216, localhost, executor driver, partition 197, ANY, 7767 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 197.0 in stage 17.0 (TID 216)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 156.0 in stage 17.0 (TID 215) in 0 ms on localhost (executor driver) (199/200)
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 197-198
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_197
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  16 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 216 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@702324ec
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 216 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@702324ec
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 216 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@702324ec
2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 216 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@702324ec
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 197.0 in stage 17.0 (TID 216). 4647 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 197.0 in stage 17.0 (TID 216) in 16 ms on localhost (executor driver) (200/200)
2022-02-10 15:13:42 INFO  TaskSchedulerImpl:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2022-02-10 15:13:42 INFO  DAGScheduler:54 - ResultStage 17 (count at UseCase5.java:65) finished in 1.245 s
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - After removal of stage 17, remaining stages = 1
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - After removal of stage 16, remaining stages = 0
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Job 14 finished: count at UseCase5.java:65, took 1.425478 s
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[82] at count at UseCase5.java:65
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[82] at count at UseCase5.java:65)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[82] at count at UseCase5.java:65
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[82] at count at UseCase5.java:65)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:42 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:42 INFO  SparkContext:54 - Starting job: count at UseCase5.java:65
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Registering RDD 76 (count at UseCase5.java:65) as input to shuffle 3
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Registering RDD 79 (count at UseCase5.java:65) as input to shuffle 4
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Got job 15 (count at UseCase5.java:65) with 1 output partitions
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Final stage: ResultStage 21 (count at UseCase5.java:65)
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 20)
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 20)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - submitStage(ResultStage 21 (name=count at UseCase5.java:65;jobs=15))
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 20)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 20 (name=count at UseCase5.java:65;jobs=15))
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 19)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 19 (name=count at UseCase5.java:65;jobs=15))
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 19 (MapPartitionsRDD[76] at count at UseCase5.java:65), which has no missing parents
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 19)
2022-02-10 15:13:42 INFO  MemoryStore:54 - Block broadcast_35 stored as values in memory (estimated size 31.7 KB, free 1967.1 MB)
2022-02-10 15:13:42 DEBUG BlockManager:58 - Put block broadcast_35 locally took  0 ms
2022-02-10 15:13:42 DEBUG BlockManager:58 - Putting block broadcast_35 without replication took  0 ms
2022-02-10 15:13:42 INFO  MemoryStore:54 - Block broadcast_35_piece0 stored as bytes in memory (estimated size 14.7 KB, free 1967.1 MB)
2022-02-10 15:13:42 INFO  BlockManagerInfo:54 - Added broadcast_35_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 14.7 KB, free: 1970.2 MB)
2022-02-10 15:13:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_35_piece0
2022-02-10 15:13:42 DEBUG BlockManager:58 - Told master about block broadcast_35_piece0
2022-02-10 15:13:42 DEBUG BlockManager:58 - Put block broadcast_35_piece0 locally took  0 ms
2022-02-10 15:13:42 DEBUG BlockManager:58 - Putting block broadcast_35_piece0 without replication took  0 ms
2022-02-10 15:13:42 INFO  SparkContext:54 - Created broadcast 35 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:42 INFO  DAGScheduler:54 - Submitting 200 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[76] at count at UseCase5.java:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2022-02-10 15:13:42 INFO  TaskSchedulerImpl:54 - Adding task set 19.0 with 200 tasks
2022-02-10 15:13:42 DEBUG TaskSetManager:58 - Epoch for TaskSet 19.0: 3
2022-02-10 15:13:42 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 19.0: NO_PREF, ANY
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 0.0 in stage 19.0 (TID 217, localhost, executor driver, partition 0, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 0.0 in stage 19.0 (TID 217)
2022-02-10 15:13:42 DEBUG BlockManager:58 - Getting local block broadcast_35
2022-02-10 15:13:42 DEBUG BlockManager:58 - Level for block broadcast_35 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 0-1
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 217 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@52b0725
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 217 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@52b0725
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 0.0 in stage 19.0 (TID 217). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 1.0 in stage 19.0 (TID 218, localhost, executor driver, partition 1, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 1.0 in stage 19.0 (TID 218)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 0.0 in stage 19.0 (TID 217) in 15 ms on localhost (executor driver) (1/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 1-2
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 218 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@71caa37b
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 218 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@71caa37b
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 1.0 in stage 19.0 (TID 218). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 2.0 in stage 19.0 (TID 219, localhost, executor driver, partition 2, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 2.0 in stage 19.0 (TID 219)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 1.0 in stage 19.0 (TID 218) in 16 ms on localhost (executor driver) (2/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 2-3
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(579)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 579
2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 219 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@44ee2905
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 579
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(567)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 567
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 567
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(561)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 561
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 561
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(573)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 573
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 573
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(598)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 598
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 598
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(557)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 557
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 557
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(558)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 558
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 558
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(559)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 559
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 559
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(566)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 566
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 566
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(593)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 593
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 593
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(601)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 601
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 601
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(560)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 560
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 560
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(582)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 582
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 582
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(576)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 576
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 576
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(592)
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 219 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@44ee2905
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 592
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 592
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(564)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 564
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 564
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(591)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 591
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 591
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(554)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 554
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 554
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(580)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 580
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 580
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(585)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 585
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 585
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(583)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 583
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 583
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(587)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 587
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 587
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(589)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 589
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 589
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(600)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 600
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 600
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(577)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 577
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 577
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(597)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 597
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 597
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(571)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 571
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 571
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(572)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 572
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 572
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(578)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 578
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 578
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(556)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 556
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 556
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(575)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 575
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 575
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(568)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 568
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 568
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(562)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 562
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 562
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(34)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning broadcast 34
2022-02-10 15:13:42 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 34
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 34
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing broadcast 34
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing block broadcast_34
2022-02-10 15:13:42 DEBUG MemoryStore:58 - Block broadcast_34 of size 31320 dropped from memory (free 2062657481)
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing block broadcast_34_piece0
2022-02-10 15:13:42 DEBUG MemoryStore:58 - Block broadcast_34_piece0 of size 14574 dropped from memory (free 2062672055)
2022-02-10 15:13:42 INFO  BlockManagerInfo:54 - Removed broadcast_34_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 14.2 KB, free: 1970.3 MB)
2022-02-10 15:13:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_34_piece0
2022-02-10 15:13:42 DEBUG BlockManager:58 - Told master about block broadcast_34_piece0
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 2.0 in stage 19.0 (TID 219). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 34, response is 0
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaned broadcast 34
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(33)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning broadcast 33
2022-02-10 15:13:42 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 33
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 3.0 in stage 19.0 (TID 220, localhost, executor driver, partition 3, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 3.0 in stage 19.0 (TID 220)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 2.0 in stage 19.0 (TID 219) in 33 ms on localhost (executor driver) (3/200)
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 33
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing broadcast 33
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing block broadcast_33_piece0
2022-02-10 15:13:42 DEBUG MemoryStore:58 - Block broadcast_33_piece0 of size 14040 dropped from memory (free 2062686095)
2022-02-10 15:13:42 INFO  BlockManagerInfo:54 - Removed broadcast_33_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 13.7 KB, free: 1970.3 MB)
2022-02-10 15:13:42 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 15:13:42 DEBUG BlockManager:58 - Told master about block broadcast_33_piece0
2022-02-10 15:13:42 DEBUG BlockManager:58 - Removing block broadcast_33
2022-02-10 15:13:42 DEBUG MemoryStore:58 - Block broadcast_33 of size 31744 dropped from memory (free 2062717839)
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 33, response is 0
2022-02-10 15:13:42 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaned broadcast 33
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(595)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 595
2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 3-4
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 595
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(563)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 563
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 563
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(586)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 586
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 586
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(596)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 596
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 596
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(570)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 570
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 570
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(590)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 590
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 590
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(602)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 602
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 602
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(581)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 581
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 581
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(588)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 588
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 588
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(594)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 594
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 594
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(569)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 569
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 569
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(584)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 584
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 584
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(574)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 574
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 574
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(555)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 555
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 555
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(565)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 565
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 565
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(553)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 553
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 553
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(599)
2022-02-10 15:13:42 DEBUG ContextCleaner:58 - Cleaning accumulator 599
2022-02-10 15:13:42 INFO  ContextCleaner:54 - Cleaned accumulator 599
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 220 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3f160caa
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 220 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3f160caa
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 3.0 in stage 19.0 (TID 220). 4354 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 4.0 in stage 19.0 (TID 221, localhost, executor driver, partition 4, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 4.0 in stage 19.0 (TID 221)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 3.0 in stage 19.0 (TID 220) in 3 ms on localhost (executor driver) (4/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 4-5
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 221 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3985d45
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 221 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3985d45
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 4.0 in stage 19.0 (TID 221). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 5.0 in stage 19.0 (TID 222, localhost, executor driver, partition 5, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 5.0 in stage 19.0 (TID 222)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 4.0 in stage 19.0 (TID 221) in 16 ms on localhost (executor driver) (5/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 5-6
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 222 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@211bb159
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 222 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@211bb159
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 5.0 in stage 19.0 (TID 222). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 6.0 in stage 19.0 (TID 223, localhost, executor driver, partition 6, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 6.0 in stage 19.0 (TID 223)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 5.0 in stage 19.0 (TID 222) in 0 ms on localhost (executor driver) (6/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 6-7
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 223 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@56d9b950
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 223 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@56d9b950
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 6.0 in stage 19.0 (TID 223). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 7.0 in stage 19.0 (TID 224, localhost, executor driver, partition 7, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 7.0 in stage 19.0 (TID 224)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 6.0 in stage 19.0 (TID 223) in 16 ms on localhost (executor driver) (7/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 7-8
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 224 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1a452b95
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 224 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1a452b95
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 7.0 in stage 19.0 (TID 224). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 8.0 in stage 19.0 (TID 225, localhost, executor driver, partition 8, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 7.0 in stage 19.0 (TID 224) in 18 ms on localhost (executor driver) (8/200)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 8.0 in stage 19.0 (TID 225)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 8-9
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 225 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5dfbc526
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 225 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5dfbc526
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 8.0 in stage 19.0 (TID 225). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 9.0 in stage 19.0 (TID 226, localhost, executor driver, partition 9, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 9.0 in stage 19.0 (TID 226)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 8.0 in stage 19.0 (TID 225) in 8 ms on localhost (executor driver) (9/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 9-10
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 226 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@31694836
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 226 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@31694836
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 9.0 in stage 19.0 (TID 226). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 10.0 in stage 19.0 (TID 227, localhost, executor driver, partition 10, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 10.0 in stage 19.0 (TID 227)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 9.0 in stage 19.0 (TID 226) in 16 ms on localhost (executor driver) (10/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 10-11
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 227 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@f110b46
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 227 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@f110b46
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 10.0 in stage 19.0 (TID 227). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 11.0 in stage 19.0 (TID 228, localhost, executor driver, partition 11, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 11.0 in stage 19.0 (TID 228)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 10.0 in stage 19.0 (TID 227) in 0 ms on localhost (executor driver) (11/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 11-12
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 228 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7adfbc75
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 228 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7adfbc75
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 11.0 in stage 19.0 (TID 228). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 12.0 in stage 19.0 (TID 229, localhost, executor driver, partition 12, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 12.0 in stage 19.0 (TID 229)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 11.0 in stage 19.0 (TID 228) in 16 ms on localhost (executor driver) (12/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 12-13
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 229 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6c50df6
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 229 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6c50df6
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 12.0 in stage 19.0 (TID 229). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 13.0 in stage 19.0 (TID 230, localhost, executor driver, partition 13, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 13.0 in stage 19.0 (TID 230)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 12.0 in stage 19.0 (TID 229) in 0 ms on localhost (executor driver) (13/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 13-14
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 230 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2a33698
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 230 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2a33698
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 13.0 in stage 19.0 (TID 230). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 14.0 in stage 19.0 (TID 231, localhost, executor driver, partition 14, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 14.0 in stage 19.0 (TID 231)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 13.0 in stage 19.0 (TID 230) in 16 ms on localhost (executor driver) (14/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 14-15
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 231 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6d6fcd9f
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 231 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6d6fcd9f
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 14.0 in stage 19.0 (TID 231). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 15.0 in stage 19.0 (TID 232, localhost, executor driver, partition 15, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 15.0 in stage 19.0 (TID 232)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 14.0 in stage 19.0 (TID 231) in 15 ms on localhost (executor driver) (15/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 15-16
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 232 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@39170593
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 232 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@39170593
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 15.0 in stage 19.0 (TID 232). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 16.0 in stage 19.0 (TID 233, localhost, executor driver, partition 16, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 16.0 in stage 19.0 (TID 233)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 15.0 in stage 19.0 (TID 232) in 0 ms on localhost (executor driver) (16/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 16-17
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 233 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@d84ecd9
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 233 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@d84ecd9
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 16.0 in stage 19.0 (TID 233). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 17.0 in stage 19.0 (TID 234, localhost, executor driver, partition 17, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 17.0 in stage 19.0 (TID 234)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 16.0 in stage 19.0 (TID 233) in 16 ms on localhost (executor driver) (17/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 17-18
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 234 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@26ebaf4c
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 234 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@26ebaf4c
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 17.0 in stage 19.0 (TID 234). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 18.0 in stage 19.0 (TID 235, localhost, executor driver, partition 18, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 18.0 in stage 19.0 (TID 235)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 17.0 in stage 19.0 (TID 234) in 0 ms on localhost (executor driver) (18/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 18-19
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 235 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@12d70ca8
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 235 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@12d70ca8
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 18.0 in stage 19.0 (TID 235). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 19.0 in stage 19.0 (TID 236, localhost, executor driver, partition 19, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 19.0 in stage 19.0 (TID 236)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 18.0 in stage 19.0 (TID 235) in 16 ms on localhost (executor driver) (19/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 19-20
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 236 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@35e547d3
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 236 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@35e547d3
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 19.0 in stage 19.0 (TID 236). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 20.0 in stage 19.0 (TID 237, localhost, executor driver, partition 20, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 20.0 in stage 19.0 (TID 237)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 19.0 in stage 19.0 (TID 236) in 0 ms on localhost (executor driver) (20/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 20-21
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 237 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@536b68
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 237 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@536b68
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 20.0 in stage 19.0 (TID 237). 4397 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 21.0 in stage 19.0 (TID 238, localhost, executor driver, partition 21, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 21.0 in stage 19.0 (TID 238)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 20.0 in stage 19.0 (TID 237) in 15 ms on localhost (executor driver) (21/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 21-22
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 238 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1af27f22
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:233 - Task 238 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1af27f22
2022-02-10 15:13:42 INFO  Executor:54 - Finished task 21.0 in stage 19.0 (TID 238). 4311 bytes result sent to driver
2022-02-10 15:13:42 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Starting task 22.0 in stage 19.0 (TID 239, localhost, executor driver, partition 22, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:42 INFO  Executor:54 - Running task 22.0 in stage 19.0 (TID 239)
2022-02-10 15:13:42 INFO  TaskSetManager:54 - Finished task 21.0 in stage 19.0 (TID 238) in 0 ms on localhost (executor driver) (22/200)
2022-02-10 15:13:42 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:42 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:42 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 22-23
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:42 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:42 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:42 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:42 DEBUG TaskMemoryManager:224 - Task 239 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@78dbb75f
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 239 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@78dbb75f
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 22.0 in stage 19.0 (TID 239). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 23.0 in stage 19.0 (TID 240, localhost, executor driver, partition 23, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 23.0 in stage 19.0 (TID 240)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 22.0 in stage 19.0 (TID 239) in 16 ms on localhost (executor driver) (23/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 23-24
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 240 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@363ea888
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 240 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@363ea888
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 23.0 in stage 19.0 (TID 240). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 24.0 in stage 19.0 (TID 241, localhost, executor driver, partition 24, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 24.0 in stage 19.0 (TID 241)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 23.0 in stage 19.0 (TID 240) in 0 ms on localhost (executor driver) (24/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 24-25
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 241 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@690274d4
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 241 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@690274d4
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 24.0 in stage 19.0 (TID 241). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 25.0 in stage 19.0 (TID 242, localhost, executor driver, partition 25, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 25.0 in stage 19.0 (TID 242)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 24.0 in stage 19.0 (TID 241) in 16 ms on localhost (executor driver) (25/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 25-26
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 242 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4ecf3272
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 242 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4ecf3272
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 25.0 in stage 19.0 (TID 242). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 26.0 in stage 19.0 (TID 243, localhost, executor driver, partition 26, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 26.0 in stage 19.0 (TID 243)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 25.0 in stage 19.0 (TID 242) in 15 ms on localhost (executor driver) (26/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 26-27
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 243 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@26a1fc3a
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 243 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@26a1fc3a
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 26.0 in stage 19.0 (TID 243). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 27.0 in stage 19.0 (TID 244, localhost, executor driver, partition 27, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 27.0 in stage 19.0 (TID 244)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 26.0 in stage 19.0 (TID 243) in 0 ms on localhost (executor driver) (27/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 27-28
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 244 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@42eed26
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 244 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@42eed26
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 27.0 in stage 19.0 (TID 244). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 28.0 in stage 19.0 (TID 245, localhost, executor driver, partition 28, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 28.0 in stage 19.0 (TID 245)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 27.0 in stage 19.0 (TID 244) in 16 ms on localhost (executor driver) (28/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 28-29
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 245 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@320284bf
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 245 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@320284bf
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 28.0 in stage 19.0 (TID 245). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 29.0 in stage 19.0 (TID 246, localhost, executor driver, partition 29, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 29.0 in stage 19.0 (TID 246)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 28.0 in stage 19.0 (TID 245) in 16 ms on localhost (executor driver) (29/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 29-30
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 246 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1c10f9b4
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 246 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1c10f9b4
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 29.0 in stage 19.0 (TID 246). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 30.0 in stage 19.0 (TID 247, localhost, executor driver, partition 30, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 30.0 in stage 19.0 (TID 247)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 29.0 in stage 19.0 (TID 246) in 0 ms on localhost (executor driver) (30/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 30-31
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 247 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4c6d6325
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 247 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4c6d6325
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 30.0 in stage 19.0 (TID 247). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 31.0 in stage 19.0 (TID 248, localhost, executor driver, partition 31, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 31.0 in stage 19.0 (TID 248)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 30.0 in stage 19.0 (TID 247) in 16 ms on localhost (executor driver) (31/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 31-32
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 248 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@798cb824
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 248 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@798cb824
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 31.0 in stage 19.0 (TID 248). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 32.0 in stage 19.0 (TID 249, localhost, executor driver, partition 32, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 32.0 in stage 19.0 (TID 249)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 31.0 in stage 19.0 (TID 248) in 16 ms on localhost (executor driver) (32/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 32-33
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 249 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54f637c5
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 249 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54f637c5
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 32.0 in stage 19.0 (TID 249). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 33.0 in stage 19.0 (TID 250, localhost, executor driver, partition 33, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 33.0 in stage 19.0 (TID 250)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 32.0 in stage 19.0 (TID 249) in 0 ms on localhost (executor driver) (33/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 33-34
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 250 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1444ec43
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 250 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1444ec43
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 33.0 in stage 19.0 (TID 250). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 34.0 in stage 19.0 (TID 251, localhost, executor driver, partition 34, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 34.0 in stage 19.0 (TID 251)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 33.0 in stage 19.0 (TID 250) in 16 ms on localhost (executor driver) (34/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 34-35
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 251 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2a57dcc
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 251 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2a57dcc
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 34.0 in stage 19.0 (TID 251). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 35.0 in stage 19.0 (TID 252, localhost, executor driver, partition 35, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 35.0 in stage 19.0 (TID 252)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 34.0 in stage 19.0 (TID 251) in 15 ms on localhost (executor driver) (35/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 35-36
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 252 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5fba26c8
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 252 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5fba26c8
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 35.0 in stage 19.0 (TID 252). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 36.0 in stage 19.0 (TID 253, localhost, executor driver, partition 36, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 36.0 in stage 19.0 (TID 253)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 35.0 in stage 19.0 (TID 252) in 0 ms on localhost (executor driver) (36/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 36-37
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 253 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4c0c5bdd
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 253 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4c0c5bdd
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 36.0 in stage 19.0 (TID 253). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 37.0 in stage 19.0 (TID 254, localhost, executor driver, partition 37, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 37.0 in stage 19.0 (TID 254)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 36.0 in stage 19.0 (TID 253) in 17 ms on localhost (executor driver) (37/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 37-38
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 254 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@59e86ba
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 254 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@59e86ba
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 37.0 in stage 19.0 (TID 254). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 38.0 in stage 19.0 (TID 255, localhost, executor driver, partition 38, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 38.0 in stage 19.0 (TID 255)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 37.0 in stage 19.0 (TID 254) in 0 ms on localhost (executor driver) (38/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 38-39
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 255 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@51fc87ec
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 255 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@51fc87ec
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 38.0 in stage 19.0 (TID 255). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 39.0 in stage 19.0 (TID 256, localhost, executor driver, partition 39, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 39.0 in stage 19.0 (TID 256)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 38.0 in stage 19.0 (TID 255) in 15 ms on localhost (executor driver) (39/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 39-40
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 256 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1223d419
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 256 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1223d419
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 39.0 in stage 19.0 (TID 256). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 40.0 in stage 19.0 (TID 257, localhost, executor driver, partition 40, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 40.0 in stage 19.0 (TID 257)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 39.0 in stage 19.0 (TID 256) in 0 ms on localhost (executor driver) (40/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 40-41
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 257 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@318f11a7
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 257 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@318f11a7
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 40.0 in stage 19.0 (TID 257). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 41.0 in stage 19.0 (TID 258, localhost, executor driver, partition 41, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 41.0 in stage 19.0 (TID 258)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 40.0 in stage 19.0 (TID 257) in 16 ms on localhost (executor driver) (41/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 41-42
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 258 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@34b6928
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 258 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@34b6928
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 41.0 in stage 19.0 (TID 258). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 42.0 in stage 19.0 (TID 259, localhost, executor driver, partition 42, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 42.0 in stage 19.0 (TID 259)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 41.0 in stage 19.0 (TID 258) in 0 ms on localhost (executor driver) (42/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 42-43
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 259 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1990811
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 259 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1990811
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 42.0 in stage 19.0 (TID 259). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 43.0 in stage 19.0 (TID 260, localhost, executor driver, partition 43, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 42.0 in stage 19.0 (TID 259) in 16 ms on localhost (executor driver) (43/200)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 43.0 in stage 19.0 (TID 260)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 43-44
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 260 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7c980f8a
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 260 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7c980f8a
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 43.0 in stage 19.0 (TID 260). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 44.0 in stage 19.0 (TID 261, localhost, executor driver, partition 44, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 44.0 in stage 19.0 (TID 261)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 43.0 in stage 19.0 (TID 260) in 0 ms on localhost (executor driver) (44/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 44-45
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 261 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@bfe29c5
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 261 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@bfe29c5
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 44.0 in stage 19.0 (TID 261). 4440 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 45.0 in stage 19.0 (TID 262, localhost, executor driver, partition 45, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 45.0 in stage 19.0 (TID 262)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 44.0 in stage 19.0 (TID 261) in 16 ms on localhost (executor driver) (45/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 45-46
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 262 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@461dbca7
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 262 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@461dbca7
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 45.0 in stage 19.0 (TID 262). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 46.0 in stage 19.0 (TID 263, localhost, executor driver, partition 46, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 46.0 in stage 19.0 (TID 263)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 45.0 in stage 19.0 (TID 262) in 15 ms on localhost (executor driver) (46/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 46-47
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 263 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1045c83
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 263 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1045c83
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 46.0 in stage 19.0 (TID 263). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 47.0 in stage 19.0 (TID 264, localhost, executor driver, partition 47, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 47.0 in stage 19.0 (TID 264)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 46.0 in stage 19.0 (TID 263) in 4 ms on localhost (executor driver) (47/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 47-48
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 264 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@65ee716c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 264 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@65ee716c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 47.0 in stage 19.0 (TID 264). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 48.0 in stage 19.0 (TID 265, localhost, executor driver, partition 48, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 48.0 in stage 19.0 (TID 265)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 47.0 in stage 19.0 (TID 264) in 17 ms on localhost (executor driver) (48/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 48-49
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 265 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@294d5482
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 265 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@294d5482
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 48.0 in stage 19.0 (TID 265). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 49.0 in stage 19.0 (TID 266, localhost, executor driver, partition 49, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 48.0 in stage 19.0 (TID 265) in 1 ms on localhost (executor driver) (49/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 INFO  Executor:54 - Running task 49.0 in stage 19.0 (TID 266)
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 49-50
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 266 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1cc039d3
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 266 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1cc039d3
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 49.0 in stage 19.0 (TID 266). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 50.0 in stage 19.0 (TID 267, localhost, executor driver, partition 50, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 50.0 in stage 19.0 (TID 267)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 49.0 in stage 19.0 (TID 266) in 16 ms on localhost (executor driver) (50/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 50-51
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 267 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@13f538cb
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 267 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@13f538cb
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 50.0 in stage 19.0 (TID 267). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 51.0 in stage 19.0 (TID 268, localhost, executor driver, partition 51, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 51.0 in stage 19.0 (TID 268)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 50.0 in stage 19.0 (TID 267) in 17 ms on localhost (executor driver) (51/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 51-52
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 268 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4b622b81
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 268 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4b622b81
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 51.0 in stage 19.0 (TID 268). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 52.0 in stage 19.0 (TID 269, localhost, executor driver, partition 52, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 52.0 in stage 19.0 (TID 269)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 51.0 in stage 19.0 (TID 268) in 15 ms on localhost (executor driver) (52/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 52-53
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 269 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@10a59c2
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 269 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@10a59c2
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 52.0 in stage 19.0 (TID 269). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 53.0 in stage 19.0 (TID 270, localhost, executor driver, partition 53, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 53.0 in stage 19.0 (TID 270)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 52.0 in stage 19.0 (TID 269) in 0 ms on localhost (executor driver) (53/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 53-54
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 270 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@64991e7e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 270 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@64991e7e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 53.0 in stage 19.0 (TID 270). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 54.0 in stage 19.0 (TID 271, localhost, executor driver, partition 54, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 54.0 in stage 19.0 (TID 271)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 53.0 in stage 19.0 (TID 270) in 16 ms on localhost (executor driver) (54/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 54-55
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 271 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5d7c54b2
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 271 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5d7c54b2
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 54.0 in stage 19.0 (TID 271). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 55.0 in stage 19.0 (TID 272, localhost, executor driver, partition 55, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 55.0 in stage 19.0 (TID 272)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 54.0 in stage 19.0 (TID 271) in 16 ms on localhost (executor driver) (55/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 55-56
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 272 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3c14b24d
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 272 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3c14b24d
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 55.0 in stage 19.0 (TID 272). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 56.0 in stage 19.0 (TID 273, localhost, executor driver, partition 56, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 55.0 in stage 19.0 (TID 272) in 1 ms on localhost (executor driver) (56/200)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 56.0 in stage 19.0 (TID 273)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 56-57
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 273 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3e6845e9
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 273 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3e6845e9
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 56.0 in stage 19.0 (TID 273). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 57.0 in stage 19.0 (TID 274, localhost, executor driver, partition 57, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 57.0 in stage 19.0 (TID 274)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 56.0 in stage 19.0 (TID 273) in 16 ms on localhost (executor driver) (57/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 57-58
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 274 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2a1ac213
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 274 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2a1ac213
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 57.0 in stage 19.0 (TID 274). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 58.0 in stage 19.0 (TID 275, localhost, executor driver, partition 58, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 58.0 in stage 19.0 (TID 275)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 57.0 in stage 19.0 (TID 274) in 16 ms on localhost (executor driver) (58/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 58-59
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 275 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@63b8d031
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 275 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@63b8d031
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 58.0 in stage 19.0 (TID 275). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 59.0 in stage 19.0 (TID 276, localhost, executor driver, partition 59, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 59.0 in stage 19.0 (TID 276)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 58.0 in stage 19.0 (TID 275) in 0 ms on localhost (executor driver) (59/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 59-60
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 276 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3330169c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 276 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3330169c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 59.0 in stage 19.0 (TID 276). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 60.0 in stage 19.0 (TID 277, localhost, executor driver, partition 60, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 60.0 in stage 19.0 (TID 277)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 59.0 in stage 19.0 (TID 276) in 17 ms on localhost (executor driver) (60/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 60-61
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 277 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@23eaed0d
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 277 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@23eaed0d
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 60.0 in stage 19.0 (TID 277). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 61.0 in stage 19.0 (TID 278, localhost, executor driver, partition 61, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 61.0 in stage 19.0 (TID 278)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 60.0 in stage 19.0 (TID 277) in 0 ms on localhost (executor driver) (61/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 61-62
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 278 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6ba8f582
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 278 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6ba8f582
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 61.0 in stage 19.0 (TID 278). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 62.0 in stage 19.0 (TID 279, localhost, executor driver, partition 62, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 62.0 in stage 19.0 (TID 279)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 61.0 in stage 19.0 (TID 278) in 16 ms on localhost (executor driver) (62/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 62-63
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 279 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2397fe57
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 279 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2397fe57
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 62.0 in stage 19.0 (TID 279). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 63.0 in stage 19.0 (TID 280, localhost, executor driver, partition 63, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 63.0 in stage 19.0 (TID 280)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 62.0 in stage 19.0 (TID 279) in 0 ms on localhost (executor driver) (63/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 63-64
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 280 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@631e59ae
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 280 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@631e59ae
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 63.0 in stage 19.0 (TID 280). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 64.0 in stage 19.0 (TID 281, localhost, executor driver, partition 64, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 64.0 in stage 19.0 (TID 281)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 63.0 in stage 19.0 (TID 280) in 16 ms on localhost (executor driver) (64/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 64-65
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 281 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3a70bb29
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 281 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3a70bb29
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 64.0 in stage 19.0 (TID 281). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 65.0 in stage 19.0 (TID 282, localhost, executor driver, partition 65, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 65.0 in stage 19.0 (TID 282)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 64.0 in stage 19.0 (TID 281) in 0 ms on localhost (executor driver) (65/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 65-66
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 282 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4f8d980c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 282 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4f8d980c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 65.0 in stage 19.0 (TID 282). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 66.0 in stage 19.0 (TID 283, localhost, executor driver, partition 66, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 66.0 in stage 19.0 (TID 283)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 65.0 in stage 19.0 (TID 282) in 16 ms on localhost (executor driver) (66/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 66-67
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 283 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@730d140a
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 283 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@730d140a
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 66.0 in stage 19.0 (TID 283). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 67.0 in stage 19.0 (TID 284, localhost, executor driver, partition 67, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 67.0 in stage 19.0 (TID 284)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 66.0 in stage 19.0 (TID 283) in 0 ms on localhost (executor driver) (67/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 67-68
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 284 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@ad2525a
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 284 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@ad2525a
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 67.0 in stage 19.0 (TID 284). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 68.0 in stage 19.0 (TID 285, localhost, executor driver, partition 68, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 68.0 in stage 19.0 (TID 285)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 67.0 in stage 19.0 (TID 284) in 15 ms on localhost (executor driver) (68/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 68-69
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 285 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@481b63e2
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 285 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@481b63e2
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 68.0 in stage 19.0 (TID 285). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 69.0 in stage 19.0 (TID 286, localhost, executor driver, partition 69, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 69.0 in stage 19.0 (TID 286)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 68.0 in stage 19.0 (TID 285) in 17 ms on localhost (executor driver) (69/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 69-70
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 286 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@56cbc7b8
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 286 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@56cbc7b8
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 69.0 in stage 19.0 (TID 286). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 70.0 in stage 19.0 (TID 287, localhost, executor driver, partition 70, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 70.0 in stage 19.0 (TID 287)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 69.0 in stage 19.0 (TID 286) in 0 ms on localhost (executor driver) (70/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 70-71
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  17 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 287 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7f1d4159
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 287 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7f1d4159
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 70.0 in stage 19.0 (TID 287). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 71.0 in stage 19.0 (TID 288, localhost, executor driver, partition 71, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 71.0 in stage 19.0 (TID 288)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 70.0 in stage 19.0 (TID 287) in 17 ms on localhost (executor driver) (71/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 71-72
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 288 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30606c60
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 288 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30606c60
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 71.0 in stage 19.0 (TID 288). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 72.0 in stage 19.0 (TID 289, localhost, executor driver, partition 72, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 72.0 in stage 19.0 (TID 289)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 71.0 in stage 19.0 (TID 288) in 16 ms on localhost (executor driver) (72/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 72-73
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 289 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@61a25024
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 289 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@61a25024
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 72.0 in stage 19.0 (TID 289). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 73.0 in stage 19.0 (TID 290, localhost, executor driver, partition 73, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 73.0 in stage 19.0 (TID 290)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 72.0 in stage 19.0 (TID 289) in 0 ms on localhost (executor driver) (73/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 73-74
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 290 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6a898485
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 290 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6a898485
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 73.0 in stage 19.0 (TID 290). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 74.0 in stage 19.0 (TID 291, localhost, executor driver, partition 74, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 74.0 in stage 19.0 (TID 291)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 73.0 in stage 19.0 (TID 290) in 15 ms on localhost (executor driver) (74/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 74-75
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 291 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54c07aa7
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 291 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54c07aa7
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 74.0 in stage 19.0 (TID 291). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 75.0 in stage 19.0 (TID 292, localhost, executor driver, partition 75, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 75.0 in stage 19.0 (TID 292)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 74.0 in stage 19.0 (TID 291) in 0 ms on localhost (executor driver) (75/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 75-76
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 292 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@19689ccf
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 292 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@19689ccf
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 75.0 in stage 19.0 (TID 292). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 76.0 in stage 19.0 (TID 293, localhost, executor driver, partition 76, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 76.0 in stage 19.0 (TID 293)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 75.0 in stage 19.0 (TID 292) in 16 ms on localhost (executor driver) (76/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 76-77
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 293 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@320f5877
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 293 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@320f5877
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 76.0 in stage 19.0 (TID 293). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 77.0 in stage 19.0 (TID 294, localhost, executor driver, partition 77, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 77.0 in stage 19.0 (TID 294)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 76.0 in stage 19.0 (TID 293) in 0 ms on localhost (executor driver) (77/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 77-78
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 294 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@d9c4236
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 294 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@d9c4236
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 77.0 in stage 19.0 (TID 294). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 78.0 in stage 19.0 (TID 295, localhost, executor driver, partition 78, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 78.0 in stage 19.0 (TID 295)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 77.0 in stage 19.0 (TID 294) in 16 ms on localhost (executor driver) (78/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 78-79
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 295 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@47b7e61e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 295 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@47b7e61e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 78.0 in stage 19.0 (TID 295). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 79.0 in stage 19.0 (TID 296, localhost, executor driver, partition 79, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 79.0 in stage 19.0 (TID 296)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 78.0 in stage 19.0 (TID 295) in 0 ms on localhost (executor driver) (79/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 79-80
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 296 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3de359e2
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 296 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3de359e2
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 79.0 in stage 19.0 (TID 296). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 80.0 in stage 19.0 (TID 297, localhost, executor driver, partition 80, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 80.0 in stage 19.0 (TID 297)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 79.0 in stage 19.0 (TID 296) in 16 ms on localhost (executor driver) (80/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 80-81
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 297 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6d275b08
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 297 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6d275b08
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 80.0 in stage 19.0 (TID 297). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 81.0 in stage 19.0 (TID 298, localhost, executor driver, partition 81, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 81.0 in stage 19.0 (TID 298)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 80.0 in stage 19.0 (TID 297) in 0 ms on localhost (executor driver) (81/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 81-82
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 298 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2b86fc4e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 298 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2b86fc4e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 81.0 in stage 19.0 (TID 298). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 82.0 in stage 19.0 (TID 299, localhost, executor driver, partition 82, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 82.0 in stage 19.0 (TID 299)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 81.0 in stage 19.0 (TID 298) in 16 ms on localhost (executor driver) (82/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 82-83
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 299 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@640fe749
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 299 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@640fe749
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 82.0 in stage 19.0 (TID 299). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 83.0 in stage 19.0 (TID 300, localhost, executor driver, partition 83, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 83.0 in stage 19.0 (TID 300)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 82.0 in stage 19.0 (TID 299) in 0 ms on localhost (executor driver) (83/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 83-84
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 300 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2d32ae32
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 300 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2d32ae32
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 83.0 in stage 19.0 (TID 300). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 84.0 in stage 19.0 (TID 301, localhost, executor driver, partition 84, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 84.0 in stage 19.0 (TID 301)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 83.0 in stage 19.0 (TID 300) in 15 ms on localhost (executor driver) (84/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 84-85
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 301 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@8c1a81e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 301 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@8c1a81e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 84.0 in stage 19.0 (TID 301). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 85.0 in stage 19.0 (TID 302, localhost, executor driver, partition 85, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 85.0 in stage 19.0 (TID 302)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 84.0 in stage 19.0 (TID 301) in 0 ms on localhost (executor driver) (85/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 85-86
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 302 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2b9cbde2
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 302 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2b9cbde2
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 85.0 in stage 19.0 (TID 302). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 86.0 in stage 19.0 (TID 303, localhost, executor driver, partition 86, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 86.0 in stage 19.0 (TID 303)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 85.0 in stage 19.0 (TID 302) in 16 ms on localhost (executor driver) (86/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 86-87
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 303 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4416c1bd
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 303 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4416c1bd
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 86.0 in stage 19.0 (TID 303). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 87.0 in stage 19.0 (TID 304, localhost, executor driver, partition 87, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 87.0 in stage 19.0 (TID 304)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 86.0 in stage 19.0 (TID 303) in 0 ms on localhost (executor driver) (87/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 87-88
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 304 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@765524ac
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 304 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@765524ac
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 87.0 in stage 19.0 (TID 304). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 88.0 in stage 19.0 (TID 305, localhost, executor driver, partition 88, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 88.0 in stage 19.0 (TID 305)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 87.0 in stage 19.0 (TID 304) in 16 ms on localhost (executor driver) (88/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 88-89
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 305 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@63cc63ef
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 305 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@63cc63ef
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 88.0 in stage 19.0 (TID 305). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 89.0 in stage 19.0 (TID 306, localhost, executor driver, partition 89, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 89.0 in stage 19.0 (TID 306)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 88.0 in stage 19.0 (TID 305) in 0 ms on localhost (executor driver) (89/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 89-90
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 306 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2d08217a
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 306 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2d08217a
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 89.0 in stage 19.0 (TID 306). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 90.0 in stage 19.0 (TID 307, localhost, executor driver, partition 90, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 90.0 in stage 19.0 (TID 307)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 89.0 in stage 19.0 (TID 306) in 15 ms on localhost (executor driver) (90/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 90-91
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 307 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7a4358bf
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 307 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7a4358bf
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 90.0 in stage 19.0 (TID 307). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 91.0 in stage 19.0 (TID 308, localhost, executor driver, partition 91, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 91.0 in stage 19.0 (TID 308)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 90.0 in stage 19.0 (TID 307) in 17 ms on localhost (executor driver) (91/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 91-92
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 308 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7bb1c259
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 308 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7bb1c259
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 91.0 in stage 19.0 (TID 308). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 92.0 in stage 19.0 (TID 309, localhost, executor driver, partition 92, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 92.0 in stage 19.0 (TID 309)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 91.0 in stage 19.0 (TID 308) in 2 ms on localhost (executor driver) (92/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 92-93
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 309 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@32cf1d2e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 309 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@32cf1d2e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 92.0 in stage 19.0 (TID 309). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 93.0 in stage 19.0 (TID 310, localhost, executor driver, partition 93, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 93.0 in stage 19.0 (TID 310)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 92.0 in stage 19.0 (TID 309) in 17 ms on localhost (executor driver) (93/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 93-94
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 310 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@98dfe12
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 310 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@98dfe12
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 93.0 in stage 19.0 (TID 310). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 94.0 in stage 19.0 (TID 311, localhost, executor driver, partition 94, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 94.0 in stage 19.0 (TID 311)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 93.0 in stage 19.0 (TID 310) in 0 ms on localhost (executor driver) (94/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 94-95
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 311 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54162b8e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 311 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54162b8e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 94.0 in stage 19.0 (TID 311). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 95.0 in stage 19.0 (TID 312, localhost, executor driver, partition 95, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 95.0 in stage 19.0 (TID 312)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 94.0 in stage 19.0 (TID 311) in 16 ms on localhost (executor driver) (95/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 95-96
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 312 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6f1cb11d
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 312 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6f1cb11d
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 95.0 in stage 19.0 (TID 312). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 96.0 in stage 19.0 (TID 313, localhost, executor driver, partition 96, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 96.0 in stage 19.0 (TID 313)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 95.0 in stage 19.0 (TID 312) in 6 ms on localhost (executor driver) (96/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 96-97
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 313 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@60890a2c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 313 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@60890a2c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 96.0 in stage 19.0 (TID 313). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 98.0 in stage 19.0 (TID 314, localhost, executor driver, partition 98, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 98.0 in stage 19.0 (TID 314)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 96.0 in stage 19.0 (TID 313) in 0 ms on localhost (executor driver) (97/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 98-99
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 314 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7e11f99b
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 314 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7e11f99b
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 98.0 in stage 19.0 (TID 314). 4440 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 99.0 in stage 19.0 (TID 315, localhost, executor driver, partition 99, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 99.0 in stage 19.0 (TID 315)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 98.0 in stage 19.0 (TID 314) in 16 ms on localhost (executor driver) (98/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 99-100
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 315 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3a169e16
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 315 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3a169e16
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 99.0 in stage 19.0 (TID 315). 4354 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 100.0 in stage 19.0 (TID 316, localhost, executor driver, partition 100, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 100.0 in stage 19.0 (TID 316)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 99.0 in stage 19.0 (TID 315) in 16 ms on localhost (executor driver) (99/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 100-101
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 316 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@416acc63
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 316 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@416acc63
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 100.0 in stage 19.0 (TID 316). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 101.0 in stage 19.0 (TID 317, localhost, executor driver, partition 101, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 101.0 in stage 19.0 (TID 317)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 100.0 in stage 19.0 (TID 316) in 0 ms on localhost (executor driver) (100/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 101-102
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 317 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6da8987c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 317 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6da8987c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 101.0 in stage 19.0 (TID 317). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 102.0 in stage 19.0 (TID 318, localhost, executor driver, partition 102, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 102.0 in stage 19.0 (TID 318)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 101.0 in stage 19.0 (TID 317) in 15 ms on localhost (executor driver) (101/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 102-103
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 318 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@669f2b3
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 318 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@669f2b3
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 102.0 in stage 19.0 (TID 318). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 103.0 in stage 19.0 (TID 319, localhost, executor driver, partition 103, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 103.0 in stage 19.0 (TID 319)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 102.0 in stage 19.0 (TID 318) in 0 ms on localhost (executor driver) (102/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 103-104
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 319 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@fedd3d9
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 319 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@fedd3d9
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 103.0 in stage 19.0 (TID 319). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 104.0 in stage 19.0 (TID 320, localhost, executor driver, partition 104, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 104.0 in stage 19.0 (TID 320)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 103.0 in stage 19.0 (TID 319) in 16 ms on localhost (executor driver) (103/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 104-105
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 320 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5614254d
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 320 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5614254d
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 104.0 in stage 19.0 (TID 320). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 105.0 in stage 19.0 (TID 321, localhost, executor driver, partition 105, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 105.0 in stage 19.0 (TID 321)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 104.0 in stage 19.0 (TID 320) in 0 ms on localhost (executor driver) (104/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 105-106
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 321 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30781f48
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 321 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30781f48
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 105.0 in stage 19.0 (TID 321). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 106.0 in stage 19.0 (TID 322, localhost, executor driver, partition 106, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 106.0 in stage 19.0 (TID 322)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 105.0 in stage 19.0 (TID 321) in 16 ms on localhost (executor driver) (105/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 106-107
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 322 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2c5a5614
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 322 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2c5a5614
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 106.0 in stage 19.0 (TID 322). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 107.0 in stage 19.0 (TID 323, localhost, executor driver, partition 107, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 107.0 in stage 19.0 (TID 323)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 106.0 in stage 19.0 (TID 322) in 0 ms on localhost (executor driver) (106/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 107-108
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 323 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2c36cac7
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 323 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2c36cac7
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 107.0 in stage 19.0 (TID 323). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 109.0 in stage 19.0 (TID 324, localhost, executor driver, partition 109, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 109.0 in stage 19.0 (TID 324)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 107.0 in stage 19.0 (TID 323) in 16 ms on localhost (executor driver) (107/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 109-110
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 324 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@70ed61fc
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 324 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@70ed61fc
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 109.0 in stage 19.0 (TID 324). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 110.0 in stage 19.0 (TID 325, localhost, executor driver, partition 110, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 110.0 in stage 19.0 (TID 325)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 109.0 in stage 19.0 (TID 324) in 0 ms on localhost (executor driver) (108/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 110-111
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 325 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1b59324
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 325 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1b59324
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 110.0 in stage 19.0 (TID 325). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 111.0 in stage 19.0 (TID 326, localhost, executor driver, partition 111, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 111.0 in stage 19.0 (TID 326)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 110.0 in stage 19.0 (TID 325) in 16 ms on localhost (executor driver) (109/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 111-112
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 326 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@73f9f2b4
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 326 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@73f9f2b4
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 111.0 in stage 19.0 (TID 326). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 112.0 in stage 19.0 (TID 327, localhost, executor driver, partition 112, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 112.0 in stage 19.0 (TID 327)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 111.0 in stage 19.0 (TID 326) in 0 ms on localhost (executor driver) (110/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 112-113
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 327 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4a97d98c
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 327 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4a97d98c
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 112.0 in stage 19.0 (TID 327). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 113.0 in stage 19.0 (TID 328, localhost, executor driver, partition 113, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 113.0 in stage 19.0 (TID 328)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 112.0 in stage 19.0 (TID 327) in 15 ms on localhost (executor driver) (111/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 113-114
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 328 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@10e1b273
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 328 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@10e1b273
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 113.0 in stage 19.0 (TID 328). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 114.0 in stage 19.0 (TID 329, localhost, executor driver, partition 114, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 114.0 in stage 19.0 (TID 329)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 113.0 in stage 19.0 (TID 328) in 16 ms on localhost (executor driver) (112/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 114-115
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 329 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@408f111
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 329 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@408f111
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 114.0 in stage 19.0 (TID 329). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 115.0 in stage 19.0 (TID 330, localhost, executor driver, partition 115, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 115.0 in stage 19.0 (TID 330)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 114.0 in stage 19.0 (TID 329) in 0 ms on localhost (executor driver) (113/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 115-116
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 330 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@667c7cc8
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 330 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@667c7cc8
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 115.0 in stage 19.0 (TID 330). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 117.0 in stage 19.0 (TID 331, localhost, executor driver, partition 117, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 117.0 in stage 19.0 (TID 331)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 115.0 in stage 19.0 (TID 330) in 16 ms on localhost (executor driver) (114/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 117-118
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 331 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2f4d91c0
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 331 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2f4d91c0
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 117.0 in stage 19.0 (TID 331). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 118.0 in stage 19.0 (TID 332, localhost, executor driver, partition 118, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 118.0 in stage 19.0 (TID 332)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 117.0 in stage 19.0 (TID 331) in 0 ms on localhost (executor driver) (115/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 118-119
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 332 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5d3721f4
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 332 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5d3721f4
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 118.0 in stage 19.0 (TID 332). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 119.0 in stage 19.0 (TID 333, localhost, executor driver, partition 119, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 119.0 in stage 19.0 (TID 333)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 118.0 in stage 19.0 (TID 332) in 16 ms on localhost (executor driver) (116/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 119-120
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 333 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3050fab9
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 333 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3050fab9
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 119.0 in stage 19.0 (TID 333). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 120.0 in stage 19.0 (TID 334, localhost, executor driver, partition 120, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 120.0 in stage 19.0 (TID 334)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 119.0 in stage 19.0 (TID 333) in 0 ms on localhost (executor driver) (117/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 120-121
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  16 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 334 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@50d8a31e
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 334 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@50d8a31e
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 120.0 in stage 19.0 (TID 334). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 121.0 in stage 19.0 (TID 335, localhost, executor driver, partition 121, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 121.0 in stage 19.0 (TID 335)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 120.0 in stage 19.0 (TID 334) in 16 ms on localhost (executor driver) (118/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 121-122
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 335 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2defbda8
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 335 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2defbda8
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 121.0 in stage 19.0 (TID 335). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 122.0 in stage 19.0 (TID 336, localhost, executor driver, partition 122, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 122.0 in stage 19.0 (TID 336)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 121.0 in stage 19.0 (TID 335) in 0 ms on localhost (executor driver) (119/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 122-123
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 1 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  1 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 336 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@78a4ebb8
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 336 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@78a4ebb8
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 122.0 in stage 19.0 (TID 336). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 123.0 in stage 19.0 (TID 337, localhost, executor driver, partition 123, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 123.0 in stage 19.0 (TID 337)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 122.0 in stage 19.0 (TID 336) in 17 ms on localhost (executor driver) (120/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 123-124
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 337 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1f6d5e10
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 337 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1f6d5e10
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 123.0 in stage 19.0 (TID 337). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 124.0 in stage 19.0 (TID 338, localhost, executor driver, partition 124, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 124.0 in stage 19.0 (TID 338)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 123.0 in stage 19.0 (TID 337) in 0 ms on localhost (executor driver) (121/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 124-125
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 338 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@38e560ca
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 338 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@38e560ca
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 124.0 in stage 19.0 (TID 338). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 125.0 in stage 19.0 (TID 339, localhost, executor driver, partition 125, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 125.0 in stage 19.0 (TID 339)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 124.0 in stage 19.0 (TID 338) in 16 ms on localhost (executor driver) (122/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 125-126
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 339 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1bc31a22
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 339 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1bc31a22
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 125.0 in stage 19.0 (TID 339). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 126.0 in stage 19.0 (TID 340, localhost, executor driver, partition 126, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 126.0 in stage 19.0 (TID 340)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 125.0 in stage 19.0 (TID 339) in 0 ms on localhost (executor driver) (123/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 126-127
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 340 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@63833b21
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 340 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@63833b21
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 126.0 in stage 19.0 (TID 340). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 127.0 in stage 19.0 (TID 341, localhost, executor driver, partition 127, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 127.0 in stage 19.0 (TID 341)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 126.0 in stage 19.0 (TID 340) in 16 ms on localhost (executor driver) (124/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 127-128
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 341 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@43bc8916
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 341 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@43bc8916
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 127.0 in stage 19.0 (TID 341). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 128.0 in stage 19.0 (TID 342, localhost, executor driver, partition 128, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 128.0 in stage 19.0 (TID 342)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 127.0 in stage 19.0 (TID 341) in 16 ms on localhost (executor driver) (125/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 128-129
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 342 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@704d5ff9
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 342 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@704d5ff9
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 128.0 in stage 19.0 (TID 342). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 129.0 in stage 19.0 (TID 343, localhost, executor driver, partition 129, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 129.0 in stage 19.0 (TID 343)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 128.0 in stage 19.0 (TID 342) in 0 ms on localhost (executor driver) (126/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 129-130
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 343 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7452354d
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 343 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7452354d
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 129.0 in stage 19.0 (TID 343). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 130.0 in stage 19.0 (TID 344, localhost, executor driver, partition 130, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 130.0 in stage 19.0 (TID 344)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 129.0 in stage 19.0 (TID 343) in 15 ms on localhost (executor driver) (127/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 130-131
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 344 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@26b76b6
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 344 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@26b76b6
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 130.0 in stage 19.0 (TID 344). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 131.0 in stage 19.0 (TID 345, localhost, executor driver, partition 131, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 131.0 in stage 19.0 (TID 345)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 130.0 in stage 19.0 (TID 344) in 0 ms on localhost (executor driver) (128/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 131-132
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 345 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@36bee3c0
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 345 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@36bee3c0
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 131.0 in stage 19.0 (TID 345). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 132.0 in stage 19.0 (TID 346, localhost, executor driver, partition 132, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 132.0 in stage 19.0 (TID 346)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 131.0 in stage 19.0 (TID 345) in 16 ms on localhost (executor driver) (129/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 132-133
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 346 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2cef3b71
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 346 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2cef3b71
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 132.0 in stage 19.0 (TID 346). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 133.0 in stage 19.0 (TID 347, localhost, executor driver, partition 133, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 133.0 in stage 19.0 (TID 347)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 132.0 in stage 19.0 (TID 346) in 0 ms on localhost (executor driver) (130/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 133-134
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 347 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@256de2e9
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 347 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@256de2e9
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 133.0 in stage 19.0 (TID 347). 4397 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 135.0 in stage 19.0 (TID 348, localhost, executor driver, partition 135, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 135.0 in stage 19.0 (TID 348)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 133.0 in stage 19.0 (TID 347) in 16 ms on localhost (executor driver) (131/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 135-136
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 348 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4fabe408
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 348 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4fabe408
2022-02-10 15:13:43 INFO  Executor:54 - Finished task 135.0 in stage 19.0 (TID 348). 4311 bytes result sent to driver
2022-02-10 15:13:43 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Starting task 136.0 in stage 19.0 (TID 349, localhost, executor driver, partition 136, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:43 INFO  Executor:54 - Running task 136.0 in stage 19.0 (TID 349)
2022-02-10 15:13:43 INFO  TaskSetManager:54 - Finished task 135.0 in stage 19.0 (TID 348) in 0 ms on localhost (executor driver) (132/200)
2022-02-10 15:13:43 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:43 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:43 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 136-137
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:43 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:43 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:224 - Task 349 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@10d7f9d5
2022-02-10 15:13:43 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:43 DEBUG TaskMemoryManager:233 - Task 349 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@10d7f9d5
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 136.0 in stage 19.0 (TID 349). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 137.0 in stage 19.0 (TID 350, localhost, executor driver, partition 137, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 137.0 in stage 19.0 (TID 350)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 136.0 in stage 19.0 (TID 349) in 15 ms on localhost (executor driver) (133/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 137-138
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 350 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6134dce8
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 350 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6134dce8
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 137.0 in stage 19.0 (TID 350). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 138.0 in stage 19.0 (TID 351, localhost, executor driver, partition 138, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 138.0 in stage 19.0 (TID 351)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 137.0 in stage 19.0 (TID 350) in 0 ms on localhost (executor driver) (134/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 138-139
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 351 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1cf249c4
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 351 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1cf249c4
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 138.0 in stage 19.0 (TID 351). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 139.0 in stage 19.0 (TID 352, localhost, executor driver, partition 139, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 139.0 in stage 19.0 (TID 352)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 138.0 in stage 19.0 (TID 351) in 16 ms on localhost (executor driver) (135/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 139-140
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 352 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2066d2a8
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 352 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2066d2a8
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 139.0 in stage 19.0 (TID 352). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 140.0 in stage 19.0 (TID 353, localhost, executor driver, partition 140, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 140.0 in stage 19.0 (TID 353)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 139.0 in stage 19.0 (TID 352) in 0 ms on localhost (executor driver) (136/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 140-141
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 353 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@346eefef
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 353 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@346eefef
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 140.0 in stage 19.0 (TID 353). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 141.0 in stage 19.0 (TID 354, localhost, executor driver, partition 141, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 141.0 in stage 19.0 (TID 354)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 140.0 in stage 19.0 (TID 353) in 16 ms on localhost (executor driver) (137/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 141-142
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 354 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@64a97a78
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 354 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@64a97a78
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 141.0 in stage 19.0 (TID 354). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 142.0 in stage 19.0 (TID 355, localhost, executor driver, partition 142, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 142.0 in stage 19.0 (TID 355)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 141.0 in stage 19.0 (TID 354) in 15 ms on localhost (executor driver) (138/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 142-143
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 355 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6413d6e6
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 355 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6413d6e6
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 142.0 in stage 19.0 (TID 355). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 143.0 in stage 19.0 (TID 356, localhost, executor driver, partition 143, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 143.0 in stage 19.0 (TID 356)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 142.0 in stage 19.0 (TID 355) in 0 ms on localhost (executor driver) (139/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 143-144
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 356 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@359a1eac
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 356 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@359a1eac
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 143.0 in stage 19.0 (TID 356). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 144.0 in stage 19.0 (TID 357, localhost, executor driver, partition 144, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 144.0 in stage 19.0 (TID 357)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 143.0 in stage 19.0 (TID 356) in 16 ms on localhost (executor driver) (140/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 144-145
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 357 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5c9dae69
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 357 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5c9dae69
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 144.0 in stage 19.0 (TID 357). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 145.0 in stage 19.0 (TID 358, localhost, executor driver, partition 145, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 145.0 in stage 19.0 (TID 358)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 144.0 in stage 19.0 (TID 357) in 0 ms on localhost (executor driver) (141/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 145-146
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 358 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5bfc1030
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 358 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5bfc1030
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 145.0 in stage 19.0 (TID 358). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 146.0 in stage 19.0 (TID 359, localhost, executor driver, partition 146, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 146.0 in stage 19.0 (TID 359)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 145.0 in stage 19.0 (TID 358) in 16 ms on localhost (executor driver) (142/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 146-147
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 359 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@37093a87
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 359 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@37093a87
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 146.0 in stage 19.0 (TID 359). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 147.0 in stage 19.0 (TID 360, localhost, executor driver, partition 147, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 147.0 in stage 19.0 (TID 360)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 146.0 in stage 19.0 (TID 359) in 16 ms on localhost (executor driver) (143/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 147-148
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 360 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@36f65d0e
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 360 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@36f65d0e
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 147.0 in stage 19.0 (TID 360). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 148.0 in stage 19.0 (TID 361, localhost, executor driver, partition 148, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 148.0 in stage 19.0 (TID 361)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 147.0 in stage 19.0 (TID 360) in 0 ms on localhost (executor driver) (144/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 148-149
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 1 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  1 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 361 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@21eb00f3
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 361 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@21eb00f3
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 148.0 in stage 19.0 (TID 361). 4440 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 149.0 in stage 19.0 (TID 362, localhost, executor driver, partition 149, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 149.0 in stage 19.0 (TID 362)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 148.0 in stage 19.0 (TID 361) in 16 ms on localhost (executor driver) (145/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 149-150
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 362 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@16e4962b
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 362 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@16e4962b
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 149.0 in stage 19.0 (TID 362). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 150.0 in stage 19.0 (TID 363, localhost, executor driver, partition 150, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 150.0 in stage 19.0 (TID 363)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 149.0 in stage 19.0 (TID 362) in 0 ms on localhost (executor driver) (146/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 150-151
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 363 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7b86618f
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 363 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7b86618f
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 150.0 in stage 19.0 (TID 363). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 151.0 in stage 19.0 (TID 364, localhost, executor driver, partition 151, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 151.0 in stage 19.0 (TID 364)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 150.0 in stage 19.0 (TID 363) in 16 ms on localhost (executor driver) (147/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 151-152
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 364 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@391d2ad1
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 364 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@391d2ad1
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 151.0 in stage 19.0 (TID 364). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 152.0 in stage 19.0 (TID 365, localhost, executor driver, partition 152, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 152.0 in stage 19.0 (TID 365)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 151.0 in stage 19.0 (TID 364) in 16 ms on localhost (executor driver) (148/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 152-153
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 365 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2453f600
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 365 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2453f600
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 152.0 in stage 19.0 (TID 365). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 153.0 in stage 19.0 (TID 366, localhost, executor driver, partition 153, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 153.0 in stage 19.0 (TID 366)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 152.0 in stage 19.0 (TID 365) in 0 ms on localhost (executor driver) (149/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 153-154
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 366 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6e45217f
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 366 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6e45217f
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 153.0 in stage 19.0 (TID 366). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 154.0 in stage 19.0 (TID 367, localhost, executor driver, partition 154, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 154.0 in stage 19.0 (TID 367)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 153.0 in stage 19.0 (TID 366) in 16 ms on localhost (executor driver) (150/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 154-155
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 367 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2d8cbc10
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 367 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2d8cbc10
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 154.0 in stage 19.0 (TID 367). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 155.0 in stage 19.0 (TID 368, localhost, executor driver, partition 155, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 155.0 in stage 19.0 (TID 368)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 154.0 in stage 19.0 (TID 367) in 16 ms on localhost (executor driver) (151/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 155-156
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 368 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4c7ddb5c
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 368 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4c7ddb5c
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 155.0 in stage 19.0 (TID 368). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 157.0 in stage 19.0 (TID 369, localhost, executor driver, partition 157, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 157.0 in stage 19.0 (TID 369)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 155.0 in stage 19.0 (TID 368) in 0 ms on localhost (executor driver) (152/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 157-158
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 369 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6466b1e8
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 369 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6466b1e8
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 157.0 in stage 19.0 (TID 369). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 158.0 in stage 19.0 (TID 370, localhost, executor driver, partition 158, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 158.0 in stage 19.0 (TID 370)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 157.0 in stage 19.0 (TID 369) in 15 ms on localhost (executor driver) (153/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 158-159
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 370 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30bc7de0
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 370 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30bc7de0
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 158.0 in stage 19.0 (TID 370). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 159.0 in stage 19.0 (TID 371, localhost, executor driver, partition 159, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 159.0 in stage 19.0 (TID 371)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 158.0 in stage 19.0 (TID 370) in 0 ms on localhost (executor driver) (154/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 159-160
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 371 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@46ce9022
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 371 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@46ce9022
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 159.0 in stage 19.0 (TID 371). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 160.0 in stage 19.0 (TID 372, localhost, executor driver, partition 160, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 160.0 in stage 19.0 (TID 372)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 159.0 in stage 19.0 (TID 371) in 16 ms on localhost (executor driver) (155/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 160-161
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 372 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@144a513e
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 372 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@144a513e
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 160.0 in stage 19.0 (TID 372). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 161.0 in stage 19.0 (TID 373, localhost, executor driver, partition 161, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 161.0 in stage 19.0 (TID 373)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 160.0 in stage 19.0 (TID 372) in 0 ms on localhost (executor driver) (156/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 161-162
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 373 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@f783e38
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 373 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@f783e38
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 161.0 in stage 19.0 (TID 373). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 162.0 in stage 19.0 (TID 374, localhost, executor driver, partition 162, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 162.0 in stage 19.0 (TID 374)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 161.0 in stage 19.0 (TID 373) in 16 ms on localhost (executor driver) (157/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 162-163
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 374 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@77bd574e
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 374 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@77bd574e
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 162.0 in stage 19.0 (TID 374). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 163.0 in stage 19.0 (TID 375, localhost, executor driver, partition 163, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 162.0 in stage 19.0 (TID 374) in 0 ms on localhost (executor driver) (158/200)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 163.0 in stage 19.0 (TID 375)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 163-164
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 375 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7f002f89
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 375 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7f002f89
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 163.0 in stage 19.0 (TID 375). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 164.0 in stage 19.0 (TID 376, localhost, executor driver, partition 164, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 164.0 in stage 19.0 (TID 376)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 163.0 in stage 19.0 (TID 375) in 16 ms on localhost (executor driver) (159/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 164-165
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 376 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@74a66273
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 376 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@74a66273
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 164.0 in stage 19.0 (TID 376). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 165.0 in stage 19.0 (TID 377, localhost, executor driver, partition 165, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 165.0 in stage 19.0 (TID 377)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 164.0 in stage 19.0 (TID 376) in 0 ms on localhost (executor driver) (160/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 165-166
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 377 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2f0f8bb6
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 377 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2f0f8bb6
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 165.0 in stage 19.0 (TID 377). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 166.0 in stage 19.0 (TID 378, localhost, executor driver, partition 166, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 166.0 in stage 19.0 (TID 378)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 165.0 in stage 19.0 (TID 377) in 16 ms on localhost (executor driver) (161/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 166-167
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 378 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@633ea0a3
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 378 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@633ea0a3
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 166.0 in stage 19.0 (TID 378). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 167.0 in stage 19.0 (TID 379, localhost, executor driver, partition 167, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 167.0 in stage 19.0 (TID 379)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 166.0 in stage 19.0 (TID 378) in 0 ms on localhost (executor driver) (162/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 167-168
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 379 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6c466649
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 379 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6c466649
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 167.0 in stage 19.0 (TID 379). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 168.0 in stage 19.0 (TID 380, localhost, executor driver, partition 168, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 168.0 in stage 19.0 (TID 380)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 167.0 in stage 19.0 (TID 379) in 15 ms on localhost (executor driver) (163/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 168-169
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 380 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@73fb0b8f
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 380 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@73fb0b8f
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 168.0 in stage 19.0 (TID 380). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 169.0 in stage 19.0 (TID 381, localhost, executor driver, partition 169, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 169.0 in stage 19.0 (TID 381)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 168.0 in stage 19.0 (TID 380) in 0 ms on localhost (executor driver) (164/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 169-170
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 381 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@15f9fbc5
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 381 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@15f9fbc5
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 169.0 in stage 19.0 (TID 381). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 170.0 in stage 19.0 (TID 382, localhost, executor driver, partition 170, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 170.0 in stage 19.0 (TID 382)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 169.0 in stage 19.0 (TID 381) in 16 ms on localhost (executor driver) (165/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 170-171
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 382 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7d5de327
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 382 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7d5de327
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 170.0 in stage 19.0 (TID 382). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 171.0 in stage 19.0 (TID 383, localhost, executor driver, partition 171, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 171.0 in stage 19.0 (TID 383)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 170.0 in stage 19.0 (TID 382) in 0 ms on localhost (executor driver) (166/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 171-172
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 383 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@64da7ac6
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 383 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@64da7ac6
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 171.0 in stage 19.0 (TID 383). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 172.0 in stage 19.0 (TID 384, localhost, executor driver, partition 172, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 171.0 in stage 19.0 (TID 383) in 16 ms on localhost (executor driver) (167/200)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 172.0 in stage 19.0 (TID 384)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 172-173
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 384 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@41b02667
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 384 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@41b02667
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 172.0 in stage 19.0 (TID 384). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 173.0 in stage 19.0 (TID 385, localhost, executor driver, partition 173, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 173.0 in stage 19.0 (TID 385)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 172.0 in stage 19.0 (TID 384) in 16 ms on localhost (executor driver) (168/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 173-174
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 385 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6e1933cc
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 385 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6e1933cc
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 173.0 in stage 19.0 (TID 385). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 174.0 in stage 19.0 (TID 386, localhost, executor driver, partition 174, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 174.0 in stage 19.0 (TID 386)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 173.0 in stage 19.0 (TID 385) in 0 ms on localhost (executor driver) (169/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 174-175
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 386 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5949949a
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 386 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5949949a
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 174.0 in stage 19.0 (TID 386). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 175.0 in stage 19.0 (TID 387, localhost, executor driver, partition 175, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 175.0 in stage 19.0 (TID 387)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 174.0 in stage 19.0 (TID 386) in 16 ms on localhost (executor driver) (170/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 175-176
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 387 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@610e6bdb
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 387 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@610e6bdb
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 175.0 in stage 19.0 (TID 387). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 176.0 in stage 19.0 (TID 388, localhost, executor driver, partition 176, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 176.0 in stage 19.0 (TID 388)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 175.0 in stage 19.0 (TID 387) in 0 ms on localhost (executor driver) (171/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 176-177
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 388 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4122cf22
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 388 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4122cf22
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 176.0 in stage 19.0 (TID 388). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 177.0 in stage 19.0 (TID 389, localhost, executor driver, partition 177, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 177.0 in stage 19.0 (TID 389)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 176.0 in stage 19.0 (TID 388) in 16 ms on localhost (executor driver) (172/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 177-178
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 389 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6cfcf775
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 389 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6cfcf775
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 177.0 in stage 19.0 (TID 389). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 178.0 in stage 19.0 (TID 390, localhost, executor driver, partition 178, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 178.0 in stage 19.0 (TID 390)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 177.0 in stage 19.0 (TID 389) in 16 ms on localhost (executor driver) (173/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 178-179
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 390 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@f62459f
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 390 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@f62459f
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 178.0 in stage 19.0 (TID 390). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 179.0 in stage 19.0 (TID 391, localhost, executor driver, partition 179, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 179.0 in stage 19.0 (TID 391)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 178.0 in stage 19.0 (TID 390) in 0 ms on localhost (executor driver) (174/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 179-180
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 391 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@51a1d07b
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 391 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@51a1d07b
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 179.0 in stage 19.0 (TID 391). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 180.0 in stage 19.0 (TID 392, localhost, executor driver, partition 180, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 180.0 in stage 19.0 (TID 392)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 179.0 in stage 19.0 (TID 391) in 15 ms on localhost (executor driver) (175/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 180-181
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 392 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6d06b8e0
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 392 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6d06b8e0
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 180.0 in stage 19.0 (TID 392). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 181.0 in stage 19.0 (TID 393, localhost, executor driver, partition 181, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 181.0 in stage 19.0 (TID 393)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 180.0 in stage 19.0 (TID 392) in 0 ms on localhost (executor driver) (176/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 181-182
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 393 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@58dc7ae1
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 393 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@58dc7ae1
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 181.0 in stage 19.0 (TID 393). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 182.0 in stage 19.0 (TID 394, localhost, executor driver, partition 182, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 182.0 in stage 19.0 (TID 394)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 181.0 in stage 19.0 (TID 393) in 16 ms on localhost (executor driver) (177/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 182-183
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 394 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6b716f0f
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 394 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6b716f0f
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 182.0 in stage 19.0 (TID 394). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 183.0 in stage 19.0 (TID 395, localhost, executor driver, partition 183, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 183.0 in stage 19.0 (TID 395)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 182.0 in stage 19.0 (TID 394) in 0 ms on localhost (executor driver) (178/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 183-184
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 395 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1481cf99
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 395 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1481cf99
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 183.0 in stage 19.0 (TID 395). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 184.0 in stage 19.0 (TID 396, localhost, executor driver, partition 184, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 184.0 in stage 19.0 (TID 396)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 183.0 in stage 19.0 (TID 395) in 16 ms on localhost (executor driver) (179/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 184-185
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 396 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@68a09e90
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 396 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@68a09e90
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 184.0 in stage 19.0 (TID 396). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 185.0 in stage 19.0 (TID 397, localhost, executor driver, partition 185, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 185.0 in stage 19.0 (TID 397)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 184.0 in stage 19.0 (TID 396) in 0 ms on localhost (executor driver) (180/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 185-186
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 397 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7a6851f4
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 397 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7a6851f4
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 185.0 in stage 19.0 (TID 397). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 186.0 in stage 19.0 (TID 398, localhost, executor driver, partition 186, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 186.0 in stage 19.0 (TID 398)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 185.0 in stage 19.0 (TID 397) in 15 ms on localhost (executor driver) (181/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 186-187
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 398 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1ccac2ba
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 398 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1ccac2ba
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 186.0 in stage 19.0 (TID 398). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 187.0 in stage 19.0 (TID 399, localhost, executor driver, partition 187, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 187.0 in stage 19.0 (TID 399)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 186.0 in stage 19.0 (TID 398) in 0 ms on localhost (executor driver) (182/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 187-188
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 399 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@745e4a41
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 399 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@745e4a41
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 187.0 in stage 19.0 (TID 399). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 188.0 in stage 19.0 (TID 400, localhost, executor driver, partition 188, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 188.0 in stage 19.0 (TID 400)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 187.0 in stage 19.0 (TID 399) in 16 ms on localhost (executor driver) (183/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 188-189
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 400 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@41353af8
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 400 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@41353af8
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 188.0 in stage 19.0 (TID 400). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 189.0 in stage 19.0 (TID 401, localhost, executor driver, partition 189, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 189.0 in stage 19.0 (TID 401)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 188.0 in stage 19.0 (TID 400) in 0 ms on localhost (executor driver) (184/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 189-190
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 401 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1df7ed1a
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 401 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1df7ed1a
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 189.0 in stage 19.0 (TID 401). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 190.0 in stage 19.0 (TID 402, localhost, executor driver, partition 190, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 190.0 in stage 19.0 (TID 402)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 189.0 in stage 19.0 (TID 401) in 16 ms on localhost (executor driver) (185/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 190-191
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 402 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5b71d00e
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 402 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5b71d00e
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 190.0 in stage 19.0 (TID 402). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 191.0 in stage 19.0 (TID 403, localhost, executor driver, partition 191, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 191.0 in stage 19.0 (TID 403)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 190.0 in stage 19.0 (TID 402) in 16 ms on localhost (executor driver) (186/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 191-192
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 403 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@c2f257a
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 403 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@c2f257a
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 191.0 in stage 19.0 (TID 403). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 192.0 in stage 19.0 (TID 404, localhost, executor driver, partition 192, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 192.0 in stage 19.0 (TID 404)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 191.0 in stage 19.0 (TID 403) in 16 ms on localhost (executor driver) (187/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 192-193
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 404 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7ff425a3
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 404 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7ff425a3
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 192.0 in stage 19.0 (TID 404). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 193.0 in stage 19.0 (TID 405, localhost, executor driver, partition 193, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 193.0 in stage 19.0 (TID 405)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 192.0 in stage 19.0 (TID 404) in 16 ms on localhost (executor driver) (188/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 193-194
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 405 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@364ea1c7
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 405 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@364ea1c7
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 193.0 in stage 19.0 (TID 405). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 194.0 in stage 19.0 (TID 406, localhost, executor driver, partition 194, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 194.0 in stage 19.0 (TID 406)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 193.0 in stage 19.0 (TID 405) in 0 ms on localhost (executor driver) (189/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 194-195
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 406 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@57672f7
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 406 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@57672f7
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 194.0 in stage 19.0 (TID 406). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 195.0 in stage 19.0 (TID 407, localhost, executor driver, partition 195, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 194.0 in stage 19.0 (TID 406) in 15 ms on localhost (executor driver) (190/200)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 195.0 in stage 19.0 (TID 407)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 195-196
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 407 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@292ad580
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 407 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@292ad580
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 195.0 in stage 19.0 (TID 407). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 196.0 in stage 19.0 (TID 408, localhost, executor driver, partition 196, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 196.0 in stage 19.0 (TID 408)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 195.0 in stage 19.0 (TID 407) in 16 ms on localhost (executor driver) (191/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 196-197
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 408 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1f6f5918
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 408 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1f6f5918
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 196.0 in stage 19.0 (TID 408). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 198.0 in stage 19.0 (TID 409, localhost, executor driver, partition 198, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 198.0 in stage 19.0 (TID 409)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 196.0 in stage 19.0 (TID 408) in 0 ms on localhost (executor driver) (192/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 198-199
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 409 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1dcf45ff
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 409 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1dcf45ff
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 198.0 in stage 19.0 (TID 409). 4311 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 199.0 in stage 19.0 (TID 410, localhost, executor driver, partition 199, PROCESS_LOCAL, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 199.0 in stage 19.0 (TID 410)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 198.0 in stage 19.0 (TID 409) in 0 ms on localhost (executor driver) (193/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 199-200
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 410 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@56ea24f2
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 410 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@56ea24f2
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 199.0 in stage 19.0 (TID 410). 4397 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 97.0 in stage 19.0 (TID 411, localhost, executor driver, partition 97, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 199.0 in stage 19.0 (TID 410) in 16 ms on localhost (executor driver) (194/200)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 97.0 in stage 19.0 (TID 411)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 97-98
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_97
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 411 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@57f245de
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 411 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@57f245de
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 411 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@57f245de
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 411 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@57f245de
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 97.0 in stage 19.0 (TID 411). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 108.0 in stage 19.0 (TID 412, localhost, executor driver, partition 108, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 108.0 in stage 19.0 (TID 412)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 97.0 in stage 19.0 (TID 411) in 16 ms on localhost (executor driver) (195/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 108-109
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_108
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 412 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5e2536f3
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 412 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@5e2536f3
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 412 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5e2536f3
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 412 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@5e2536f3
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 108.0 in stage 19.0 (TID 412). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 116.0 in stage 19.0 (TID 413, localhost, executor driver, partition 116, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 116.0 in stage 19.0 (TID 413)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 108.0 in stage 19.0 (TID 412) in 16 ms on localhost (executor driver) (196/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 116-117
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_116
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 413 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6efbfa29
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 413 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@6efbfa29
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 413 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6efbfa29
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 413 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@6efbfa29
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 116.0 in stage 19.0 (TID 413). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 134.0 in stage 19.0 (TID 414, localhost, executor driver, partition 134, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 134.0 in stage 19.0 (TID 414)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 116.0 in stage 19.0 (TID 413) in 16 ms on localhost (executor driver) (197/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 134-135
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_134
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 414 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7159f558
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 414 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@7159f558
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 414 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7159f558
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 414 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@7159f558
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 134.0 in stage 19.0 (TID 414). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 156.0 in stage 19.0 (TID 415, localhost, executor driver, partition 156, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 156.0 in stage 19.0 (TID 415)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 134.0 in stage 19.0 (TID 414) in 16 ms on localhost (executor driver) (198/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 156-157
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_156
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 415 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5b3f0bd3
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 415 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@5b3f0bd3
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 415 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5b3f0bd3
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 415 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@5b3f0bd3
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 156.0 in stage 19.0 (TID 415). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 197.0 in stage 19.0 (TID 416, localhost, executor driver, partition 197, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 197.0 in stage 19.0 (TID 416)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 156.0 in stage 19.0 (TID 415) in 16 ms on localhost (executor driver) (199/200)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 197-198
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_2_0_197
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     UTF8String value_1 = isNull_1 ?
/* 042 */     null : (i.getUTF8String(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */     return (mutableStateArray_0[0].getRow());
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 416 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5deaedba
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 416 acquired 64.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@5deaedba
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 416 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5deaedba
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 416 release 64.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@5deaedba
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 197.0 in stage 19.0 (TID 416). 4526 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 197.0 in stage 19.0 (TID 416) in 15 ms on localhost (executor driver) (200/200)
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 INFO  DAGScheduler:54 - ShuffleMapStage 19 (count at UseCase5.java:65) finished in 1.885 s
2022-02-10 15:13:44 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:44 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - waiting: Set(ShuffleMapStage 20, ResultStage 21)
2022-02-10 15:13:44 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 4
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 20 (name=count at UseCase5.java:65;jobs=15))
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 20 (MapPartitionsRDD[79] at count at UseCase5.java:65), which has no missing parents
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 20)
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_36 stored as values in memory (estimated size 32.7 KB, free 1967.1 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_36 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_36 without replication took  0 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_36_piece0 stored as bytes in memory (estimated size 15.3 KB, free 1967.1 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_36_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 15.3 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_36_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_36_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_36_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_36_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 36 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting 6 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[79] at count at UseCase5.java:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Adding task set 20.0 with 6 tasks
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Epoch for TaskSet 20.0: 4
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 20.0: ANY
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 20.0 (TID 417, localhost, executor driver, partition 0, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 0.0 in stage 20.0 (TID 417)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_36
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_36 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 0-1
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_134_0
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 INFO  CodeGenerator:54 - Code generated in 7.655 ms
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 417 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7bcf39d
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 417 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7bcf39d
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 417 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7bcf39d
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 417 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7bcf39d
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 0.0 in stage 20.0 (TID 417). 5497 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 1.0 in stage 20.0 (TID 418, localhost, executor driver, partition 1, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 1.0 in stage 20.0 (TID 418)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 20.0 (TID 417) in 65 ms on localhost (executor driver) (1/6)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 1-2
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_97_1
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 418 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@401ecab0
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 418 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@401ecab0
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 418 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@401ecab0
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 418 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@401ecab0
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 1.0 in stage 20.0 (TID 418). 5411 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 2.0 in stage 20.0 (TID 419, localhost, executor driver, partition 2, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 2.0 in stage 20.0 (TID 419)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 1.0 in stage 20.0 (TID 418) in 16 ms on localhost (executor driver) (2/6)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 2-3
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_156_2
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 419 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@188939c4
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 419 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@188939c4
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 419 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@188939c4
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 419 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@188939c4
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 2.0 in stage 20.0 (TID 419). 5411 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 3.0 in stage 20.0 (TID 420, localhost, executor driver, partition 3, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 3.0 in stage 20.0 (TID 420)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 2.0 in stage 20.0 (TID 419) in 16 ms on localhost (executor driver) (3/6)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 3-4
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_197_3
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 420 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@690c1084
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 420 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@690c1084
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 420 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@690c1084
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 420 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@690c1084
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 3.0 in stage 20.0 (TID 420). 5411 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 4.0 in stage 20.0 (TID 421, localhost, executor driver, partition 4, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 4.0 in stage 20.0 (TID 421)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 3.0 in stage 20.0 (TID 420) in 16 ms on localhost (executor driver) (4/6)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 4-5
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_116_4
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 421 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5974a97e
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 421 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5974a97e
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 421 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5974a97e
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 421 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5974a97e
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 4.0 in stage 20.0 (TID 421). 5411 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 5.0 in stage 20.0 (TID 422, localhost, executor driver, partition 5, ANY, 7756 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 5.0 in stage 20.0 (TID 422)
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 4.0 in stage 20.0 (TID 421) in 16 ms on localhost (executor driver) (5/6)
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 3, partitions 5-6
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_3_108_5
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 422 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3c399598
2022-02-10 15:13:44 DEBUG TaskMemoryManager:224 - Task 422 acquired 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3c399598
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 422 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3c399598
2022-02-10 15:13:44 DEBUG TaskMemoryManager:233 - Task 422 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3c399598
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 5.0 in stage 20.0 (TID 422). 5411 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 5.0 in stage 20.0 (TID 422) in 15 ms on localhost (executor driver) (6/6)
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:44 INFO  DAGScheduler:54 - ShuffleMapStage 20 (count at UseCase5.java:65) finished in 0.144 s
2022-02-10 15:13:44 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:44 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - waiting: Set(ResultStage 21)
2022-02-10 15:13:44 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 5
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitStage(ResultStage 21 (name=count at UseCase5.java:65;jobs=15))
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting ResultStage 21 (MapPartitionsRDD[82] at count at UseCase5.java:65), which has no missing parents
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 21)
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_37 stored as values in memory (estimated size 7.3 KB, free 1967.1 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_37 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_37 without replication took  0 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1967.1 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_37_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_37_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_37_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_37_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_37_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 37 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[82] at count at UseCase5.java:65) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Adding task set 21.0 with 1 tasks
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Epoch for TaskSet 21.0: 5
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 21.0: ANY
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_21.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 21.0 (TID 423, localhost, executor driver, partition 0, ANY, 7767 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 0.0 in stage 21.0 (TID 423)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_37
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_37 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 4, partitions 0-1
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks
2022-02-10 15:13:44 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_4_0_0, shuffle_4_1_0, shuffle_4_2_0, shuffle_4_3_0, shuffle_4_4_0, shuffle_4_5_0
2022-02-10 15:13:44 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 0.0 in stage 21.0 (TID 423). 1610 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_21.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 21.0 (TID 423) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2022-02-10 15:13:44 INFO  DAGScheduler:54 - ResultStage 21 (count at UseCase5.java:65) finished in 0.016 s
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 20, remaining stages = 3
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 19, remaining stages = 2
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 18, remaining stages = 1
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 21, remaining stages = 0
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Job 15 finished: count at UseCase5.java:65, took 2.061207 s
2022-02-10 15:13:44 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 15:13:44 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 15:13:44 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 15:13:44 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#186
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#186]
 +- Relation[value#186] text                +- Relation[value#186] text
          
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#190: java.lang.String   DeserializeToObject cast(value#186 as string).toString, obj#190: java.lang.String
 +- LocalRelation <empty>, [value#186]                                                                                                                                      +- LocalRelation <empty>, [value#186]
          
2022-02-10 15:13:44 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#186
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#186, None)) > 0)
 +- Project [value#186]                     +- Project [value#186]
    +- Relation[value#186] text                +- Relation[value#186] text
          
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#191: java.lang.String   DeserializeToObject cast(value#186 as string).toString, obj#191: java.lang.String
 +- LocalRelation <empty>, [value#186]                                                                                                                                      +- LocalRelation <empty>, [value#186]
          
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#192: java.lang.String   DeserializeToObject cast(value#186 as string).toString, obj#192: java.lang.String
 +- LocalRelation <empty>, [value#186]                                                                                                                                      +- LocalRelation <empty>, [value#186]
          
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#186, None)) > 0)      +- Filter (length(trim(value#186, None)) > 0)
!      +- Project [value#186]                             +- Relation[value#186] text
!         +- Relation[value#186] text               
          
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#186, None)) > 0)
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:44 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:44 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_38 stored as values in memory (estimated size 221.9 KB, free 1966.9 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_38 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_38 without replication took  0 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_38_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1966.9 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_38_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_38_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_38_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_38_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_38_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 38 from load at UseCase5.java:20
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8388788 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:44 INFO  SparkContext:54 - Starting job: load at UseCase5.java:20
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Got job 16 (load at UseCase5.java:20) with 1 output partitions
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Final stage: ResultStage 22 (load at UseCase5.java:20)
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitStage(ResultStage 22 (name=load at UseCase5.java:20;jobs=16))
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting ResultStage 22 (MapPartitionsRDD[86] at load at UseCase5.java:20), which has no missing parents
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 22)
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_39 stored as values in memory (estimated size 8.9 KB, free 1966.9 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_39 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_39 without replication took  0 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1966.9 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_39_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 4.6 KB, free: 1970.2 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_39_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_39_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_39_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_39_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 39 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[86] at load at UseCase5.java:20) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Adding task set 22.0 with 1 tasks
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Epoch for TaskSet 22.0: 5
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 22.0: NO_PREF, ANY
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_22.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 22.0 (TID 424, localhost, executor driver, partition 0, PROCESS_LOCAL, 8355 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 0.0 in stage 22.0 (TID 424)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_39
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_39 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_38
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_38 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 0.0 in stage 22.0 (TID 424). 1111 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_22.0, runningTasks: 0
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 22.0 (TID 424) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2022-02-10 15:13:44 INFO  DAGScheduler:54 - ResultStage 22 (load at UseCase5.java:20) finished in 0.000 s
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 22, remaining stages = 0
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Job 16 finished: load at UseCase5.java:20, took 0.010288 s
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#194: java.lang.String   DeserializeToObject cast(value#186 as string).toString, obj#194: java.lang.String
 +- Project [value#186]                                                                                                                                                     +- Project [value#186]
    +- Relation[value#186] text                                                                                                                                                +- Relation[value#186] text
          
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#186 as string).toString, obj#194: java.lang.String   DeserializeToObject value#186.toString, obj#194: java.lang.String
!+- Project [value#186]                                                              +- Relation[value#186] text
!   +- Relation[value#186] text                                                      
          
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:44 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(673)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 673
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 673
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(669)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 669
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 669
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(680)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 680
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 680
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(666)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 666
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 666
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(619)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 619
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 619
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(486)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 486
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 486
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(35)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 35
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 35
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 35
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_40 stored as values in memory (estimated size 221.9 KB, free 1966.6 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 35
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_35_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_40 locally took  11 ms
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_35_piece0 of size 15036 dropped from memory (free 2062182812)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_40 without replication took  11 ms
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_35_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 14.7 KB, free: 1970.2 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_35_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_35_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_35
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_35 of size 32424 dropped from memory (free 2062215236)
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 35, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 35
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(638)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 638
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 638
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(452)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 452
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 452
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(694)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 694
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 694
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(620)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 620
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 620
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(623)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 623
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 623
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(693)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 693
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 693
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(27)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 27
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 27
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 27
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 27
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_27
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_27 of size 227072 dropped from memory (free 2062442308)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_27_piece0
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_27_piece0 of size 21155 dropped from memory (free 2062463463)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_27_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 27, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 27
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(698)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 698
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 698
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(633)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 633
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 633
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(471)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 471
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 471
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(500)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 500
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 500
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(624)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 624
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 624
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(621)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 621
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 621
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(454)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 454
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 454
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(463)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 463
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 463
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(697)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 697
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 697
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(648)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 648
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 648
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(38)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 38
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 38
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_40_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1966.9 MB)
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 38
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 38
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_38
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_38 of size 227232 dropped from memory (free 2062669525)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_40_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_38_piece0
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_40_piece0
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_38_piece0 of size 21170 dropped from memory (free 2062690695)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_40_piece0
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_38_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_40_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_38_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_38_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_40_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 40 from load at UseCase5.java:20
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 38, response is 0
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 8388788 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 38
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(451)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 451
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 451
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(495)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 495
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 495
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(445)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 445
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 445
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(628)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 628
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 628
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(661)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 661
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 661
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(457)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 457
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 457
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(488)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 488
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 488
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(615)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 615
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 615
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(706)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 706
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 706
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(678)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 678
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 678
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(26)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 26
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 26
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 26
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 26
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_26
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_26 of size 227072 dropped from memory (free 2062917767)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_26_piece0
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_26_piece0 of size 21155 dropped from memory (free 2062938922)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_26_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 26, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 26
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(481)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 481
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 481
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(652)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 652
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 652
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(494)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 494
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 494
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(631)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 631
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 631
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(689)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 689
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 689
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(612)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 612
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 612
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(455)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 455
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 455
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(37)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 37
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 37
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 37
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 37
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_37_piece0
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_37_piece0 of size 4015 dropped from memory (free 2062942937)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_37_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_37_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_37_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_37
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_37 of size 7432 dropped from memory (free 2062950369)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 37, response is 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 37
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(496)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 496
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 496
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(603)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 603
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 603
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(32)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 32
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 32
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 32
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 32
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_32_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_32_piece0 of size 21155 dropped from memory (free 2062971524)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_32_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_32_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_32
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_32 of size 227072 dropped from memory (free 2063198596)
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 32, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 32
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(668)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 668
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 668
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(3)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning shuffle 3
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned shuffle 3
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(472)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 472
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 472
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(687)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 687
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 687
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(497)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 497
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 497
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 3
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(39)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 39
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 39
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 39
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 39
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_39
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_39 of size 9144 dropped from memory (free 2063207740)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_39_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_39_piece0 of size 4743 dropped from memory (free 2063212483)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_39_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_39_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_39_piece0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 39, response is 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 39
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(655)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 655
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 655
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(460)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 460
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 460
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(632)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 632
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 632
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(656)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 656
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 656
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(676)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 676
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 676
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(461)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 461
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 461
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(637)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 637
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 637
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(700)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 700
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 700
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(617)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 617
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 617
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(685)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 685
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 685
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(704)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 704
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 704
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(705)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 705
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 705
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(613)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 613
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 613
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(658)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 658
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 658
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(629)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 629
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 629
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(473)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 473
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 473
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(465)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 465
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 465
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(30)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 30
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 30
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 30
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 30
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_30_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_30_piece0 of size 357 dropped from memory (free 2063212840)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_30_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 357.0 B, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_30_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_30
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_30 of size 1048648 dropped from memory (free 2064261488)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 30, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 30
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(702)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 702
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 702
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(448)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 448
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 448
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(657)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 657
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 657
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(649)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 649
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 649
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(659)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 659
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 659
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(695)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 695
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 695
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(640)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 640
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 640
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(696)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 696
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 696
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(699)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 699
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 699
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(688)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 688
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 688
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(482)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 482
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 482
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(608)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 608
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 608
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(664)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 664
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 664
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(703)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 703
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 703
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(647)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 647
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 647
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(490)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 490
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 490
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(491)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 491
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 491
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(627)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 627
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 627
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(707)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 707
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 707
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(468)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 468
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 468
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(605)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 605
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 605
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(475)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 475
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 475
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(691)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 691
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 691
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(604)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 604
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 604
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(671)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 671
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 671
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(686)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 686
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 686
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(487)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 487
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 487
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(456)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 456
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 456
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(489)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 489
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 489
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(634)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 634
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 634
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(690)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 690
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 690
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(654)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 654
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 654
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(650)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 650
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 650
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(611)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 611
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 611
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(630)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 630
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 630
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(677)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 677
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 677
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(444)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 444
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 444
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(682)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 682
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 682
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(470)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 470
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 470
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(485)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 485
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 485
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(484)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 484
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 484
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(498)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 498
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 498
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(635)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 635
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 635
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(478)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 478
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 478
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(672)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 672
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 672
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(653)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 653
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 653
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(665)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 665
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 665
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(622)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 622
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 622
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(499)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 499
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 499
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(458)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 458
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 458
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(662)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 662
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 662
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(641)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 641
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 641
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(606)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 606
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 606
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(446)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 446
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 446
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(609)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 609
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 609
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(651)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 651
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 651
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(493)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 493
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 493
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(477)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 477
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 477
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(476)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 476
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 476
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(36)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 36
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 36
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 36
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 36
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_36_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_36_piece0 of size 15639 dropped from memory (free 2064277127)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_36_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 15.3 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_36_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_36_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_36
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_36 of size 33456 dropped from memory (free 2064310583)
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 36, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 36
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(692)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 692
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 692
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(674)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 674
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 674
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(479)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 479
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 479
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(502)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 502
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 502
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(4)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning shuffle 4
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned shuffle 4
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(501)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 501
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 501
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(636)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 636
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 636
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(616)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 616
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 616
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(459)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 459
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 459
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(610)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 610
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 610
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(449)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 449
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 449
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(646)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 646
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 646
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(450)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 450
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 450
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(708)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 708
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 708
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 4
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(2)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning shuffle 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned shuffle 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(643)
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 2
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 643
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 643
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(675)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 675
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 675
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(469)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 469
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 469
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(31)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning broadcast 31
2022-02-10 15:13:44 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 31
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 31
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing broadcast 31
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_31
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_31 of size 1049064 dropped from memory (free 2065359647)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG BlockManager:58 - Removing block broadcast_31_piece0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 15:13:44 DEBUG MemoryStore:58 - Block broadcast_31_piece0 of size 9297 dropped from memory (free 2065368944)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Removed broadcast_31_piece0 on Clairvoyant-324.mshome.net:58899 in memory (size: 9.1 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_31_piece0
2022-02-10 15:13:44 INFO  SparkContext:54 - Starting job: load at UseCase5.java:20
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 31, response is 0
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaned broadcast 31
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(679)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 679
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 679
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(467)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 467
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 467
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(701)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 701
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 701
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(645)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 645
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 645
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(626)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 626
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 626
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(453)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 453
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 453
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(618)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 618
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 618
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(667)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 667
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 667
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(462)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 462
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 462
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(480)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 480
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 480
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(483)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 483
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 483
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(684)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 684
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 684
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(464)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 464
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 464
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(474)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 474
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 474
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(642)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 642
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 642
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Got job 17 (load at UseCase5.java:20) with 1 output partitions
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(607)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 607
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Final stage: ResultStage 23 (load at UseCase5.java:20)
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 607
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(660)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 660
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 660
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(644)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 644
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 644
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(681)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 681
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 681
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(683)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 683
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 683
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitStage(ResultStage 23 (name=load at UseCase5.java:20;jobs=17))
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 2, response is true
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(639)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 639
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 639
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(670)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 670
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting ResultStage 23 (MapPartitionsRDD[92] at load at UseCase5.java:20), which has no missing parents
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 670
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 23)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(466)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 466
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 466
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(614)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 614
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 614
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(663)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 663
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 663
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(625)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 625
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 625
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(492)
2022-02-10 15:13:44 DEBUG ContextCleaner:58 - Cleaning accumulator 492
2022-02-10 15:13:44 INFO  ContextCleaner:54 - Cleaned accumulator 492
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_41 stored as values in memory (estimated size 13.9 KB, free 1969.7 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_41 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_41 without replication took  0 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_41_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1969.7 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_41_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_41_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_41_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_41_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_41_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 41 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[92] at load at UseCase5.java:20) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Adding task set 23.0 with 1 tasks
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Epoch for TaskSet 23.0: 5
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 23.0: NO_PREF, ANY
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_23.0, runningTasks: 0
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Starting task 0.0 in stage 23.0 (TID 425, localhost, executor driver, partition 0, PROCESS_LOCAL, 8355 bytes)
2022-02-10 15:13:44 INFO  Executor:54 - Running task 0.0 in stage 23.0 (TID 425)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_41
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_41 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 15:13:44 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:44 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 15:13:44 DEBUG BlockManager:58 - Getting local block broadcast_40
2022-02-10 15:13:44 DEBUG BlockManager:58 - Level for block broadcast_40 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:44 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:44 INFO  Executor:54 - Finished task 0.0 in stage 23.0 (TID 425). 1352 bytes result sent to driver
2022-02-10 15:13:44 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_23.0, runningTasks: 0
2022-02-10 15:13:44 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:44 INFO  TaskSetManager:54 - Finished task 0.0 in stage 23.0 (TID 425) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:44 INFO  TaskSchedulerImpl:54 - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2022-02-10 15:13:44 INFO  DAGScheduler:54 - ResultStage 23 (load at UseCase5.java:20) finished in 0.000 s
2022-02-10 15:13:44 DEBUG DAGScheduler:58 - After removal of stage 23, remaining stages = 0
2022-02-10 15:13:44 INFO  DAGScheduler:54 - Job 17 finished: load at UseCase5.java:20, took 0.013855 s
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#203L]                       Aggregate [count(1) AS count#203L]
 +- Relation[department_id#196,department_name#197] csv   +- Relation[department_id#196,department_name#197] csv
          
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 4, response is true
2022-02-10 15:13:44 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:44 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#203L]                       Aggregate [count(1) AS count#203L]
!+- Relation[department_id#196,department_name#197] csv   +- Project
!                                                            +- Relation[department_id#196,department_name#197] csv
          
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 15:13:44 INFO  FileSourceStrategy:54 - Output Data Schema: struct<>
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 15:13:44 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 15:13:44 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 013 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     scan_mutableStateArray_0[0] = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       agg_doConsume_0(scan_row_0);
/* 037 */       if (shouldStop()) return;
/* 038 */     }
/* 039 */
/* 040 */   }
/* 041 */
/* 042 */   private void agg_doConsume_0(InternalRow scan_row_0) throws java.io.IOException {
/* 043 */     // do aggregate
/* 044 */     // common sub-expressions
/* 045 */
/* 046 */     // evaluate aggregate function
/* 047 */     long agg_value_1 = -1L;
/* 048 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 049 */     // update aggregation buffer
/* 050 */     agg_bufIsNull_0 = false;
/* 051 */     agg_bufValue_0 = agg_value_1;
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   protected void processNext() throws java.io.IOException {
/* 056 */     while (!agg_initAgg_0) {
/* 057 */       agg_initAgg_0 = true;
/* 058 */       long agg_beforeAgg_0 = System.nanoTime();
/* 059 */       agg_doAggregateWithoutKey_0();
/* 060 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 061 */
/* 062 */       // output the result
/* 063 */
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 065 */       agg_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 070 */       append((agg_mutableStateArray_0[0].getRow()));
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_42 stored as values in memory (estimated size 221.8 KB, free 1969.5 MB)
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_42 locally took  17 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_42 without replication took  17 ms
2022-02-10 15:13:44 INFO  MemoryStore:54 - Block broadcast_42_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 15:13:44 INFO  BlockManagerInfo:54 - Added broadcast_42_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 15:13:44 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_42_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Told master about block broadcast_42_piece0
2022-02-10 15:13:44 DEBUG BlockManager:58 - Put block broadcast_42_piece0 locally took  0 ms
2022-02-10 15:13:44 DEBUG BlockManager:58 - Putting block broadcast_42_piece0 without replication took  0 ms
2022-02-10 15:13:44 INFO  SparkContext:54 - Created broadcast 42 from count at UseCase5.java:37
2022-02-10 15:13:44 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194394 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[98] at count at UseCase5.java:37
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[98] at count at UseCase5.java:37)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[98] at count at UseCase5.java:37
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[98] at count at UseCase5.java:37)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 15:13:44 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 15:13:44 INFO  SparkContext:54 - Starting job: count at UseCase5.java:37
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Registering RDD 95 (count at UseCase5.java:37) as input to shuffle 5
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Got job 18 (count at UseCase5.java:37) with 1 output partitions
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Final stage: ResultStage 25 (count at UseCase5.java:37)
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 24)
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 24)
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - submitStage(ResultStage 25 (name=count at UseCase5.java:37;jobs=18))
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 24)
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 24 (name=count at UseCase5.java:37;jobs=18))
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 24 (MapPartitionsRDD[95] at count at UseCase5.java:37), which has no missing parents
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 24)
2022-02-10 15:13:45 INFO  MemoryStore:54 - Block broadcast_43 stored as values in memory (estimated size 12.5 KB, free 1969.4 MB)
2022-02-10 15:13:45 DEBUG BlockManager:58 - Put block broadcast_43 locally took  0 ms
2022-02-10 15:13:45 DEBUG BlockManager:58 - Putting block broadcast_43 without replication took  0 ms
2022-02-10 15:13:45 INFO  MemoryStore:54 - Block broadcast_43_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1969.4 MB)
2022-02-10 15:13:45 INFO  BlockManagerInfo:54 - Added broadcast_43_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 6.7 KB, free: 1970.3 MB)
2022-02-10 15:13:45 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_43_piece0
2022-02-10 15:13:45 DEBUG BlockManager:58 - Told master about block broadcast_43_piece0
2022-02-10 15:13:45 DEBUG BlockManager:58 - Put block broadcast_43_piece0 locally took  0 ms
2022-02-10 15:13:45 DEBUG BlockManager:58 - Putting block broadcast_43_piece0 without replication took  0 ms
2022-02-10 15:13:45 INFO  SparkContext:54 - Created broadcast 43 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[95] at count at UseCase5.java:37) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:45 INFO  TaskSchedulerImpl:54 - Adding task set 24.0 with 1 tasks
2022-02-10 15:13:45 DEBUG TaskSetManager:58 - Epoch for TaskSet 24.0: 5
2022-02-10 15:13:45 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 24.0: NO_PREF, ANY
2022-02-10 15:13:45 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_24.0, runningTasks: 0
2022-02-10 15:13:45 INFO  TaskSetManager:54 - Starting task 0.0 in stage 24.0 (TID 426, localhost, executor driver, partition 0, PROCESS_LOCAL, 8312 bytes)
2022-02-10 15:13:45 INFO  Executor:54 - Running task 0.0 in stage 24.0 (TID 426)
2022-02-10 15:13:45 DEBUG BlockManager:58 - Getting local block broadcast_43
2022-02-10 15:13:45 DEBUG BlockManager:58 - Level for block broadcast_43 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:45 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/departments/part-00000, range: 0-90, partition values: [empty row]
2022-02-10 15:13:45 DEBUG GenerateUnsafeProjection:58 - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

2022-02-10 15:13:45 DEBUG BlockManager:58 - Getting local block broadcast_42
2022-02-10 15:13:45 DEBUG BlockManager:58 - Level for block broadcast_42 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:45 INFO  Executor:54 - Finished task 0.0 in stage 24.0 (TID 426). 1516 bytes result sent to driver
2022-02-10 15:13:45 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_24.0, runningTasks: 0
2022-02-10 15:13:45 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 15:13:45 INFO  TaskSetManager:54 - Finished task 0.0 in stage 24.0 (TID 426) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:45 INFO  TaskSchedulerImpl:54 - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 15:13:45 INFO  DAGScheduler:54 - ShuffleMapStage 24 (count at UseCase5.java:37) finished in 0.016 s
2022-02-10 15:13:45 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 15:13:45 INFO  DAGScheduler:54 - running: Set()
2022-02-10 15:13:45 INFO  DAGScheduler:54 - waiting: Set(ResultStage 25)
2022-02-10 15:13:45 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 15:13:45 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 6
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - submitStage(ResultStage 25 (name=count at UseCase5.java:37;jobs=18))
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Submitting ResultStage 25 (MapPartitionsRDD[98] at count at UseCase5.java:37), which has no missing parents
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 25)
2022-02-10 15:13:45 INFO  MemoryStore:54 - Block broadcast_44 stored as values in memory (estimated size 7.3 KB, free 1969.4 MB)
2022-02-10 15:13:45 DEBUG BlockManager:58 - Put block broadcast_44 locally took  0 ms
2022-02-10 15:13:45 DEBUG BlockManager:58 - Putting block broadcast_44 without replication took  0 ms
2022-02-10 15:13:45 INFO  MemoryStore:54 - Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1969.4 MB)
2022-02-10 15:13:45 INFO  BlockManagerInfo:54 - Added broadcast_44_piece0 in memory on Clairvoyant-324.mshome.net:58899 (size: 3.9 KB, free: 1970.3 MB)
2022-02-10 15:13:45 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_44_piece0
2022-02-10 15:13:45 DEBUG BlockManager:58 - Told master about block broadcast_44_piece0
2022-02-10 15:13:45 DEBUG BlockManager:58 - Put block broadcast_44_piece0 locally took  0 ms
2022-02-10 15:13:45 DEBUG BlockManager:58 - Putting block broadcast_44_piece0 without replication took  0 ms
2022-02-10 15:13:45 INFO  SparkContext:54 - Created broadcast 44 from broadcast at DAGScheduler.scala:1184
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[98] at count at UseCase5.java:37) (first 15 tasks are for partitions Vector(0))
2022-02-10 15:13:45 INFO  TaskSchedulerImpl:54 - Adding task set 25.0 with 1 tasks
2022-02-10 15:13:45 DEBUG TaskSetManager:58 - Epoch for TaskSet 25.0: 6
2022-02-10 15:13:45 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 25.0: ANY
2022-02-10 15:13:45 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_25.0, runningTasks: 0
2022-02-10 15:13:45 INFO  TaskSetManager:54 - Starting task 0.0 in stage 25.0 (TID 427, localhost, executor driver, partition 0, ANY, 7767 bytes)
2022-02-10 15:13:45 INFO  Executor:54 - Running task 0.0 in stage 25.0 (TID 427)
2022-02-10 15:13:45 DEBUG BlockManager:58 - Getting local block broadcast_44
2022-02-10 15:13:45 DEBUG BlockManager:58 - Level for block broadcast_44 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 15:13:45 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 5, partitions 0-1
2022-02-10 15:13:45 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 15:13:45 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 15:13:45 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 15:13:45 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_5_0_0
2022-02-10 15:13:45 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 15:13:45 INFO  Executor:54 - Finished task 0.0 in stage 25.0 (TID 427). 1610 bytes result sent to driver
2022-02-10 15:13:45 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_25.0, runningTasks: 0
2022-02-10 15:13:45 INFO  TaskSetManager:54 - Finished task 0.0 in stage 25.0 (TID 427) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 15:13:45 INFO  TaskSchedulerImpl:54 - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2022-02-10 15:13:45 INFO  DAGScheduler:54 - ResultStage 25 (count at UseCase5.java:37) finished in 0.000 s
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - After removal of stage 25, remaining stages = 1
2022-02-10 15:13:45 DEBUG DAGScheduler:58 - After removal of stage 24, remaining stages = 0
2022-02-10 15:13:45 INFO  DAGScheduler:54 - Job 18 finished: count at UseCase5.java:37, took 0.027595 s
2022-02-10 15:13:45 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping Server@538613b3{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 15:13:45 DEBUG Server:433 - doStop Server@538613b3{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran SparkUI-31-acceptor-0@133e019b-ServerConnector@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractHandlerContainer:167 - Graceful shutdown Server@538613b3{STOPPING}[9.4.z-SNAPSHOT] by 
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping SelectorManager@Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@6ab72419{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@7fb74f64 on ManagedSelector@6ab72419{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@6ab72419{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@7fb74f64
2022-02-10 15:13:45 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@6ab72419{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe waiting with 0 keys
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@4cba6ca3 on ManagedSelector@6ab72419{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@6ab72419{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@10cdd6fe processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@4cba6ca3
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/PRODUCING/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@77b7ffa4 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@70e0accd/SelectorProducer@7957dc72/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@6ab72419{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@7544a1e4{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@1ef41632 on ManagedSelector@7544a1e4{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@7544a1e4{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@1ef41632
2022-02-10 15:13:45 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@7544a1e4{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@1004e11 on ManagedSelector@7544a1e4{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 waiting with 0 keys
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@7544a1e4{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@58e66e97 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@1004e11
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/PRODUCING/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@443dbe42 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@63fd4873/SelectorProducer@1e11bc55/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@7544a1e4{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@24f43aa3{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@4409ed3a on ManagedSelector@24f43aa3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@24f43aa3{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@4409ed3a
2022-02-10 15:13:45 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@24f43aa3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 waiting with 0 keys
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@5bc95e6c on ManagedSelector@24f43aa3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@24f43aa3{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@3ec26759 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@5bc95e6c
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/PRODUCING/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@345e5a17 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@3e587920/SelectorProducer@2ef8a8c3/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@24f43aa3{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@f8908f6{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@65b286a1 on ManagedSelector@f8908f6{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@f8908f6{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@65b286a1
2022-02-10 15:13:45 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@f8908f6{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 waiting with 0 keys
2022-02-10 15:13:45 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@66c12cc0 on ManagedSelector@f8908f6{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 15:13:45 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@f8908f6{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 15:13:45 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 woken with none selected
2022-02-10 15:13:45 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 woken up from select, 0/0/0 selected
2022-02-10 15:13:45 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@3e0b8b17 processing 0 keys, 1 updates
2022-02-10 15:13:45 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 15:13:45 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@66c12cc0
2022-02-10 15:13:45 DEBUG ManagedSelector:587 - updates 0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/PRODUCING/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/137123763@5be82d43 in QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@aec50a1/SelectorProducer@e3cee7b/IDLE/p=false/QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T15:13:45.034+05:30
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@f8908f6{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED SelectorManager@Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping HttpConnectionFactory@681aad3b[HTTP/1.1]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED HttpConnectionFactory@681aad3b[HTTP/1.1]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ScheduledExecutorScheduler@3a0807b7{STARTED}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ScheduledExecutorScheduler@3a0807b7{STOPPED}
2022-02-10 15:13:45 INFO  AbstractConnector:381 - Stopped Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED Spark@c00fff0{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 15:13:45 DEBUG AbstractHandler:107 - stopping Server@538613b3{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ContextHandlerCollection@560cbf1a{STARTED}
2022-02-10 15:13:45 DEBUG AbstractHandler:107 - stopping ContextHandlerCollection@560cbf1a{STOPPING}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ContextHandlerCollection@560cbf1a{STOPPED}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ErrorHandler@5c1bd44c{STARTED}
2022-02-10 15:13:45 DEBUG AbstractHandler:107 - stopping ErrorHandler@5c1bd44c{STOPPING}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ErrorHandler@5c1bd44c{STOPPED}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping QueuedThreadPool[SparkUI]@4a9e6faf{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:224 - Stopping QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@3d6300e8{s=0/8,p=0}]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:212 - stopping ReservedThreadExecutor@3d6300e8{s=0/8,p=0}
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED ReservedThreadExecutor@3d6300e8{s=-1/8,p=0}
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-31,5,main] for 14999
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-32,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-31,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-34,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-32,5,] for 14999
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-27,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-28,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-29,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/373437697@7945be81 in QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-33,5,main] for 14998
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-33,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-30,5,main] exited for QueuedThreadPool[SparkUI]@4a9e6faf{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-30,5,main] for 14998
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED QueuedThreadPool[SparkUI]@4a9e6faf{STOPPED,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 15:13:45 DEBUG AbstractLifeCycle:224 - STOPPED Server@538613b3{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 15:13:45 INFO  SparkUI:54 - Stopped Spark web UI at http://Clairvoyant-324.mshome.net:4040
2022-02-10 15:13:45 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2022-02-10 15:13:45 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 3, response is true
2022-02-10 15:13:45 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:58884
2022-02-10 15:13:45 INFO  MemoryStore:54 - MemoryStore cleared
2022-02-10 15:13:45 INFO  BlockManager:54 - BlockManager stopped
2022-02-10 15:13:45 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2022-02-10 15:13:45 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2022-02-10 15:13:45 INFO  SparkContext:54 - Successfully stopped SparkContext
2022-02-10 15:13:45 INFO  ShutdownHookManager:54 - Shutdown hook called
2022-02-10 15:13:45 INFO  ShutdownHookManager:54 - Deleting directory C:\Users\Anukul Thalkar\AppData\Local\Temp\spark-69f7013c-b837-4bef-876e-7f8267962964
