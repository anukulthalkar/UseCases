2022-02-10 13:31:56 INFO  UseCase2:63 - ------------------------------------------running UseCase 2------------------------------------------------------
2022-02-10 13:31:57 INFO  SparkContext:54 - Running Spark version 2.4.8
2022-02-10 13:31:57 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 13:31:57 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
2022-02-10 13:31:57 DEBUG MutableMetricsFactory:42 - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
2022-02-10 13:31:57 DEBUG MetricsSystemImpl:231 - UgiMetrics, User and group related metrics
2022-02-10 13:31:57 DEBUG KerberosName:88 - Kerberos krb5 configuration not found, setting default realm to empty
2022-02-10 13:31:57 DEBUG Groups:291 -  Creating new Groups object
2022-02-10 13:31:57 DEBUG NativeCodeLoader:46 - Trying to load the custom-built native-hadoop library...
2022-02-10 13:31:57 DEBUG NativeCodeLoader:50 - Loaded the native-hadoop library
2022-02-10 13:31:57 DEBUG JniBasedUnixGroupsMapping:50 - Using JniBasedUnixGroupsMapping for Group resolution
2022-02-10 13:31:57 DEBUG JniBasedUnixGroupsMappingWithFallback:45 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
2022-02-10 13:31:57 ERROR Shell:396 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable C:\Users\Anukul Thalkar\hadoop\bin\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:293)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2526)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at util.getSparkSession(util.java:4)
	at UseCase1.getOrders(UseCase1.java:19)
	at UseCase2.main(UseCase2.java:65)
2022-02-10 13:31:57 DEBUG Groups:103 - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2022-02-10 13:31:57 DEBUG UserGroupInformation:221 - hadoop login
2022-02-10 13:31:57 DEBUG UserGroupInformation:156 - hadoop login commit
2022-02-10 13:31:57 DEBUG UserGroupInformation:186 - using local user:NTUserPrincipal: Anukul Thalkar
2022-02-10 13:31:57 DEBUG UserGroupInformation:192 - Using user: "NTUserPrincipal: Anukul Thalkar" with name Anukul Thalkar
2022-02-10 13:31:57 DEBUG UserGroupInformation:202 - User entry: "Anukul Thalkar"
2022-02-10 13:31:57 DEBUG UserGroupInformation:825 - UGI loginUser:Anukul Thalkar (auth:SIMPLE)
2022-02-10 13:31:57 INFO  SparkContext:54 - Submitted application: 1446a7c6-d598-4aa6-9f24-f19cb2a983bb
2022-02-10 13:31:57 INFO  SecurityManager:54 - Changing view acls to: Anukul Thalkar
2022-02-10 13:31:57 INFO  SecurityManager:54 - Changing modify acls to: Anukul Thalkar
2022-02-10 13:31:57 INFO  SecurityManager:54 - Changing view acls groups to: 
2022-02-10 13:31:57 INFO  SecurityManager:54 - Changing modify acls groups to: 
2022-02-10 13:31:57 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Anukul Thalkar); groups with view permissions: Set(); users  with modify permissions: Set(Anukul Thalkar); groups with modify permissions: Set()
2022-02-10 13:31:57 DEBUG InternalLoggerFactory:45 - Using SLF4J as the default logging framework
2022-02-10 13:31:57 DEBUG InternalThreadLocalMap:56 - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2022-02-10 13:31:57 DEBUG InternalThreadLocalMap:59 - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2022-02-10 13:31:57 DEBUG MultithreadEventLoopGroup:44 - -Dio.netty.eventLoopThreads: 16
2022-02-10 13:31:57 DEBUG NioEventLoop:106 - -Dio.netty.noKeySetOptimization: false
2022-02-10 13:31:57 DEBUG NioEventLoop:107 - -Dio.netty.selectorAutoRebuildThreshold: 512
2022-02-10 13:31:57 DEBUG PlatformDependent:1003 - Platform: Windows
2022-02-10 13:31:57 DEBUG PlatformDependent0:396 - -Dio.netty.noUnsafe: false
2022-02-10 13:31:57 DEBUG PlatformDependent0:852 - Java version: 8
2022-02-10 13:31:57 DEBUG PlatformDependent0:121 - sun.misc.Unsafe.theUnsafe: available
2022-02-10 13:31:57 DEBUG PlatformDependent0:145 - sun.misc.Unsafe.copyMemory: available
2022-02-10 13:31:57 DEBUG PlatformDependent0:183 - java.nio.Buffer.address: available
2022-02-10 13:31:57 DEBUG PlatformDependent0:244 - direct buffer constructor: available
2022-02-10 13:31:57 DEBUG PlatformDependent0:314 - java.nio.Bits.unaligned: available, true
2022-02-10 13:31:57 DEBUG PlatformDependent0:379 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2022-02-10 13:31:57 DEBUG PlatformDependent0:386 - java.nio.DirectByteBuffer.<init>(long, int): available
2022-02-10 13:31:57 DEBUG PlatformDependent:1046 - sun.misc.Unsafe: available
2022-02-10 13:31:57 DEBUG PlatformDependent:1165 - -Dio.netty.tmpdir: C:\Users\ANUKUL~1\AppData\Local\Temp (java.io.tmpdir)
2022-02-10 13:31:57 DEBUG PlatformDependent:1244 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
2022-02-10 13:31:57 DEBUG PlatformDependent:177 - -Dio.netty.maxDirectMemory: 3758096384 bytes
2022-02-10 13:31:57 DEBUG PlatformDependent:184 - -Dio.netty.uninitializedArrayAllocationThreshold: -1
2022-02-10 13:31:57 DEBUG CleanerJava6:92 - java.nio.ByteBuffer.cleaner(): available
2022-02-10 13:31:57 DEBUG PlatformDependent:204 - -Dio.netty.noPreferDirect: false
2022-02-10 13:31:57 DEBUG PlatformDependent:907 - org.jctools-core.MpscChunkedArrayQueue: available
2022-02-10 13:31:57 DEBUG ResourceLeakDetector:130 - -Dio.netty.leakDetection.level: simple
2022-02-10 13:31:57 DEBUG ResourceLeakDetector:131 - -Dio.netty.leakDetection.targetRecords: 4
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:156 - -Dio.netty.allocator.numHeapArenas: 16
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:157 - -Dio.netty.allocator.numDirectArenas: 16
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:159 - -Dio.netty.allocator.pageSize: 8192
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:164 - -Dio.netty.allocator.maxOrder: 11
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:168 - -Dio.netty.allocator.chunkSize: 16777216
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:169 - -Dio.netty.allocator.tinyCacheSize: 512
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:170 - -Dio.netty.allocator.smallCacheSize: 256
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:171 - -Dio.netty.allocator.normalCacheSize: 64
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:172 - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:173 - -Dio.netty.allocator.cacheTrimInterval: 8192
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:174 - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:175 - -Dio.netty.allocator.useCacheForAllThreads: true
2022-02-10 13:31:57 DEBUG PooledByteBufAllocator:176 - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
2022-02-10 13:31:57 DEBUG DefaultChannelId:79 - -Dio.netty.processId: 2180 (auto-detected)
2022-02-10 13:31:57 DEBUG NetUtil:139 - -Djava.net.preferIPv4Stack: false
2022-02-10 13:31:57 DEBUG NetUtil:140 - -Djava.net.preferIPv6Addresses: false
2022-02-10 13:31:57 DEBUG NetUtil:224 - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)
2022-02-10 13:31:57 DEBUG NetUtil:289 - Failed to get SOMAXCONN from sysctl and file \proc\sys\net\core\somaxconn. Default: 200
2022-02-10 13:31:58 DEBUG DefaultChannelId:101 - -Dio.netty.machineId: 7c:70:db:ff:fe:41:5e:f6 (auto-detected)
2022-02-10 13:31:58 DEBUG ByteBufUtil:86 - -Dio.netty.allocator.type: pooled
2022-02-10 13:31:58 DEBUG ByteBufUtil:95 - -Dio.netty.threadLocalDirectBufferSize: 0
2022-02-10 13:31:58 DEBUG ByteBufUtil:98 - -Dio.netty.maxThreadLocalCharBufferSize: 16384
2022-02-10 13:31:58 DEBUG TransportServer:141 - Shuffle server started on port: 53712
2022-02-10 13:31:58 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 53712.
2022-02-10 13:31:58 DEBUG SparkEnv:58 - Using serializer: class org.apache.spark.serializer.JavaSerializer
2022-02-10 13:31:58 INFO  SparkEnv:54 - Registering MapOutputTracker
2022-02-10 13:31:58 DEBUG MapOutputTrackerMasterEndpoint:58 - init
2022-02-10 13:31:58 INFO  SparkEnv:54 - Registering BlockManagerMaster
2022-02-10 13:31:58 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2022-02-10 13:31:58 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2022-02-10 13:31:58 INFO  DiskBlockManager:54 - Created local directory at C:\Users\Anukul Thalkar\AppData\Local\Temp\blockmgr-2cdc453c-1476-4704-8c22-6cba8cb80e40
2022-02-10 13:31:58 DEBUG DiskBlockManager:58 - Adding shutdown hook
2022-02-10 13:31:58 DEBUG ShutdownHookManager:58 - Adding shutdown hook
2022-02-10 13:31:58 INFO  MemoryStore:54 - MemoryStore started with capacity 1970.4 MB
2022-02-10 13:31:58 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2022-02-10 13:31:58 DEBUG OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:58 - init
2022-02-10 13:31:58 DEBUG SecurityManager:58 - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2022-02-10 13:31:58 DEBUG log:159 - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.spark_project.jetty.util.log) via org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 13:31:58 INFO  log:169 - Logging initialized @2201ms to org.spark_project.jetty.util.log.Slf4jLog
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@26a4842b
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@15c25153{/,null,STOPPED} added {ServletHandler@2dbf4cbd{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2dbf4cbd{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-426b6a74==org.apache.spark.ui.JettyUtils$$anon$3@51478dde{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2dbf4cbd{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-426b6a74,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@71984c3
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@165b2f7f{/,null,STOPPED} added {ServletHandler@5536379e{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@5536379e{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3==org.apache.spark.ui.JettyUtils$$anon$3@8bf2181c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@5536379e{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2b491fee
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3f1c5af9{/,null,STOPPED} added {ServletHandler@1c55f277{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@1c55f277{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18==org.apache.spark.ui.JettyUtils$$anon$3@5ca525df{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@1c55f277{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3e8f7922
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@63192798{/,null,STOPPED} added {ServletHandler@50eca7c6{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@50eca7c6{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8==org.apache.spark.ui.JettyUtils$$anon$3@446a1ea6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@50eca7c6{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@4275c20c
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@7c56e013{/,null,STOPPED} added {ServletHandler@3fc9dfc5{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@3fc9dfc5{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-40258c2f==org.apache.spark.ui.JettyUtils$$anon$3@de2e830a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@3fc9dfc5{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-40258c2f,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@2cac4385
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@6731787b{/,null,STOPPED} added {ServletHandler@16f7b4af{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@16f7b4af{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa==org.apache.spark.ui.JettyUtils$$anon$3@8de45dfd{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@16f7b4af{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@58bf8650
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@73c60324{/,null,STOPPED} added {ServletHandler@71ae31b0{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@71ae31b0{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0==org.apache.spark.ui.JettyUtils$$anon$3@8356ea3d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@71ae31b0{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6f0ca692
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@2c104774{/,null,STOPPED} added {ServletHandler@2cb3d0f7{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2cb3d0f7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4e517165==org.apache.spark.ui.JettyUtils$$anon$3@d6bdc392{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2cb3d0f7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4e517165,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@6a66a204
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5860f3d7{/,null,STOPPED} added {ServletHandler@1d7f7be7{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@1d7f7be7{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-42f3156d==org.apache.spark.ui.JettyUtils$$anon$3@a096f009{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@1d7f7be7{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-42f3156d,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1ddae9b5
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@427b5f92{/,null,STOPPED} added {ServletHandler@24bdb479{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@24bdb479{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe==org.apache.spark.ui.JettyUtils$$anon$3@de37468{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@24bdb479{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@533377b
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3383649e{/,null,STOPPED} added {ServletHandler@10fde30a{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@10fde30a{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-f27ea3==org.apache.spark.ui.JettyUtils$$anon$3@5d2226f8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@10fde30a{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-f27ea3,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1ce61929
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@346939bf{/,null,STOPPED} added {ServletHandler@4bf3798b{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@4bf3798b{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-58670130==org.apache.spark.ui.JettyUtils$$anon$3@f42cbcae{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@4bf3798b{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-58670130,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@9bd0fa6
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@59d2103b{/,null,STOPPED} added {ServletHandler@39dcf4b0{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@39dcf4b0{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b==org.apache.spark.ui.JettyUtils$$anon$3@a7aedd00{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@39dcf4b0{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@f6c03cb
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@46f699d5{/,null,STOPPED} added {ServletHandler@18518ccf{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@18518ccf{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-1991f767==org.apache.spark.ui.JettyUtils$$anon$3@8e70e43e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@18518ccf{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-1991f767,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@659eef7
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@162be91c{/,null,STOPPED} added {ServletHandler@2488b073{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2488b073{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20==org.apache.spark.ui.JettyUtils$$anon$3@3f098bba{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2488b073{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@55787112
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1cd201a8{/,null,STOPPED} added {ServletHandler@7db82169{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7db82169{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4==org.apache.spark.ui.JettyUtils$$anon$3@8e7f2ad7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7db82169{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@21d8bcbe
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5be067de{/,null,STOPPED} added {ServletHandler@7383eae2{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7383eae2{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-18245eb0==org.apache.spark.ui.JettyUtils$$anon$3@4dad6ef3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7383eae2{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-18245eb0,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7c7d3c46
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@24fb6a80{/,null,STOPPED} added {ServletHandler@48c35007{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@48c35007{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-72a85671==org.apache.spark.ui.JettyUtils$$anon$3@cffa8639{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@48c35007{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-72a85671,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@18f20260
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@4ae33a11{/,null,STOPPED} added {ServletHandler@7a48e6e2{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7a48e6e2{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e==org.apache.spark.ui.JettyUtils$$anon$3@768f7f21{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7a48e6e2{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3a94964
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@5049d8b2{/,null,STOPPED} added {ServletHandler@6d0b5baf{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@6d0b5baf{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-631e06ab==org.apache.spark.ui.JettyUtils$$anon$3@4d8ed7e4{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@6d0b5baf{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-631e06ab,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@34a75079
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@346a361{/,null,STOPPED} added {ServletHandler@107ed6fc{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG PreEncodedHttpField:61 - HttpField encoders loaded: []
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@107ed6fc{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-bcec031==org.spark_project.jetty.servlet.DefaultServlet@a56673a0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@107ed6fc{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-bcec031,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@545de5a4
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@29ef6856{/,null,STOPPED} added {ServletHandler@ab7a938{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@ab7a938{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d==org.apache.spark.ui.JettyUtils$$anon$4@e60ad9c3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@ab7a938{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@61526469
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@274872f8{/,null,STOPPED} added {ServletHandler@76ba13c{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@76ba13c{STOPPED} added {org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@76ba13c{STOPPED} added {[/*]=>org.glassfish.jersey.servlet.ServletContainer-592e843a,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@25ddbbbb
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1536602f{/,null,STOPPED} added {ServletHandler@4ebea12c{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@4ebea12c{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4==org.apache.spark.ui.JettyUtils$$anon$4@23ca801b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@4ebea12c{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@7fcbe147
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@235f4c10{/,null,STOPPED} added {ServletHandler@743cb8e0{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@743cb8e0{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$4-c7a975a==org.apache.spark.ui.JettyUtils$$anon$4@4777ee2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@743cb8e0{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$4-c7a975a,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[qtp375097969]@165b8a71{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY] added {org.spark_project.jetty.util.thread.ThreadPoolBudget@41a90fa8,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - Server@4b85880b{STOPPED}[9.4.z-SNAPSHOT] added {QueuedThreadPool[SparkUI]@165b8a71{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY],AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - Server@4b85880b{STOPPED}[9.4.z-SNAPSHOT] added {ErrorHandler@76a36b71{STOPPED},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - Server@4b85880b{STOPPED}[9.4.z-SNAPSHOT] added {ContextHandlerCollection@6c67e137{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting Server@4b85880b{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 13:31:58 INFO  Server:375 - jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_281-b09
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting Server@4b85880b{STARTING}[9.4.z-SNAPSHOT]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting QueuedThreadPool[SparkUI]@165b8a71{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY]
2022-02-10 13:31:58 DEBUG ReservedThreadExecutor:85 - ReservedThreadExecutor@47da3952{s=0/8,p=0}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=0<=200,i=0,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}] added {ReservedThreadExecutor@47da3952{s=0/8,p=0},AUTO}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ReservedThreadExecutor@47da3952{s=0/8,p=0}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2285ms ReservedThreadExecutor@47da3952{s=0/8,p=0}
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-27,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-28,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-29,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-30,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=4<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=5<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-31,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=3<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-32,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-33,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=6<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=7<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:784 - Starting Thread[SparkUI-34,5,main]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2289ms QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG QueuedThreadPool:980 - Runner started for QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ErrorHandler@76a36b71{STOPPED}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ErrorHandler@76a36b71{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2289ms ErrorHandler@76a36b71{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ContextHandlerCollection@6c67e137{STOPPED}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ContextHandlerCollection@6c67e137{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2290ms ContextHandlerCollection@6c67e137{STARTED}
2022-02-10 13:31:58 INFO  Server:415 - Started @2290ms
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2290ms Server@4b85880b{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 13:31:58 DEBUG JettyUtils:58 - Using requestHeaderSize: 8192
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - HttpConnectionFactory@71104a4[HTTP/1.1] added {HttpConfiguration@4985cbcb{32768/8192,8192/8192,https://:0,[]},POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{null, ()}{0.0.0.0:0} added {Server@4b85880b{STARTED}[9.4.z-SNAPSHOT],UNMANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{null, ()}{0.0.0.0:0} added {QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}],UNMANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{null, ()}{0.0.0.0:0} added {ScheduledExecutorScheduler@b91d8c4{STOPPED},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{null, ()}{0.0.0.0:0} added {org.spark_project.jetty.io.ArrayByteBufferPool@4b6166aa,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{null, (http/1.1)}{0.0.0.0:0} added {HttpConnectionFactory@71104a4[HTTP/1.1],AUTO}
2022-02-10 13:31:58 DEBUG AbstractConnector:484 - ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added HttpConnectionFactory@71104a4[HTTP/1.1]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:0} added {SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:0},MANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:4040],POJO}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ScheduledExecutorScheduler@b91d8c4{STOPPED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2313ms ScheduledExecutorScheduler@b91d8c4{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting HttpConnectionFactory@71104a4[HTTP/1.1]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2313ms HttpConnectionFactory@71104a4[HTTP/1.1]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 added {SelectorProducer@3b9d6699,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 added {QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}],UNMANAGED}
2022-02-10 13:31:58 DEBUG EatWhatYouKill:93 - EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 created
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ManagedSelector@25f9407e{STOPPED} id=0 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30,MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@25f9407e{STOPPED} id=0 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 added {SelectorProducer@1a69561c,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 added {QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}],UNMANAGED}
2022-02-10 13:31:58 DEBUG EatWhatYouKill:93 - EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30 created
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ManagedSelector@59aa20b3{STOPPED} id=1 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.397+05:30,MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@59aa20b3{STOPPED} id=1 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 added {SelectorProducer@4b21844c,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 added {QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}],UNMANAGED}
2022-02-10 13:31:58 DEBUG EatWhatYouKill:93 - EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 created
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ManagedSelector@1b28f282{STOPPED} id=2 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30,MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@1b28f282{STOPPED} id=2 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 added {SelectorProducer@5e77f0f4,POJO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 added {QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}],UNMANAGED}
2022-02-10 13:31:58 DEBUG EatWhatYouKill:93 - EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 created
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ManagedSelector@19b30c92{STOPPED} id=3 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30,MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {ManagedSelector@19b30c92{STOPPED} id=3 keys=-1 selected=-1 updates=0,AUTO}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@25f9407e{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2340ms EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4940809c startThread=0
2022-02-10 13:31:58 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4940809c in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@16423501 on ManagedSelector@25f9407e{STARTING} id=0 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG EatWhatYouKill:141 - EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 tryProduce false
2022-02-10 13:31:58 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:31:58 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@16423501
2022-02-10 13:31:58 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:31:58 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b waiting with 0 keys
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2343ms ManagedSelector@25f9407e{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@59aa20b3{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2344ms EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4efcf8a startThread=0
2022-02-10 13:31:58 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@7a138fc5 on ManagedSelector@59aa20b3{STARTING} id=1 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4efcf8a in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG EatWhatYouKill:141 - EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 tryProduce false
2022-02-10 13:31:58 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:31:58 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@7a138fc5
2022-02-10 13:31:58 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2346ms ManagedSelector@59aa20b3{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c waiting with 0 keys
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@1b28f282{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2346ms EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@379ab47b startThread=0
2022-02-10 13:31:58 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@379ab47b in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@307765b4 on ManagedSelector@1b28f282{STARTING} id=2 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG EatWhatYouKill:141 - EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 tryProduce false
2022-02-10 13:31:58 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:31:58 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@307765b4
2022-02-10 13:31:58 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:31:58 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f waiting with 0 keys
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2348ms ManagedSelector@1b28f282{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ManagedSelector@19b30c92{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2349ms EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30
2022-02-10 13:31:58 DEBUG QueuedThreadPool:719 - queue org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@2c95ac9e startThread=0
2022-02-10 13:31:58 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@2c95ac9e in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$Start@4e4efc1b on ManagedSelector@19b30c92{STARTING} id=3 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG EatWhatYouKill:141 - EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:31:58.413+05:30 tryProduce false
2022-02-10 13:31:58 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:31:58 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$Start@4e4efc1b
2022-02-10 13:31:58 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:31:58 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 waiting with 0 keys
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2350ms ManagedSelector@19b30c92{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2351ms SelectorManager@ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} added {acceptor-0@7db534f2,POJO}
2022-02-10 13:31:58 DEBUG QueuedThreadPool:719 - queue acceptor-0@7db534f2 startThread=0
2022-02-10 13:31:58 INFO  AbstractConnector:331 - Started ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:31:58 DEBUG QueuedThreadPool:1035 - run acceptor-0@7db534f2 in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2352ms ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:31:58 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - Server@4b85880b{STARTED}[9.4.z-SNAPSHOT] added {Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040},UNMANAGED}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@69f63d95{STOPPED,min=32,inflate=-1} mime types IncludeExclude@9cd25ff{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@69f63d95{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@15c25153{/jobs,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@69f63d95{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@69f63d95{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@69f63d95{STARTING,min=32,inflate=-1} added {DeflaterPool@780ec4a5{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@69f63d95{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@15c25153{/jobs,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@15c25153{/jobs,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2dbf4cbd{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-426b6a74[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-426b6a74==org.apache.spark.ui.JettyUtils$$anon$3@51478dde{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-426b6a74=org.apache.spark.ui.JettyUtils$$anon$3-426b6a74==org.apache.spark.ui.JettyUtils$$anon$3@51478dde{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@2dbf4cbd{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2369ms ServletHandler@2dbf4cbd{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-426b6a74==org.apache.spark.ui.JettyUtils$$anon$3@51478dde{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2372ms org.apache.spark.ui.JettyUtils$$anon$3-426b6a74==org.apache.spark.ui.JettyUtils$$anon$3@51478dde{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-426b6a74
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2374ms o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@780ec4a5{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2374ms DeflaterPool@780ec4a5{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2374ms GzipHandler@69f63d95{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@51f49060{STOPPED,min=32,inflate=-1} mime types IncludeExclude@514eedd8{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@51f49060{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@51f49060{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@51f49060{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@51f49060{STARTING,min=32,inflate=-1} added {DeflaterPool@617fe9e1{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@51f49060{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@5536379e{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3==org.apache.spark.ui.JettyUtils$$anon$3@8bf2181c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3=org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3==org.apache.spark.ui.JettyUtils$$anon$3@8bf2181c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@5536379e{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2376ms ServletHandler@5536379e{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3==org.apache.spark.ui.JettyUtils$$anon$3@8bf2181c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2377ms org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3==org.apache.spark.ui.JettyUtils$$anon$3@8bf2181c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-277f7dd3
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2377ms o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@617fe9e1{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2377ms DeflaterPool@617fe9e1{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2377ms GzipHandler@51f49060{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@6970140a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1cf2fed4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@6970140a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@6970140a{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@6970140a{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@6970140a{STARTING,min=32,inflate=-1} added {DeflaterPool@3af4e0bf{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@6970140a{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@1c55f277{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18==org.apache.spark.ui.JettyUtils$$anon$3@5ca525df{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18=org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18==org.apache.spark.ui.JettyUtils$$anon$3@5ca525df{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@1c55f277{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2379ms ServletHandler@1c55f277{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18==org.apache.spark.ui.JettyUtils$$anon$3@5ca525df{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2379ms org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18==org.apache.spark.ui.JettyUtils$$anon$3@5ca525df{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-5ddabb18
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2379ms o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@3af4e0bf{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2379ms DeflaterPool@3af4e0bf{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2380ms GzipHandler@6970140a{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@4d63b624{STOPPED,min=32,inflate=-1} mime types IncludeExclude@466cf502{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4d63b624{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@4d63b624{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4d63b624{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4d63b624{STARTING,min=32,inflate=-1} added {DeflaterPool@5b800468{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@4d63b624{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@50eca7c6{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8==org.apache.spark.ui.JettyUtils$$anon$3@446a1ea6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8=org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8==org.apache.spark.ui.JettyUtils$$anon$3@446a1ea6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@50eca7c6{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2383ms ServletHandler@50eca7c6{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8==org.apache.spark.ui.JettyUtils$$anon$3@446a1ea6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2384ms org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8==org.apache.spark.ui.JettyUtils$$anon$3@446a1ea6{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-58e6d4b8
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2384ms o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@5b800468{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2384ms DeflaterPool@5b800468{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2384ms GzipHandler@4d63b624{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@2e185cd7{STOPPED,min=32,inflate=-1} mime types IncludeExclude@7e1a1da6{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@2e185cd7{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@7c56e013{/stages,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@2e185cd7{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@2e185cd7{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@2e185cd7{STARTING,min=32,inflate=-1} added {DeflaterPool@5f7f2382{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@2e185cd7{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@7c56e013{/stages,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@7c56e013{/stages,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@3fc9dfc5{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-40258c2f[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-40258c2f==org.apache.spark.ui.JettyUtils$$anon$3@de2e830a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-40258c2f=org.apache.spark.ui.JettyUtils$$anon$3-40258c2f==org.apache.spark.ui.JettyUtils$$anon$3@de2e830a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@3fc9dfc5{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2386ms ServletHandler@3fc9dfc5{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-40258c2f==org.apache.spark.ui.JettyUtils$$anon$3@de2e830a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2387ms org.apache.spark.ui.JettyUtils$$anon$3-40258c2f==org.apache.spark.ui.JettyUtils$$anon$3@de2e830a{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-40258c2f
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2387ms o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@5f7f2382{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2387ms DeflaterPool@5f7f2382{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2387ms GzipHandler@2e185cd7{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@407cf41{STOPPED,min=32,inflate=-1} mime types IncludeExclude@6815c5f2{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@407cf41{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@407cf41{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@407cf41{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@407cf41{STARTING,min=32,inflate=-1} added {DeflaterPool@46cc127b{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@407cf41{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@16f7b4af{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa==org.apache.spark.ui.JettyUtils$$anon$3@8de45dfd{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa=org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa==org.apache.spark.ui.JettyUtils$$anon$3@8de45dfd{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@16f7b4af{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2389ms ServletHandler@16f7b4af{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa==org.apache.spark.ui.JettyUtils$$anon$3@8de45dfd{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2389ms org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa==org.apache.spark.ui.JettyUtils$$anon$3@8de45dfd{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7adf16aa
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2389ms o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@46cc127b{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2389ms DeflaterPool@46cc127b{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2390ms GzipHandler@407cf41{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@60094a13{STOPPED,min=32,inflate=-1} mime types IncludeExclude@5aceec94{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@60094a13{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@60094a13{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@60094a13{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@60094a13{STARTING,min=32,inflate=-1} added {DeflaterPool@1c32886a{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@60094a13{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@71ae31b0{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0==org.apache.spark.ui.JettyUtils$$anon$3@8356ea3d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0=org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0==org.apache.spark.ui.JettyUtils$$anon$3@8356ea3d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@71ae31b0{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2393ms ServletHandler@71ae31b0{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0==org.apache.spark.ui.JettyUtils$$anon$3@8356ea3d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2393ms org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0==org.apache.spark.ui.JettyUtils$$anon$3@8356ea3d{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4ba534b0
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2393ms o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1c32886a{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2394ms DeflaterPool@1c32886a{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2394ms GzipHandler@60094a13{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@3d3f761a{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3546d80f{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@3d3f761a{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@3d3f761a{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@3d3f761a{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@3d3f761a{STARTING,min=32,inflate=-1} added {DeflaterPool@579d011c{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@3d3f761a{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2cb3d0f7{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4e517165[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4e517165==org.apache.spark.ui.JettyUtils$$anon$3@d6bdc392{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4e517165=org.apache.spark.ui.JettyUtils$$anon$3-4e517165==org.apache.spark.ui.JettyUtils$$anon$3@d6bdc392{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@2cb3d0f7{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2397ms ServletHandler@2cb3d0f7{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4e517165==org.apache.spark.ui.JettyUtils$$anon$3@d6bdc392{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2398ms org.apache.spark.ui.JettyUtils$$anon$3-4e517165==org.apache.spark.ui.JettyUtils$$anon$3@d6bdc392{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4e517165
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2398ms o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@579d011c{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2398ms DeflaterPool@579d011c{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2398ms GzipHandler@3d3f761a{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@3670f00{STOPPED,min=32,inflate=-1} mime types IncludeExclude@452e26d0{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@3670f00{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@3670f00{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@3670f00{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@3670f00{STARTING,min=32,inflate=-1} added {DeflaterPool@46ab18da{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@3670f00{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@1d7f7be7{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-42f3156d[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-42f3156d==org.apache.spark.ui.JettyUtils$$anon$3@a096f009{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-42f3156d=org.apache.spark.ui.JettyUtils$$anon$3-42f3156d==org.apache.spark.ui.JettyUtils$$anon$3@a096f009{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@1d7f7be7{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2400ms ServletHandler@1d7f7be7{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-42f3156d==org.apache.spark.ui.JettyUtils$$anon$3@a096f009{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2401ms org.apache.spark.ui.JettyUtils$$anon$3-42f3156d==org.apache.spark.ui.JettyUtils$$anon$3@a096f009{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-42f3156d
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2401ms o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@46ab18da{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2401ms DeflaterPool@46ab18da{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2401ms GzipHandler@3670f00{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@790174f2{STOPPED,min=32,inflate=-1} mime types IncludeExclude@42257bdd{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@790174f2{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@790174f2{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@790174f2{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@790174f2{STARTING,min=32,inflate=-1} added {DeflaterPool@7689ddef{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@790174f2{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@24bdb479{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe==org.apache.spark.ui.JettyUtils$$anon$3@de37468{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe=org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe==org.apache.spark.ui.JettyUtils$$anon$3@de37468{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@24bdb479{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2403ms ServletHandler@24bdb479{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe==org.apache.spark.ui.JettyUtils$$anon$3@de37468{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2403ms org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe==org.apache.spark.ui.JettyUtils$$anon$3@de37468{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-7e3f95fe
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2404ms o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@7689ddef{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2404ms DeflaterPool@7689ddef{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2404ms GzipHandler@790174f2{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@687a762c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1a2e2935{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@687a762c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@3383649e{/storage,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@687a762c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@687a762c{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@687a762c{STARTING,min=32,inflate=-1} added {DeflaterPool@733c423e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@687a762c{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3383649e{/storage,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3383649e{/storage,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@10fde30a{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-f27ea3[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-f27ea3==org.apache.spark.ui.JettyUtils$$anon$3@5d2226f8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-f27ea3=org.apache.spark.ui.JettyUtils$$anon$3-f27ea3==org.apache.spark.ui.JettyUtils$$anon$3@5d2226f8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@10fde30a{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2406ms ServletHandler@10fde30a{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-f27ea3==org.apache.spark.ui.JettyUtils$$anon$3@5d2226f8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2406ms org.apache.spark.ui.JettyUtils$$anon$3-f27ea3==org.apache.spark.ui.JettyUtils$$anon$3@5d2226f8{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-f27ea3
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2407ms o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@733c423e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2407ms DeflaterPool@733c423e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2407ms GzipHandler@687a762c{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@4b629f13{STOPPED,min=32,inflate=-1} mime types IncludeExclude@70925b45{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4b629f13{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@4b629f13{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4b629f13{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4b629f13{STARTING,min=32,inflate=-1} added {DeflaterPool@1b9ea3e3{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@4b629f13{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4bf3798b{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-58670130[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-58670130==org.apache.spark.ui.JettyUtils$$anon$3@f42cbcae{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-58670130=org.apache.spark.ui.JettyUtils$$anon$3-58670130==org.apache.spark.ui.JettyUtils$$anon$3@f42cbcae{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@4bf3798b{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2409ms ServletHandler@4bf3798b{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-58670130==org.apache.spark.ui.JettyUtils$$anon$3@f42cbcae{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2410ms org.apache.spark.ui.JettyUtils$$anon$3-58670130==org.apache.spark.ui.JettyUtils$$anon$3@f42cbcae{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-58670130
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2410ms o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1b9ea3e3{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2410ms DeflaterPool@1b9ea3e3{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2410ms GzipHandler@4b629f13{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@aa22f1c{STOPPED,min=32,inflate=-1} mime types IncludeExclude@55e7a35c{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@aa22f1c{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@aa22f1c{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@aa22f1c{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@aa22f1c{STARTING,min=32,inflate=-1} added {DeflaterPool@37cd92d6{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@aa22f1c{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@39dcf4b0{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b==org.apache.spark.ui.JettyUtils$$anon$3@a7aedd00{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b=org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b==org.apache.spark.ui.JettyUtils$$anon$3@a7aedd00{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@39dcf4b0{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2414ms ServletHandler@39dcf4b0{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b==org.apache.spark.ui.JettyUtils$$anon$3@a7aedd00{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2415ms org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b==org.apache.spark.ui.JettyUtils$$anon$3@a7aedd00{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-6e4de19b
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2415ms o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@37cd92d6{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2415ms DeflaterPool@37cd92d6{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2415ms GzipHandler@aa22f1c{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@5922ae77{STOPPED,min=32,inflate=-1} mime types IncludeExclude@4263b080{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5922ae77{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@5922ae77{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5922ae77{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5922ae77{STARTING,min=32,inflate=-1} added {DeflaterPool@2af616d3{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@5922ae77{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@18518ccf{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-1991f767[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-1991f767==org.apache.spark.ui.JettyUtils$$anon$3@8e70e43e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-1991f767=org.apache.spark.ui.JettyUtils$$anon$3-1991f767==org.apache.spark.ui.JettyUtils$$anon$3@8e70e43e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@18518ccf{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2417ms ServletHandler@18518ccf{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-1991f767==org.apache.spark.ui.JettyUtils$$anon$3@8e70e43e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2418ms org.apache.spark.ui.JettyUtils$$anon$3-1991f767==org.apache.spark.ui.JettyUtils$$anon$3@8e70e43e{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-1991f767
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2418ms o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2af616d3{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2418ms DeflaterPool@2af616d3{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2418ms GzipHandler@5922ae77{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@71f67a79{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3deb2326{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@71f67a79{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@162be91c{/environment,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@71f67a79{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@71f67a79{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@71f67a79{STARTING,min=32,inflate=-1} added {DeflaterPool@62d363ab{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@71f67a79{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@162be91c{/environment,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@162be91c{/environment,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2488b073{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20==org.apache.spark.ui.JettyUtils$$anon$3@3f098bba{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20=org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20==org.apache.spark.ui.JettyUtils$$anon$3@3f098bba{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@2488b073{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2420ms ServletHandler@2488b073{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20==org.apache.spark.ui.JettyUtils$$anon$3@3f098bba{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2420ms org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20==org.apache.spark.ui.JettyUtils$$anon$3@3f098bba{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-1c9f0a20
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2420ms o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@62d363ab{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2420ms DeflaterPool@62d363ab{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2420ms GzipHandler@71f67a79{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@7889a1ac{STOPPED,min=32,inflate=-1} mime types IncludeExclude@3aee3976{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@7889a1ac{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@7889a1ac{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@7889a1ac{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@7889a1ac{STARTING,min=32,inflate=-1} added {DeflaterPool@5ef8df1e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@7889a1ac{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7db82169{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4==org.apache.spark.ui.JettyUtils$$anon$3@8e7f2ad7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4=org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4==org.apache.spark.ui.JettyUtils$$anon$3@8e7f2ad7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@7db82169{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2423ms ServletHandler@7db82169{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4==org.apache.spark.ui.JettyUtils$$anon$3@8e7f2ad7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2423ms org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4==org.apache.spark.ui.JettyUtils$$anon$3@8e7f2ad7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-1992eaf4
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2423ms o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@5ef8df1e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2423ms DeflaterPool@5ef8df1e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2423ms GzipHandler@7889a1ac{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@27cf3151{STOPPED,min=32,inflate=-1} mime types IncludeExclude@127e70c5{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@27cf3151{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5be067de{/executors,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@27cf3151{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@27cf3151{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@27cf3151{STARTING,min=32,inflate=-1} added {DeflaterPool@5910de75{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@27cf3151{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5be067de{/executors,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5be067de{/executors,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7383eae2{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-18245eb0[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-18245eb0==org.apache.spark.ui.JettyUtils$$anon$3@4dad6ef3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-18245eb0=org.apache.spark.ui.JettyUtils$$anon$3-18245eb0==org.apache.spark.ui.JettyUtils$$anon$3@4dad6ef3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@7383eae2{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2428ms ServletHandler@7383eae2{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-18245eb0==org.apache.spark.ui.JettyUtils$$anon$3@4dad6ef3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2428ms org.apache.spark.ui.JettyUtils$$anon$3-18245eb0==org.apache.spark.ui.JettyUtils$$anon$3@4dad6ef3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-18245eb0
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2428ms o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@5910de75{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2429ms DeflaterPool@5910de75{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2429ms GzipHandler@27cf3151{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@4108fa66{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1f130eaf{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4108fa66{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@4108fa66{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@4108fa66{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@4108fa66{STARTING,min=32,inflate=-1} added {DeflaterPool@7e0aadd0{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@4108fa66{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@48c35007{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-72a85671[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-72a85671==org.apache.spark.ui.JettyUtils$$anon$3@cffa8639{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-72a85671=org.apache.spark.ui.JettyUtils$$anon$3-72a85671==org.apache.spark.ui.JettyUtils$$anon$3@cffa8639{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@48c35007{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2433ms ServletHandler@48c35007{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-72a85671==org.apache.spark.ui.JettyUtils$$anon$3@cffa8639{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2434ms org.apache.spark.ui.JettyUtils$$anon$3-72a85671==org.apache.spark.ui.JettyUtils$$anon$3@cffa8639{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-72a85671
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2434ms o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@7e0aadd0{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2434ms DeflaterPool@7e0aadd0{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2434ms GzipHandler@4108fa66{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@21362712{STOPPED,min=32,inflate=-1} mime types IncludeExclude@27eb3298{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@21362712{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@21362712{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@21362712{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@21362712{STARTING,min=32,inflate=-1} added {DeflaterPool@200a26bc{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@21362712{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7a48e6e2{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e==org.apache.spark.ui.JettyUtils$$anon$3@768f7f21{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e=org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e==org.apache.spark.ui.JettyUtils$$anon$3@768f7f21{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@7a48e6e2{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2438ms ServletHandler@7a48e6e2{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e==org.apache.spark.ui.JettyUtils$$anon$3@768f7f21{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2438ms org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e==org.apache.spark.ui.JettyUtils$$anon$3@768f7f21{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-b40bb6e
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2438ms o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@200a26bc{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2438ms DeflaterPool@200a26bc{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2438ms GzipHandler@21362712{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@bc57b40{STOPPED,min=32,inflate=-1} mime types IncludeExclude@1b5bc39d{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@bc57b40{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@bc57b40{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@bc57b40{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@bc57b40{STARTING,min=32,inflate=-1} added {DeflaterPool@655a5d9c{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@bc57b40{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@6d0b5baf{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-631e06ab[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-631e06ab==org.apache.spark.ui.JettyUtils$$anon$3@4d8ed7e4{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-631e06ab=org.apache.spark.ui.JettyUtils$$anon$3-631e06ab==org.apache.spark.ui.JettyUtils$$anon$3@4d8ed7e4{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@6d0b5baf{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2442ms ServletHandler@6d0b5baf{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-631e06ab==org.apache.spark.ui.JettyUtils$$anon$3@4d8ed7e4{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2442ms org.apache.spark.ui.JettyUtils$$anon$3-631e06ab==org.apache.spark.ui.JettyUtils$$anon$3@4d8ed7e4{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-631e06ab
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2442ms o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@655a5d9c{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2442ms DeflaterPool@655a5d9c{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2442ms GzipHandler@bc57b40{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@1494b84d{STOPPED,min=32,inflate=-1} mime types IncludeExclude@34abdee4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@1494b84d{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@346a361{/static,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@1494b84d{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@1494b84d{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@1494b84d{STARTING,min=32,inflate=-1} added {DeflaterPool@71a9b4c7{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@1494b84d{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@346a361{/static,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@346a361{/static,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@107ed6fc{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-bcec031[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-bcec031==org.spark_project.jetty.servlet.DefaultServlet@a56673a0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-bcec031=org.spark_project.jetty.servlet.DefaultServlet-bcec031==org.spark_project.jetty.servlet.DefaultServlet@a56673a0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@107ed6fc{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2445ms ServletHandler@107ed6fc{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-bcec031==org.spark_project.jetty.servlet.DefaultServlet@a56673a0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2445ms org.spark_project.jetty.servlet.DefaultServlet-bcec031==org.spark_project.jetty.servlet.DefaultServlet@a56673a0{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-bcec031
2022-02-10 13:31:58 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-core_2.11/2.4.8/spark-core_2.11-2.4.8.jar!/org/apache/spark/ui/static
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2455ms o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@71a9b4c7{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2455ms DeflaterPool@71a9b4c7{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2455ms GzipHandler@1494b84d{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@5aa0dbf4{STOPPED,min=32,inflate=-1} mime types IncludeExclude@16afbd92{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5aa0dbf4{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@29ef6856{/,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@5aa0dbf4{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5aa0dbf4{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5aa0dbf4{STARTING,min=32,inflate=-1} added {DeflaterPool@2c5d601e{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@5aa0dbf4{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@29ef6856{/,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@29ef6856{/,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@ab7a938{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d==org.apache.spark.ui.JettyUtils$$anon$4@e60ad9c3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d=org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d==org.apache.spark.ui.JettyUtils$$anon$4@e60ad9c3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@ab7a938{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2460ms ServletHandler@ab7a938{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d==org.apache.spark.ui.JettyUtils$$anon$4@e60ad9c3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2460ms org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d==org.apache.spark.ui.JettyUtils$$anon$4@e60ad9c3{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-3faf2e7d
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2460ms o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2c5d601e{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2461ms DeflaterPool@2c5d601e{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2461ms GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@7fe083b1{STOPPED,min=32,inflate=-1} mime types IncludeExclude@23c388c2{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@7fe083b1{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@274872f8{/api,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@7fe083b1{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@7fe083b1{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@7fe083b1{STARTING,min=32,inflate=-1} added {DeflaterPool@486be205{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@7fe083b1{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@274872f8{/api,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@274872f8{/api,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@76ba13c{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-592e843a[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-592e843a=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG ServletHandler:169 - Adding Default404Servlet to ServletHandler@76ba13c{STARTING}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@76ba13c{STARTING} added {org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@76ba13c{STARTING} added {[/]=>org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216,POJO}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-592e843a[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-592e843a=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-592e843a[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@5fa{/*},resource=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=2]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=2]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-592e843a=org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}, org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216=org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@76ba13c{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2466ms ServletHandler@76ba13c{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2466ms org.glassfish.jersey.servlet.ServletContainer-592e843a==org.glassfish.jersey.servlet.ServletContainer@7cd7dbaf{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2467ms org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-4b4dd216==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet@4bc81455{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2467ms o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@486be205{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2467ms DeflaterPool@486be205{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2467ms GzipHandler@7fe083b1{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@5ca17ab0{STOPPED,min=32,inflate=-1} mime types IncludeExclude@5a62b2a4{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5ca17ab0{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@5ca17ab0{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@5ca17ab0{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@5ca17ab0{STARTING,min=32,inflate=-1} added {DeflaterPool@1051817b{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@5ca17ab0{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@4ebea12c{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4==org.apache.spark.ui.JettyUtils$$anon$4@23ca801b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4=org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4==org.apache.spark.ui.JettyUtils$$anon$4@23ca801b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@4ebea12c{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2470ms ServletHandler@4ebea12c{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4==org.apache.spark.ui.JettyUtils$$anon$4@23ca801b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2470ms org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4==org.apache.spark.ui.JettyUtils$$anon$4@23ca801b{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-2a1edad4
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2470ms o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@1051817b{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2470ms DeflaterPool@1051817b{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2470ms GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4a69d1f9{*.svgz},resource=true] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG GzipHandler:208 - GzipHandler@35293c05{STOPPED,min=32,inflate=-1} mime types IncludeExclude@620aa4ea{i=[],ip=CONTAINS,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=CONTAINS}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@35293c05{STOPPED,min=32,inflate=-1} added {o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,STOPPED,@Spark},MANAGED}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STOPPED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {GzipHandler@35293c05{STOPPED,min=32,inflate=-1},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting GzipHandler@35293c05{STOPPED,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - GzipHandler@35293c05{STARTING,min=32,inflate=-1} added {DeflaterPool@2db2dd9d{STOPPED,size=0,capacity=UNLIMITED},AUTO}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting GzipHandler@35293c05{STARTING,min=32,inflate=-1}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@743cb8e0{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$4-c7a975a[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$4-c7a975a==org.apache.spark.ui.JettyUtils$$anon$4@4777ee2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$4-c7a975a=org.apache.spark.ui.JettyUtils$$anon$4-c7a975a==org.apache.spark.ui.JettyUtils$$anon$4@4777ee2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@743cb8e0{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2473ms ServletHandler@743cb8e0{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$4-c7a975a==org.apache.spark.ui.JettyUtils$$anon$4@4777ee2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2473ms org.apache.spark.ui.JettyUtils$$anon$4-c7a975a==org.apache.spark.ui.JettyUtils$$anon$4@4777ee2c{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$4-c7a975a
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2474ms o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting DeflaterPool@2db2dd9d{STOPPED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2474ms DeflaterPool@2db2dd9d{STARTED,size=0,capacity=UNLIMITED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2474ms GzipHandler@35293c05{STARTED,min=32,inflate=-1}
2022-02-10 13:31:58 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://Clairvoyant-324.mshome.net:4040
2022-02-10 13:31:58 INFO  Executor:54 - Starting executor ID driver on host localhost
2022-02-10 13:31:58 DEBUG TransportServer:141 - Shuffle server started on port: 53727
2022-02-10 13:31:58 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53727.
2022-02-10 13:31:58 INFO  NettyBlockTransferService:54 - Server created on Clairvoyant-324.mshome.net:53727
2022-02-10 13:31:58 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2022-02-10 13:31:58 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 53727, None)
2022-02-10 13:31:58 DEBUG DefaultTopologyMapper:58 - Got a request for Clairvoyant-324.mshome.net
2022-02-10 13:31:58 INFO  BlockManagerMasterEndpoint:54 - Registering block manager Clairvoyant-324.mshome.net:53727 with 1970.4 MB RAM, BlockManagerId(driver, Clairvoyant-324.mshome.net, 53727, None)
2022-02-10 13:31:58 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, Clairvoyant-324.mshome.net, 53727, None)
2022-02-10 13:31:58 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, Clairvoyant-324.mshome.net, 53727, None)
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@710d7aff
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@2d7e1102{/,null,STOPPED} added {ServletHandler@65327f5{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@65327f5{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-2adddc06==org.apache.spark.ui.JettyUtils$$anon$3@867a2060{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@65327f5{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-2adddc06,POJO}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@65327f5{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-2adddc06[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-2adddc06==org.apache.spark.ui.JettyUtils$$anon$3@867a2060{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-2adddc06=org.apache.spark.ui.JettyUtils$$anon$3-2adddc06==org.apache.spark.ui.JettyUtils$$anon$3@867a2060{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@65327f5{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2718ms ServletHandler@65327f5{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-2adddc06==org.apache.spark.ui.JettyUtils$$anon$3@867a2060{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2720ms org.apache.spark.ui.JettyUtils$$anon$3-2adddc06==org.apache.spark.ui.JettyUtils$$anon$3@867a2060{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-2adddc06
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2720ms o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG SparkContext:58 - Adding shutdown hook
2022-02-10 13:31:58 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse').
2022-02-10 13:31:58 INFO  SharedState:54 - Warehouse path is 'file:/C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/spark-warehouse'.
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@3b569985
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@3a022576{/,null,STOPPED} added {ServletHandler@2dbd803f{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2dbd803f{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-3e48e859==org.apache.spark.ui.JettyUtils$$anon$3@6d9f65d9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2dbd803f{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-3e48e859,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@31ddd4a4
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1a5f7e7c{/,null,STOPPED} added {ServletHandler@5b22b970{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@5b22b970{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-22d1886d==org.apache.spark.ui.JettyUtils$$anon$3@cd51efe7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@5b22b970{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-22d1886d,POJO}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3a022576{/SQL,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@3a022576{/SQL,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@3a022576{/SQL,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@3a022576{/SQL,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@3a022576{/SQL,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2dbd803f{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-3e48e859[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-3e48e859==org.apache.spark.ui.JettyUtils$$anon$3@6d9f65d9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3e48e859=org.apache.spark.ui.JettyUtils$$anon$3-3e48e859==org.apache.spark.ui.JettyUtils$$anon$3@6d9f65d9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@2dbd803f{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2837ms ServletHandler@2dbd803f{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-3e48e859==org.apache.spark.ui.JettyUtils$$anon$3@6d9f65d9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2837ms org.apache.spark.ui.JettyUtils$$anon$3-3e48e859==org.apache.spark.ui.JettyUtils$$anon$3@6d9f65d9{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-3e48e859
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2838ms o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@5b22b970{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-22d1886d[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-22d1886d==org.apache.spark.ui.JettyUtils$$anon$3@cd51efe7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-22d1886d=org.apache.spark.ui.JettyUtils$$anon$3-22d1886d==org.apache.spark.ui.JettyUtils$$anon$3@cd51efe7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@5b22b970{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2841ms ServletHandler@5b22b970{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-22d1886d==org.apache.spark.ui.JettyUtils$$anon$3@cd51efe7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2841ms org.apache.spark.ui.JettyUtils$$anon$3-22d1886d==org.apache.spark.ui.JettyUtils$$anon$3@cd51efe7{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-22d1886d
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2841ms o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1cbb3d3b
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@529cfee5{/,null,STOPPED} added {ServletHandler@7ca0863b{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7ca0863b{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-319854f0==org.apache.spark.ui.JettyUtils$$anon$3@b74dbcd2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@7ca0863b{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-319854f0,POJO}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@748fe51d
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@415156bf{/,null,STOPPED} added {ServletHandler@393881f0{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@393881f0{STOPPED} added {org.apache.spark.ui.JettyUtils$$anon$3-4af46df3==org.apache.spark.ui.JettyUtils$$anon$3@56ece462{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@393881f0{STOPPED} added {[/]=>org.apache.spark.ui.JettyUtils$$anon$3-4af46df3,POJO}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@7ca0863b{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-319854f0[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-319854f0==org.apache.spark.ui.JettyUtils$$anon$3@b74dbcd2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-319854f0=org.apache.spark.ui.JettyUtils$$anon$3-319854f0==org.apache.spark.ui.JettyUtils$$anon$3@b74dbcd2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@7ca0863b{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2847ms ServletHandler@7ca0863b{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-319854f0==org.apache.spark.ui.JettyUtils$$anon$3@b74dbcd2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2848ms org.apache.spark.ui.JettyUtils$$anon$3-319854f0==org.apache.spark.ui.JettyUtils$$anon$3@b74dbcd2{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-319854f0
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2848ms o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@393881f0{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$3-4af46df3[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.apache.spark.ui.JettyUtils$$anon$3-4af46df3==org.apache.spark.ui.JettyUtils$$anon$3@56ece462{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4af46df3=org.apache.spark.ui.JettyUtils$$anon$3-4af46df3==org.apache.spark.ui.JettyUtils$$anon$3@56ece462{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@393881f0{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2851ms ServletHandler@393881f0{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.apache.spark.ui.JettyUtils$$anon$3-4af46df3==org.apache.spark.ui.JettyUtils$$anon$3@56ece462{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2851ms org.apache.spark.ui.JettyUtils$$anon$3-4af46df3==org.apache.spark.ui.JettyUtils$$anon$3@56ece462{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.apache.spark.ui.JettyUtils$$anon$3-4af46df3
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2851ms o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG DecoratedObjectFactory:53 - Adding Decorator: org.spark_project.jetty.util.DeprecationWarning@1ecfcbc9
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - o.s.j.s.ServletContextHandler@1965539b{/,null,STOPPED} added {ServletHandler@2fc07784{STOPPED},MANAGED}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2fc07784{STOPPED} added {org.spark_project.jetty.servlet.DefaultServlet-353efdbf==org.spark_project.jetty.servlet.DefaultServlet@1e954805{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null},AUTO}
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ServletHandler@2fc07784{STOPPED} added {[/]=>org.spark_project.jetty.servlet.DefaultServlet-353efdbf,POJO}
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - ->[{GzipHandler@5aa0dbf4{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd->[{GzipHandler@aa22f1c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@59d2103b{/storage/rdd,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage->[{GzipHandler@687a762c{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3383649e{/storage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/rdd/json->[{GzipHandler@5922ae77{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@46f699d5{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/execution/json->[{o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@415156bf{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - api->[{GzipHandler@7fe083b1{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@274872f8{/api,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool/json->[{GzipHandler@790174f2{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@427b5f92{/stages/pool/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/pool->[{GzipHandler@3670f00{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5860f3d7{/stages/pool,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/json->[{GzipHandler@51f49060{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@165b2f7f{/jobs/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static->[{GzipHandler@1494b84d{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346a361{/static,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/json->[{GzipHandler@4108fa66{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@24fb6a80{/executors/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/json->[{GzipHandler@3d3f761a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@2c104774{/stages/stage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump/json->[{GzipHandler@bc57b40{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment/json->[{GzipHandler@7889a1ac{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1cd201a8{/environment/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/json->[{GzipHandler@4d63b624{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs->[{GzipHandler@69f63d95{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@15c25153{/jobs,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/json->[{GzipHandler@407cf41{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@6731787b{/stages/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage->[{GzipHandler@60094a13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@73c60324{/stages/stage,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - storage/json->[{GzipHandler@4b629f13{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@346939bf{/storage/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL->[{o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@3a022576{/SQL,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - static/sql->[{o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,STOPPED,@Spark},[o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,STOPPED,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages/stage/kill->[{GzipHandler@35293c05{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@235f4c10{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job->[{GzipHandler@6970140a{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/job,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - environment->[{GzipHandler@71f67a79{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@162be91c{/environment,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - stages->[{GzipHandler@2e185cd7{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@7c56e013{/stages,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors->[{GzipHandler@27cf3151{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@5be067de{/executors,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/json->[{o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@1a5f7e7c{/SQL/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - jobs/job/kill->[{GzipHandler@5ca17ab0{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@1536602f{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - metrics/json->[{o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@2d7e1102{/metrics/json,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - SQL/execution->[{o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark},[o.s.j.s.ServletContextHandler@529cfee5{/SQL/execution,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContextHandlerCollection:157 - executors/threadDump->[{GzipHandler@21362712{STARTED,min=32,inflate=-1},[o.s.j.s.ServletContextHandler@4ae33a11{/executors/threadDump,null,AVAILABLE,@Spark}]}]
2022-02-10 13:31:58 DEBUG ContainerLifeCycle:412 - ContextHandlerCollection@6c67e137{STARTED} added {o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,STOPPED,@Spark},UNMANAGED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,STOPPED,@Spark}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,STARTING,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting ServletHandler@2fc07784{STOPPED}
2022-02-10 13:31:58 DEBUG ServletHandler:1418 - Path=/[EMBEDDED:null] mapped to servlet=org.spark_project.jetty.servlet.DefaultServlet-353efdbf[EMBEDDED:null]
2022-02-10 13:31:58 DEBUG PathMappings:257 - Added MappedResource[pathSpec=ServletPathSpec@4e{/},resource=org.spark_project.jetty.servlet.DefaultServlet-353efdbf==org.spark_project.jetty.servlet.DefaultServlet@1e954805{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}] to PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1439 - filterNameMap={}
2022-02-10 13:31:58 DEBUG ServletHandler:1440 - pathFilters=null
2022-02-10 13:31:58 DEBUG ServletHandler:1441 - servletFilterMap=null
2022-02-10 13:31:58 DEBUG ServletHandler:1442 - servletPathMap=PathMappings[size=1]
2022-02-10 13:31:58 DEBUG ServletHandler:1443 - servletNameMap={org.spark_project.jetty.servlet.DefaultServlet-353efdbf=org.spark_project.jetty.servlet.DefaultServlet-353efdbf==org.spark_project.jetty.servlet.DefaultServlet@1e954805{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}}
2022-02-10 13:31:58 DEBUG AbstractHandler:94 - starting ServletHandler@2fc07784{STARTING}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2857ms ServletHandler@2fc07784{STARTED}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:201 - starting org.spark_project.jetty.servlet.DefaultServlet-353efdbf==org.spark_project.jetty.servlet.DefaultServlet@1e954805{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2857ms org.spark_project.jetty.servlet.DefaultServlet-353efdbf==org.spark_project.jetty.servlet.DefaultServlet@1e954805{jsp=null,order=-1,inst=false,async=true,src=EMBEDDED:null}
2022-02-10 13:31:58 DEBUG ServletHolder:621 - Servlet.init null for org.spark_project.jetty.servlet.DefaultServlet-353efdbf
2022-02-10 13:31:58 DEBUG DefaultServlet:308 - resource base = jar:file:/C:/Users/Anukul%20Thalkar/.m2/repository/org/apache/spark/spark-sql_2.11/2.4.8/spark-sql_2.11-2.4.8.jar!/org/apache/spark/sql/execution/ui/static
2022-02-10 13:31:58 INFO  ContextHandler:916 - Started o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 13:31:58 DEBUG AbstractLifeCycle:191 - STARTED @2874ms o.s.j.s.ServletContextHandler@1965539b{/static/sql,null,AVAILABLE,@Spark}
2022-02-10 13:31:59 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2022-02-10 13:31:59 INFO  InMemoryFileIndex:54 - It took 20 ms to list leaf files for 1 paths.
2022-02-10 13:31:59 INFO  InMemoryFileIndex:54 - It took 3 ms to list leaf files for 2 paths.
2022-02-10 13:32:00 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#0]
 +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#4: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#4: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 13:32:00 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#0
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#0, None)) > 0)
 +- Project [value#0]                       +- Project [value#0]
    +- Relation[value#0] text                  +- Relation[value#0] text
          
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#5: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#5: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#6: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#6: java.lang.String
 +- LocalRelation <empty>, [value#0]                                                                                                                                      +- LocalRelation <empty>, [value#0]
          
2022-02-10 13:32:00 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                    GlobalLimit 1
 +- LocalLimit 1                                  +- LocalLimit 1
    +- Filter (length(trim(value#0, None)) > 0)      +- Filter (length(trim(value#0, None)) > 0)
!      +- Project [value#0]                             +- Relation[value#0] text
!         +- Relation[value#0] text               
          
2022-02-10 13:32:00 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:00 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)
2022-02-10 13:32:00 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:00 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:00 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:00 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:00 INFO  CodeGenerator:54 - Code generated in 163.2939 ms
2022-02-10 13:32:01 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:01 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:01 INFO  CodeGenerator:54 - Code generated in 17.645 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 221.9 KB, free 1970.2 MB)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_0 locally took  31 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_0 without replication took  31 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1970.2 MB)
2022-02-10 13:32:01 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 13:32:01 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_0_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Told master about block broadcast_0_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_0_piece0 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_0_piece0 without replication took  15 ms
2022-02-10 13:32:01 INFO  SparkContext:54 - Created broadcast 0 from load at UseCase1.java:19
2022-02-10 13:32:01 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:01 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Got job 0 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (load at UseCase1.java:19)
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - submitStage(ResultStage 0 (name=load at UseCase1.java:19;jobs=0))
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at UseCase1.java:19), which has no missing parents
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 0)
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 1970.2 MB)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_1 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_1 without replication took  0 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1970.1 MB)
2022-02-10 13:32:01 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.4 MB)
2022-02-10 13:32:01 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_1_piece0 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_1_piece0 without replication took  0 ms
2022-02-10 13:32:01 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:01 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - Epoch for TaskSet 0.0: 0
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 13:32:01 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
2022-02-10 13:32:01 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:01 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Getting local block broadcast_1
2022-02-10 13:32:01 DEBUG BlockManager:58 - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:01 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:01 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:01 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:01 INFO  CodeGenerator:54 - Code generated in 9.629 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Getting local block broadcast_0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:01 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1214 bytes result sent to driver
2022-02-10 13:32:01 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_0.0, runningTasks: 0
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:01 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 110 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:01 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2022-02-10 13:32:01 INFO  DAGScheduler:54 - ResultStage 0 (load at UseCase1.java:19) finished in 0.207 s
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - After removal of stage 0, remaining stages = 0
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Job 0 finished: load at UseCase1.java:19, took 0.230258 s
2022-02-10 13:32:01 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#8: java.lang.String   DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String
 +- Project [value#0]                                                                                                                                                     +- Project [value#0]
    +- Relation[value#0] text                                                                                                                                                +- Relation[value#0] text
          
2022-02-10 13:32:01 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#0 as string).toString, obj#8: java.lang.String   DeserializeToObject value#0.toString, obj#8: java.lang.String
!+- Project [value#0]                                                            +- Relation[value#0] text
!   +- Relation[value#0] text                                                    
          
2022-02-10 13:32:01 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:01 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:01 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:01 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:01 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:01 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:01 INFO  CodeGenerator:54 - Code generated in 9.850999 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 221.9 KB, free 1969.9 MB)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_2 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_2 without replication took  0 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.9 MB)
2022-02-10 13:32:01 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 13:32:01 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_2_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Told master about block broadcast_2_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_2_piece0 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_2_piece0 without replication took  0 ms
2022-02-10 13:32:01 INFO  SparkContext:54 - Created broadcast 2 from load at UseCase1.java:19
2022-02-10 13:32:01 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:01 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:01 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Got job 1 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (load at UseCase1.java:19)
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - submitStage(ResultStage 1 (name=load at UseCase1.java:19;jobs=1))
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at UseCase1.java:19), which has no missing parents
2022-02-10 13:32:01 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 1)
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 13.9 KB, free 1969.9 MB)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_3 locally took  16 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_3 without replication took  16 ms
2022-02-10 13:32:01 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.9 MB)
2022-02-10 13:32:01 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:01 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Told master about block broadcast_3_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Put block broadcast_3_piece0 locally took  0 ms
2022-02-10 13:32:01 DEBUG BlockManager:58 - Putting block broadcast_3_piece0 without replication took  0 ms
2022-02-10 13:32:01 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:01 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:01 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - Epoch for TaskSet 1.0: 0
2022-02-10 13:32:01 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
2022-02-10 13:32:01 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 13:32:01 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:01 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Getting local block broadcast_3
2022-02-10 13:32:01 DEBUG BlockManager:58 - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:01 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:01 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:01 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:01 DEBUG BlockManager:58 - Getting local block broadcast_2
2022-02-10 13:32:01 DEBUG BlockManager:58 - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(13)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 13
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 13
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(20)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 20
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 20
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(28)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 28
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 28
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(6)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 6
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 6
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(23)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 23
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 23
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(11)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 11
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 11
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(9)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 9
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 9
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(12)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 12
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 12
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(17)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 17
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 17
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(27)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 27
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 27
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(24)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 24
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 24
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(18)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 18
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 18
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(19)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 19
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 19
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(10)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 10
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 10
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(25)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 25
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 25
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(30)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 30
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 30
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(21)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 21
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 21
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(26)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 26
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 26
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(29)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 29
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 29
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(14)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 14
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 14
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(8)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 8
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 8
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(1)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning broadcast 1
2022-02-10 13:32:01 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 1
2022-02-10 13:32:01 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 1
2022-02-10 13:32:01 DEBUG BlockManager:58 - Removing broadcast 1
2022-02-10 13:32:01 DEBUG BlockManager:58 - Removing block broadcast_1
2022-02-10 13:32:01 DEBUG MemoryStore:58 - Block broadcast_1 of size 9144 dropped from memory (free 2065590621)
2022-02-10 13:32:01 DEBUG BlockManager:58 - Removing block broadcast_1_piece0
2022-02-10 13:32:01 DEBUG MemoryStore:58 - Block broadcast_1_piece0 of size 4741 dropped from memory (free 2065595362)
2022-02-10 13:32:01 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.4 MB)
2022-02-10 13:32:01 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_1_piece0
2022-02-10 13:32:01 DEBUG BlockManager:58 - Told master about block broadcast_1_piece0
2022-02-10 13:32:01 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 1, response is 0
2022-02-10 13:32:01 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaned broadcast 1
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(7)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 7
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 7
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(15)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 15
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 15
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(22)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 22
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 22
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(16)
2022-02-10 13:32:01 DEBUG ContextCleaner:58 - Cleaning accumulator 16
2022-02-10 13:32:01 INFO  ContextCleaner:54 - Cleaned accumulator 16
2022-02-10 13:32:02 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:02 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 1544 bytes result sent to driver
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_1.0, runningTasks: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 729 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2022-02-10 13:32:02 INFO  DAGScheduler:54 - ResultStage 1 (load at UseCase1.java:19) finished in 0.745 s
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - After removal of stage 1, remaining stages = 0
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Job 1 finished: load at UseCase1.java:19, took 0.744521 s
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(order_id#10 as string), None), unresolvedalias(cast(order_date#11 as string), None), unresolvedalias(cast(order_customer_id#12 as string), None), unresolvedalias(cast(order_status#13 as string), None)]   Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]
 +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                                            +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]   Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]
 +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                          +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                 GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                               +- LocalLimit 21
!   +- Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, cast(order_status#13 as string) AS order_status#24]      +- Project [cast(order_id#10 as string) AS order_id#22, cast(order_date#11 as string) AS order_date#25, cast(order_customer_id#12 as string) AS order_customer_id#23, order_status#13]
       +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv                                                                                                                                                +- Relation[order_id#10,order_date#11,order_customer_id#12,order_status#13] csv
          
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_id: int, order_date: timestamp, order_customer_id: int, order_status: string ... 2 more fields>
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true))), obj#30: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(order_id#22.toString, order_date#25.toString, order_customer_id#23.toString, order_status#24.toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true)), obj#30: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [order_id#22, order_date#25, order_customer_id#23, order_status#24]                                                                                                                                                                                                                                                                                                                                                                 +- LocalRelation <empty>, [order_id#22, order_date#25, order_customer_id#23, order_status#24]
          
2022-02-10 13:32:02 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, StructField(order_id,StringType,true), StructField(order_date,StringType,true), StructField(order_customer_id,StringType,true), StructField(order_status,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[4];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */   }
/* 063 */
/* 064 */
/* 065 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 066 */
/* 067 */     boolean isNull_2 = i.isNullAt(0);
/* 068 */     UTF8String value_2 = isNull_2 ?
/* 069 */     null : (i.getUTF8String(0));
/* 070 */     boolean isNull_1 = true;
/* 071 */     java.lang.String value_1 = null;
/* 072 */     if (!isNull_2) {
/* 073 */
/* 074 */       isNull_1 = false;
/* 075 */       if (!isNull_1) {
/* 076 */
/* 077 */         Object funcResult_0 = null;
/* 078 */         funcResult_0 = value_2.toString();
/* 079 */         value_1 = (java.lang.String) funcResult_0;
/* 080 */
/* 081 */       }
/* 082 */     }
/* 083 */     if (isNull_1) {
/* 084 */       values_0[0] = null;
/* 085 */     } else {
/* 086 */       values_0[0] = value_1;
/* 087 */     }
/* 088 */
/* 089 */     boolean isNull_4 = i.isNullAt(1);
/* 090 */     UTF8String value_4 = isNull_4 ?
/* 091 */     null : (i.getUTF8String(1));
/* 092 */     boolean isNull_3 = true;
/* 093 */     java.lang.String value_3 = null;
/* 094 */     if (!isNull_4) {
/* 095 */
/* 096 */       isNull_3 = false;
/* 097 */       if (!isNull_3) {
/* 098 */
/* 099 */         Object funcResult_1 = null;
/* 100 */         funcResult_1 = value_4.toString();
/* 101 */         value_3 = (java.lang.String) funcResult_1;
/* 102 */
/* 103 */       }
/* 104 */     }
/* 105 */     if (isNull_3) {
/* 106 */       values_0[1] = null;
/* 107 */     } else {
/* 108 */       values_0[1] = value_3;
/* 109 */     }
/* 110 */
/* 111 */     boolean isNull_6 = i.isNullAt(2);
/* 112 */     UTF8String value_6 = isNull_6 ?
/* 113 */     null : (i.getUTF8String(2));
/* 114 */     boolean isNull_5 = true;
/* 115 */     java.lang.String value_5 = null;
/* 116 */     if (!isNull_6) {
/* 117 */
/* 118 */       isNull_5 = false;
/* 119 */       if (!isNull_5) {
/* 120 */
/* 121 */         Object funcResult_2 = null;
/* 122 */         funcResult_2 = value_6.toString();
/* 123 */         value_5 = (java.lang.String) funcResult_2;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_5) {
/* 128 */       values_0[2] = null;
/* 129 */     } else {
/* 130 */       values_0[2] = value_5;
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

2022-02-10 13:32:02 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[4];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */   }
/* 063 */
/* 064 */
/* 065 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 066 */
/* 067 */     boolean isNull_2 = i.isNullAt(0);
/* 068 */     UTF8String value_2 = isNull_2 ?
/* 069 */     null : (i.getUTF8String(0));
/* 070 */     boolean isNull_1 = true;
/* 071 */     java.lang.String value_1 = null;
/* 072 */     if (!isNull_2) {
/* 073 */
/* 074 */       isNull_1 = false;
/* 075 */       if (!isNull_1) {
/* 076 */
/* 077 */         Object funcResult_0 = null;
/* 078 */         funcResult_0 = value_2.toString();
/* 079 */         value_1 = (java.lang.String) funcResult_0;
/* 080 */
/* 081 */       }
/* 082 */     }
/* 083 */     if (isNull_1) {
/* 084 */       values_0[0] = null;
/* 085 */     } else {
/* 086 */       values_0[0] = value_1;
/* 087 */     }
/* 088 */
/* 089 */     boolean isNull_4 = i.isNullAt(1);
/* 090 */     UTF8String value_4 = isNull_4 ?
/* 091 */     null : (i.getUTF8String(1));
/* 092 */     boolean isNull_3 = true;
/* 093 */     java.lang.String value_3 = null;
/* 094 */     if (!isNull_4) {
/* 095 */
/* 096 */       isNull_3 = false;
/* 097 */       if (!isNull_3) {
/* 098 */
/* 099 */         Object funcResult_1 = null;
/* 100 */         funcResult_1 = value_4.toString();
/* 101 */         value_3 = (java.lang.String) funcResult_1;
/* 102 */
/* 103 */       }
/* 104 */     }
/* 105 */     if (isNull_3) {
/* 106 */       values_0[1] = null;
/* 107 */     } else {
/* 108 */       values_0[1] = value_3;
/* 109 */     }
/* 110 */
/* 111 */     boolean isNull_6 = i.isNullAt(2);
/* 112 */     UTF8String value_6 = isNull_6 ?
/* 113 */     null : (i.getUTF8String(2));
/* 114 */     boolean isNull_5 = true;
/* 115 */     java.lang.String value_5 = null;
/* 116 */     if (!isNull_6) {
/* 117 */
/* 118 */       isNull_5 = false;
/* 119 */       if (!isNull_5) {
/* 120 */
/* 121 */         Object funcResult_2 = null;
/* 122 */         funcResult_2 = value_6.toString();
/* 123 */         value_5 = (java.lang.String) funcResult_2;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_5) {
/* 128 */       values_0[2] = null;
/* 129 */     } else {
/* 130 */       values_0[2] = value_5;
/* 131 */     }
/* 132 */
/* 133 */   }
/* 134 */
/* 135 */ }

2022-02-10 13:32:02 INFO  CodeGenerator:54 - Code generated in 10.9079 ms
2022-02-10 13:32:02 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       long scan_value_1 = scan_isNull_1 ?
/* 038 */       -1L : (scan_row_0.getLong(1));
/* 039 */       boolean project_isNull_2 = scan_isNull_1;
/* 040 */       UTF8String project_value_2 = null;
/* 041 */       if (!scan_isNull_1) {
/* 042 */         project_value_2 = UTF8String.fromString(
/* 043 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[1] /* timeZone */)));
/* 044 */       }
/* 045 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 046 */       int scan_value_2 = scan_isNull_2 ?
/* 047 */       -1 : (scan_row_0.getInt(2));
/* 048 */       boolean project_isNull_4 = scan_isNull_2;
/* 049 */       UTF8String project_value_4 = null;
/* 050 */       if (!scan_isNull_2) {
/* 051 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));
/* 052 */       }
/* 053 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 054 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 055 */       null : (scan_row_0.getUTF8String(3));
/* 056 */       project_mutableStateArray_0[0].reset();
/* 057 */
/* 058 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 059 */
/* 060 */       if (project_isNull_0) {
/* 061 */         project_mutableStateArray_0[0].setNullAt(0);
/* 062 */       } else {
/* 063 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 064 */       }
/* 065 */
/* 066 */       if (project_isNull_2) {
/* 067 */         project_mutableStateArray_0[0].setNullAt(1);
/* 068 */       } else {
/* 069 */         project_mutableStateArray_0[0].write(1, project_value_2);
/* 070 */       }
/* 071 */
/* 072 */       if (project_isNull_4) {
/* 073 */         project_mutableStateArray_0[0].setNullAt(2);
/* 074 */       } else {
/* 075 */         project_mutableStateArray_0[0].write(2, project_value_4);
/* 076 */       }
/* 077 */
/* 078 */       if (scan_isNull_3) {
/* 079 */         project_mutableStateArray_0[0].setNullAt(3);
/* 080 */       } else {
/* 081 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 082 */       }
/* 083 */       append((project_mutableStateArray_0[0].getRow()));
/* 084 */       if (shouldStop()) return;
/* 085 */     }
/* 086 */   }
/* 087 */
/* 088 */ }

2022-02-10 13:32:02 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       long scan_value_1 = scan_isNull_1 ?
/* 038 */       -1L : (scan_row_0.getLong(1));
/* 039 */       boolean project_isNull_2 = scan_isNull_1;
/* 040 */       UTF8String project_value_2 = null;
/* 041 */       if (!scan_isNull_1) {
/* 042 */         project_value_2 = UTF8String.fromString(
/* 043 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[1] /* timeZone */)));
/* 044 */       }
/* 045 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 046 */       int scan_value_2 = scan_isNull_2 ?
/* 047 */       -1 : (scan_row_0.getInt(2));
/* 048 */       boolean project_isNull_4 = scan_isNull_2;
/* 049 */       UTF8String project_value_4 = null;
/* 050 */       if (!scan_isNull_2) {
/* 051 */         project_value_4 = UTF8String.fromString(String.valueOf(scan_value_2));
/* 052 */       }
/* 053 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 054 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 055 */       null : (scan_row_0.getUTF8String(3));
/* 056 */       project_mutableStateArray_0[0].reset();
/* 057 */
/* 058 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 059 */
/* 060 */       if (project_isNull_0) {
/* 061 */         project_mutableStateArray_0[0].setNullAt(0);
/* 062 */       } else {
/* 063 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 064 */       }
/* 065 */
/* 066 */       if (project_isNull_2) {
/* 067 */         project_mutableStateArray_0[0].setNullAt(1);
/* 068 */       } else {
/* 069 */         project_mutableStateArray_0[0].write(1, project_value_2);
/* 070 */       }
/* 071 */
/* 072 */       if (project_isNull_4) {
/* 073 */         project_mutableStateArray_0[0].setNullAt(2);
/* 074 */       } else {
/* 075 */         project_mutableStateArray_0[0].write(2, project_value_4);
/* 076 */       }
/* 077 */
/* 078 */       if (scan_isNull_3) {
/* 079 */         project_mutableStateArray_0[0].setNullAt(3);
/* 080 */       } else {
/* 081 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 082 */       }
/* 083 */       append((project_mutableStateArray_0[0].getRow()));
/* 084 */       if (shouldStop()) return;
/* 085 */     }
/* 086 */   }
/* 087 */
/* 088 */ }

2022-02-10 13:32:02 INFO  CodeGenerator:54 - Code generated in 15.9509 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 221.8 KB, free 1969.7 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_4 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_4 without replication took  0 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.7 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_4_piece0 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_4_piece0 without replication took  0 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 4 from show at UseCase2.java:65
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:02 INFO  SparkContext:54 - Starting job: show at UseCase2.java:65
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Got job 2 (show at UseCase2.java:65) with 1 output partitions
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (show at UseCase2.java:65)
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitStage(ResultStage 2 (name=show at UseCase2.java:65;jobs=2))
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[13] at show at UseCase2.java:65), which has no missing parents
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 2)
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 12.4 KB, free 1969.7 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_5 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_5 without replication took  0 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1969.7 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_5_piece0 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_5_piece0 without replication took  0 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at show at UseCase2.java:65) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Epoch for TaskSet 2.0: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 2.0: NO_PREF, ANY
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 13:32:02 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 2)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_5
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:02 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true],input[3, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */
/* 058 */     boolean isNull_3 = i.isNullAt(3);
/* 059 */     UTF8String value_3 = isNull_3 ?
/* 060 */     null : (i.getUTF8String(3));
/* 061 */     if (isNull_3) {
/* 062 */       mutableStateArray_0[0].setNullAt(3);
/* 063 */     } else {
/* 064 */       mutableStateArray_0[0].write(3, value_3);
/* 065 */     }
/* 066 */     return (mutableStateArray_0[0].getRow());
/* 067 */   }
/* 068 */
/* 069 */
/* 070 */ }

2022-02-10 13:32:02 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */
/* 058 */     boolean isNull_3 = i.isNullAt(3);
/* 059 */     UTF8String value_3 = isNull_3 ?
/* 060 */     null : (i.getUTF8String(3));
/* 061 */     if (isNull_3) {
/* 062 */       mutableStateArray_0[0].setNullAt(3);
/* 063 */     } else {
/* 064 */       mutableStateArray_0[0].write(3, value_3);
/* 065 */     }
/* 066 */     return (mutableStateArray_0[0].getRow());
/* 067 */   }
/* 068 */
/* 069 */
/* 070 */ }

2022-02-10 13:32:02 INFO  CodeGenerator:54 - Code generated in 8.9615 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_4
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 2). 1795 bytes result sent to driver
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_2.0, runningTasks: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 47 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2022-02-10 13:32:02 INFO  DAGScheduler:54 - ResultStage 2 (show at UseCase2.java:65) finished in 0.047 s
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - After removal of stage 2, remaining stages = 0
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Job 2 finished: show at UseCase2.java:65, took 0.049475 s
2022-02-10 13:32:02 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:02 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 13:32:02 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 13:32:02 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#35
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#35]
 +- Relation[value#35] text                 +- Relation[value#35] text
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#39: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#39: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 13:32:02 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#35
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#35, None)) > 0)
 +- Project [value#35]                      +- Project [value#35]
    +- Relation[value#35] text                 +- Relation[value#35] text
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#40: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#40: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#41: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#41: java.lang.String
 +- LocalRelation <empty>, [value#35]                                                                                                                                      +- LocalRelation <empty>, [value#35]
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#35, None)) > 0)      +- Filter (length(trim(value#35, None)) > 0)
!      +- Project [value#35]                             +- Relation[value#35] text
!         +- Relation[value#35] text               
          
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#35, None)) > 0)
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:02 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:02 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 221.9 KB, free 1969.4 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_6 locally took  16 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_6 without replication took  16 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_6_piece0 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_6_piece0 without replication took  0 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 6 from load at UseCase1.java:19
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:02 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Got job 3 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (load at UseCase1.java:19)
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitStage(ResultStage 3 (name=load at UseCase1.java:19;jobs=3))
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[17] at load at UseCase1.java:19), which has no missing parents
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 3)
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_7 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_7 without replication took  0 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_7_piece0 locally took  16 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_7_piece0 without replication took  16 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 1 tasks
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Epoch for TaskSet 3.0: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 3.0: NO_PREF, ANY
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:02 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_7
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:02 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_6
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 1128 bytes result sent to driver
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_3.0, runningTasks: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2022-02-10 13:32:02 INFO  DAGScheduler:54 - ResultStage 3 (load at UseCase1.java:19) finished in 0.016 s
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - After removal of stage 3, remaining stages = 0
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Job 3 finished: load at UseCase1.java:19, took 0.021028 s
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#43: java.lang.String   DeserializeToObject cast(value#35 as string).toString, obj#43: java.lang.String
 +- Project [value#35]                                                                                                                                                     +- Project [value#35]
    +- Relation[value#35] text                                                                                                                                                +- Relation[value#35] text
          
2022-02-10 13:32:02 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#35 as string).toString, obj#43: java.lang.String   DeserializeToObject value#35.toString, obj#43: java.lang.String
!+- Project [value#35]                                                             +- Relation[value#35] text
!   +- Relation[value#35] text                                                     
          
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:02 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:02 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_8 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_8 without replication took  0 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_8_piece0 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_8_piece0 without replication took  0 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 8 from load at UseCase1.java:19
2022-02-10 13:32:02 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:02 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:02 INFO  SparkContext:54 - Starting job: load at UseCase1.java:19
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Got job 4 (load at UseCase1.java:19) with 1 output partitions
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (load at UseCase1.java:19)
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitStage(ResultStage 4 (name=load at UseCase1.java:19;jobs=4))
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting ResultStage 4 (MapPartitionsRDD[23] at load at UseCase1.java:19), which has no missing parents
2022-02-10 13:32:02 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 4)
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 13.9 KB, free 1969.1 MB)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_9 locally took  16 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_9 without replication took  16 ms
2022-02-10 13:32:02 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.1 MB)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Put block broadcast_9_piece0 locally took  0 ms
2022-02-10 13:32:02 DEBUG BlockManager:58 - Putting block broadcast_9_piece0 without replication took  0 ms
2022-02-10 13:32:02 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:02 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at load at UseCase1.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:02 INFO  TaskSchedulerImpl:54 - Adding task set 4.0 with 1 tasks
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Epoch for TaskSet 4.0: 0
2022-02-10 13:32:02 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
2022-02-10 13:32:02 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 13:32:02 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:02 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 4)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_9
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_9 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:02 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:02 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:02 DEBUG BlockManager:58 - Getting local block broadcast_8
2022-02-10 13:32:02 DEBUG BlockManager:58 - Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(120)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 120
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 120
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(99)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 99
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 99
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(4)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning broadcast 4
2022-02-10 13:32:02 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 4
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 4
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing broadcast 4
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_4_piece0
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_4_piece0 of size 21151 dropped from memory (free 2064816305)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_4_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_4_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_4
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_4 of size 227072 dropped from memory (free 2065043377)
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 4, response is 0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaned broadcast 4
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(63)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 63
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 63
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(118)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 118
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 118
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(92)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 92
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 92
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(94)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 94
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 94
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(102)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 102
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 102
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(70)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 70
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 70
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(76)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 76
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 76
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(83)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 83
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 83
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(110)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 110
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 110
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(108)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 108
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 108
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(65)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 65
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 65
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(60)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 60
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 60
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(119)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 119
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 119
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(52)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 52
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 52
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(72)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 72
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 72
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(75)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 75
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 75
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(96)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 96
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 96
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(103)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 103
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 103
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(107)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 107
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 107
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(36)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 36
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 36
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(86)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 86
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 86
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(51)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 51
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 51
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(114)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 114
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 114
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(46)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 46
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 46
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(88)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 88
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 88
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(40)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 40
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 40
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(95)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 95
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 95
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(93)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 93
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 93
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(105)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 105
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 105
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(109)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 109
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 109
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(47)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 47
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 47
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(6)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning broadcast 6
2022-02-10 13:32:02 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 6
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 6
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing broadcast 6
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_6_piece0
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_6_piece0 of size 21165 dropped from memory (free 2065064542)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_6_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_6_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_6
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_6 of size 227232 dropped from memory (free 2065291774)
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 6, response is 0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaned broadcast 6
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(104)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 104
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 104
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(77)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 77
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 77
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(3)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning broadcast 3
2022-02-10 13:32:02 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 3
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 3
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing broadcast 3
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_3_piece0
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_3_piece0 of size 7738 dropped from memory (free 2065299512)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_3_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_3_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_3
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_3 of size 14256 dropped from memory (free 2065313768)
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 3, response is 0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaned broadcast 3
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(79)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 79
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 79
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(100)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 100
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 100
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(91)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 91
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 91
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(39)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 39
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 39
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(80)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 80
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 80
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(112)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 112
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 112
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(64)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 64
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 64
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(121)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 121
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 121
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(43)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 43
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 43
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(117)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 117
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 117
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(84)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 84
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 84
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(55)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 55
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 55
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(97)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 97
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 97
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(38)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 38
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 38
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(89)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 89
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 89
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(71)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 71
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 71
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(44)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 44
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 44
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(59)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 59
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 59
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(113)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 113
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 113
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(116)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 116
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 116
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(111)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 111
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 111
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(73)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 73
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 73
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(50)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 50
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 50
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(48)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 48
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 48
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(82)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 82
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 82
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(101)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 101
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 101
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(41)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 41
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 41
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(5)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning broadcast 5
2022-02-10 13:32:02 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 5
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 5
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing broadcast 5
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_5
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_5 of size 12712 dropped from memory (free 2065326480)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_5_piece0
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_5_piece0 of size 6597 dropped from memory (free 2065333077)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_5_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_5_piece0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 5, response is 0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaned broadcast 5
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(66)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 66
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 66
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(115)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 115
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 115
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(7)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning broadcast 7
2022-02-10 13:32:02 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 7
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 7
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing broadcast 7
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_7
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_7 of size 9144 dropped from memory (free 2065342221)
2022-02-10 13:32:02 DEBUG BlockManager:58 - Removing block broadcast_7_piece0
2022-02-10 13:32:02 DEBUG MemoryStore:58 - Block broadcast_7_piece0 of size 4742 dropped from memory (free 2065346963)
2022-02-10 13:32:02 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:02 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_7_piece0
2022-02-10 13:32:02 DEBUG BlockManager:58 - Told master about block broadcast_7_piece0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 7, response is 0
2022-02-10 13:32:02 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaned broadcast 7
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(58)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 58
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 58
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(49)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 49
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 49
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(90)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 90
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 90
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(106)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 106
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 106
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(62)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 62
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 62
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(57)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 57
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 57
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(78)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 78
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 78
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(61)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 61
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 61
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(37)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 37
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 37
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(42)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 42
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 42
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(87)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 87
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 87
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(56)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 56
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 56
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(85)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 85
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 85
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(53)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 53
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 53
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(67)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 67
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 67
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(74)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 74
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 74
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(68)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 68
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 68
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(81)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 81
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 81
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(69)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 69
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 69
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(45)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 45
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 45
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(54)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 54
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 54
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(98)
2022-02-10 13:32:02 DEBUG ContextCleaner:58 - Cleaning accumulator 98
2022-02-10 13:32:02 INFO  ContextCleaner:54 - Cleaned accumulator 98
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 4). 1544 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_4.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 596 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 4 (load at UseCase1.java:19) finished in 0.612 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 4, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 4 finished: load at UseCase1.java:19, took 0.611752 s
2022-02-10 13:32:03 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#53
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#53]
 +- Relation[value#53] text                 +- Relation[value#53] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#57: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#57: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#53
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#53, None)) > 0)
 +- Project [value#53]                      +- Project [value#53]
    +- Relation[value#53] text                 +- Relation[value#53] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#58: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#58: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#59: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#59: java.lang.String
 +- LocalRelation <empty>, [value#53]                                                                                                                                      +- LocalRelation <empty>, [value#53]
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                     GlobalLimit 1
 +- LocalLimit 1                                   +- LocalLimit 1
    +- Filter (length(trim(value#53, None)) > 0)      +- Filter (length(trim(value#53, None)) > 0)
!      +- Project [value#53]                             +- Relation[value#53] text
!         +- Relation[value#53] text               
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#53, None)) > 0)
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_10 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_10 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_10_piece0 locally took  16 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_10_piece0 without replication took  16 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 10 from load at UseCase1.java:24
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase1.java:24
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 5 (load at UseCase1.java:24) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (load at UseCase1.java:24)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 5 (name=load at UseCase1.java:24;jobs=5))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[27] at load at UseCase1.java:24), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 5)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_11 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_11 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_11_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_11_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[27] at load at UseCase1.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 5.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 5.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 5.0 (TID 5)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_11
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_11 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_10
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_10 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 5.0 (TID 5). 1162 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_5.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 5 (load at UseCase1.java:24) finished in 0.016 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 5, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 5 finished: load at UseCase1.java:24, took 0.017262 s
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#61: java.lang.String   DeserializeToObject cast(value#53 as string).toString, obj#61: java.lang.String
 +- Project [value#53]                                                                                                                                                     +- Project [value#53]
    +- Relation[value#53] text                                                                                                                                                +- Relation[value#53] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#53 as string).toString, obj#61: java.lang.String   DeserializeToObject value#53.toString, obj#61: java.lang.String
!+- Project [value#53]                                                             +- Relation[value#53] text
!   +- Relation[value#53] text                                                     
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_12 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_12 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_12_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_12_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 12 from load at UseCase1.java:24
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase1.java:24
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 6 (load at UseCase1.java:24) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (load at UseCase1.java:24)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 6 (name=load at UseCase1.java:24;jobs=6))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 6 (MapPartitionsRDD[33] at load at UseCase1.java:24), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 6)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_13 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_13 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_13_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_13_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at load at UseCase1.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 6.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 6.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 6.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 6.0 (TID 6)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_13
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_13 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_12
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_12 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 6.0 (TID 6). 1473 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_6.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 63 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 6 (load at UseCase1.java:24) finished in 0.063 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 6, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 6 finished: load at UseCase1.java:24, took 0.066344 s
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(customer_id#63 as string), None), unresolvedalias(cast(customer_fname#64 as string), None), unresolvedalias(cast(customer_lname#65 as string), None), unresolvedalias(cast(customer_email#66 as string), None), unresolvedalias(cast(customer_password#67 as string), None), unresolvedalias(cast(customer_street#68 as string), None), unresolvedalias(cast(customer_city#69 as string), None), unresolvedalias(cast(customer_state#70 as string), None), unresolvedalias(cast(customer_zipcode#71 as string), None)]   Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
 +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]   Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
 +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                   +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             +- LocalLimit 21
!   +- Project [cast(customer_id#63 as string) AS customer_id#90, cast(customer_fname#64 as string) AS customer_fname#91, cast(customer_lname#65 as string) AS customer_lname#92, cast(customer_email#66 as string) AS customer_email#93, cast(customer_password#67 as string) AS customer_password#94, cast(customer_street#68 as string) AS customer_street#95, cast(customer_city#69 as string) AS customer_city#96, cast(customer_state#70 as string) AS customer_state#97, cast(customer_zipcode#71 as string) AS customer_zipcode#98]      +- Project [cast(customer_id#63 as string) AS customer_id#90, customer_fname#64, customer_lname#65, customer_email#66, customer_password#67, customer_street#68, customer_city#69, customer_state#70, cast(customer_zipcode#71 as string) AS customer_zipcode#98]
       +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv                                                                                                                                                                                                                                                                                                                                                         +- Relation[customer_id#63,customer_fname#64,customer_lname#65,customer_email#66,customer_password#67,customer_street#68,customer_city#69,customer_state#70,customer_zipcode#71] csv
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int, customer_fname: string, customer_lname: string, customer_email: string, customer_password: string ... 7 more fields>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true))), obj#108: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(customer_id#90.toString, customer_fname#91.toString, customer_lname#92.toString, customer_email#93.toString, customer_password#94.toString, customer_street#95.toString, customer_city#96.toString, customer_state#97.toString, customer_zipcode#98.toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)), obj#108: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [customer_id#90, customer_fname#91, customer_lname#92, customer_email#93, customer_password#94, customer_street#95, customer_city#96, customer_state#97, customer_zipcode#98]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- LocalRelation <empty>, [customer_id#90, customer_fname#91, customer_lname#92, customer_email#93, customer_password#94, customer_street#95, customer_city#96, customer_state#97, customer_zipcode#98]
          
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, true].toString, input[7, string, true].toString, input[8, string, true].toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 13:32:03 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 13:32:03 INFO  CodeGenerator:54 - Code generated in 13.3487 ms
2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       UTF8String scan_value_1 = scan_isNull_1 ?
/* 038 */       null : (scan_row_0.getUTF8String(1));
/* 039 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 040 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 041 */       null : (scan_row_0.getUTF8String(2));
/* 042 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 043 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 044 */       null : (scan_row_0.getUTF8String(3));
/* 045 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 046 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 047 */       null : (scan_row_0.getUTF8String(4));
/* 048 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 049 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 050 */       null : (scan_row_0.getUTF8String(5));
/* 051 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 052 */       UTF8String scan_value_6 = scan_isNull_6 ?
/* 053 */       null : (scan_row_0.getUTF8String(6));
/* 054 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 055 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 056 */       null : (scan_row_0.getUTF8String(7));
/* 057 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 058 */       int scan_value_8 = scan_isNull_8 ?
/* 059 */       -1 : (scan_row_0.getInt(8));
/* 060 */       boolean project_isNull_9 = scan_isNull_8;
/* 061 */       UTF8String project_value_9 = null;
/* 062 */       if (!scan_isNull_8) {
/* 063 */         project_value_9 = UTF8String.fromString(String.valueOf(scan_value_8));
/* 064 */       }
/* 065 */       project_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       if (project_isNull_0) {
/* 070 */         project_mutableStateArray_0[0].setNullAt(0);
/* 071 */       } else {
/* 072 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 073 */       }
/* 074 */
/* 075 */       if (scan_isNull_1) {
/* 076 */         project_mutableStateArray_0[0].setNullAt(1);
/* 077 */       } else {
/* 078 */         project_mutableStateArray_0[0].write(1, scan_value_1);
/* 079 */       }
/* 080 */
/* 081 */       if (scan_isNull_2) {
/* 082 */         project_mutableStateArray_0[0].setNullAt(2);
/* 083 */       } else {
/* 084 */         project_mutableStateArray_0[0].write(2, scan_value_2);
/* 085 */       }
/* 086 */
/* 087 */       if (scan_isNull_3) {
/* 088 */         project_mutableStateArray_0[0].setNullAt(3);
/* 089 */       } else {
/* 090 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 091 */       }
/* 092 */
/* 093 */       if (scan_isNull_4) {
/* 094 */         project_mutableStateArray_0[0].setNullAt(4);
/* 095 */       } else {
/* 096 */         project_mutableStateArray_0[0].write(4, scan_value_4);
/* 097 */       }
/* 098 */
/* 099 */       if (scan_isNull_5) {
/* 100 */         project_mutableStateArray_0[0].setNullAt(5);
/* 101 */       } else {
/* 102 */         project_mutableStateArray_0[0].write(5, scan_value_5);
/* 103 */       }
/* 104 */
/* 105 */       if (scan_isNull_6) {
/* 106 */         project_mutableStateArray_0[0].setNullAt(6);
/* 107 */       } else {
/* 108 */         project_mutableStateArray_0[0].write(6, scan_value_6);
/* 109 */       }
/* 110 */
/* 111 */       if (scan_isNull_7) {
/* 112 */         project_mutableStateArray_0[0].setNullAt(7);
/* 113 */       } else {
/* 114 */         project_mutableStateArray_0[0].write(7, scan_value_7);
/* 115 */       }
/* 116 */
/* 117 */       if (project_isNull_9) {
/* 118 */         project_mutableStateArray_0[0].setNullAt(8);
/* 119 */       } else {
/* 120 */         project_mutableStateArray_0[0].write(8, project_value_9);
/* 121 */       }
/* 122 */       append((project_mutableStateArray_0[0].getRow()));
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */   }
/* 126 */
/* 127 */ }

2022-02-10 13:32:03 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 029 */       int scan_value_0 = scan_isNull_0 ?
/* 030 */       -1 : (scan_row_0.getInt(0));
/* 031 */       boolean project_isNull_0 = scan_isNull_0;
/* 032 */       UTF8String project_value_0 = null;
/* 033 */       if (!scan_isNull_0) {
/* 034 */         project_value_0 = UTF8String.fromString(String.valueOf(scan_value_0));
/* 035 */       }
/* 036 */       boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 037 */       UTF8String scan_value_1 = scan_isNull_1 ?
/* 038 */       null : (scan_row_0.getUTF8String(1));
/* 039 */       boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 040 */       UTF8String scan_value_2 = scan_isNull_2 ?
/* 041 */       null : (scan_row_0.getUTF8String(2));
/* 042 */       boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 043 */       UTF8String scan_value_3 = scan_isNull_3 ?
/* 044 */       null : (scan_row_0.getUTF8String(3));
/* 045 */       boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 046 */       UTF8String scan_value_4 = scan_isNull_4 ?
/* 047 */       null : (scan_row_0.getUTF8String(4));
/* 048 */       boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 049 */       UTF8String scan_value_5 = scan_isNull_5 ?
/* 050 */       null : (scan_row_0.getUTF8String(5));
/* 051 */       boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 052 */       UTF8String scan_value_6 = scan_isNull_6 ?
/* 053 */       null : (scan_row_0.getUTF8String(6));
/* 054 */       boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 055 */       UTF8String scan_value_7 = scan_isNull_7 ?
/* 056 */       null : (scan_row_0.getUTF8String(7));
/* 057 */       boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 058 */       int scan_value_8 = scan_isNull_8 ?
/* 059 */       -1 : (scan_row_0.getInt(8));
/* 060 */       boolean project_isNull_9 = scan_isNull_8;
/* 061 */       UTF8String project_value_9 = null;
/* 062 */       if (!scan_isNull_8) {
/* 063 */         project_value_9 = UTF8String.fromString(String.valueOf(scan_value_8));
/* 064 */       }
/* 065 */       project_mutableStateArray_0[0].reset();
/* 066 */
/* 067 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 068 */
/* 069 */       if (project_isNull_0) {
/* 070 */         project_mutableStateArray_0[0].setNullAt(0);
/* 071 */       } else {
/* 072 */         project_mutableStateArray_0[0].write(0, project_value_0);
/* 073 */       }
/* 074 */
/* 075 */       if (scan_isNull_1) {
/* 076 */         project_mutableStateArray_0[0].setNullAt(1);
/* 077 */       } else {
/* 078 */         project_mutableStateArray_0[0].write(1, scan_value_1);
/* 079 */       }
/* 080 */
/* 081 */       if (scan_isNull_2) {
/* 082 */         project_mutableStateArray_0[0].setNullAt(2);
/* 083 */       } else {
/* 084 */         project_mutableStateArray_0[0].write(2, scan_value_2);
/* 085 */       }
/* 086 */
/* 087 */       if (scan_isNull_3) {
/* 088 */         project_mutableStateArray_0[0].setNullAt(3);
/* 089 */       } else {
/* 090 */         project_mutableStateArray_0[0].write(3, scan_value_3);
/* 091 */       }
/* 092 */
/* 093 */       if (scan_isNull_4) {
/* 094 */         project_mutableStateArray_0[0].setNullAt(4);
/* 095 */       } else {
/* 096 */         project_mutableStateArray_0[0].write(4, scan_value_4);
/* 097 */       }
/* 098 */
/* 099 */       if (scan_isNull_5) {
/* 100 */         project_mutableStateArray_0[0].setNullAt(5);
/* 101 */       } else {
/* 102 */         project_mutableStateArray_0[0].write(5, scan_value_5);
/* 103 */       }
/* 104 */
/* 105 */       if (scan_isNull_6) {
/* 106 */         project_mutableStateArray_0[0].setNullAt(6);
/* 107 */       } else {
/* 108 */         project_mutableStateArray_0[0].write(6, scan_value_6);
/* 109 */       }
/* 110 */
/* 111 */       if (scan_isNull_7) {
/* 112 */         project_mutableStateArray_0[0].setNullAt(7);
/* 113 */       } else {
/* 114 */         project_mutableStateArray_0[0].write(7, scan_value_7);
/* 115 */       }
/* 116 */
/* 117 */       if (project_isNull_9) {
/* 118 */         project_mutableStateArray_0[0].setNullAt(8);
/* 119 */       } else {
/* 120 */         project_mutableStateArray_0[0].write(8, project_value_9);
/* 121 */       }
/* 122 */       append((project_mutableStateArray_0[0].getRow()));
/* 123 */       if (shouldStop()) return;
/* 124 */     }
/* 125 */   }
/* 126 */
/* 127 */ }

2022-02-10 13:32:03 INFO  CodeGenerator:54 - Code generated in 15.366299 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 221.8 KB, free 1968.9 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_14 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_14 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_14_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_14_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 14 from show at UseCase2.java:67
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: show at UseCase2.java:67
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 7 (show at UseCase2.java:67) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (show at UseCase2.java:67)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 7 (name=show at UseCase2.java:67;jobs=7))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 7 (MapPartitionsRDD[37] at show at UseCase2.java:67), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 7)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 13.9 KB, free 1968.9 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_15 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_15 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1968.9 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 6.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_15_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_15_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[37] at show at UseCase2.java:67) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 7.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 7.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 7.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 7.0 (TID 7)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_15
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_15 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 13:32:03 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 13:32:03 INFO  CodeGenerator:54 - Code generated in 13.9953 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_14
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_14 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 7.0 (TID 7). 2998 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_7.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(197)
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 36 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 197
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 197
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(147)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 147
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 147
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(144)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 144
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 144
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(204)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 7 (show at UseCase2.java:67) finished in 0.036 s
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 204
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 204
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(188)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 188
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 7, remaining stages = 0
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 188
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(10)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 10
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 10
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 7 finished: show at UseCase2.java:67, took 0.046617 s
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 10
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 10
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_10
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_10 of size 227232 dropped from memory (free 2064772105)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_10_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_10_piece0 of size 21165 dropped from memory (free 2064793270)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_10_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_10_piece0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 10, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 10
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(133)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 133
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 133
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(194)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 194
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 194
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(134)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 134
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 134
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(176)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 176
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 176
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(13)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 13
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 13
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 13
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 13
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_13_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_13_piece0 of size 7782 dropped from memory (free 2064801052)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_13_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_13_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_13
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_13 of size 14368 dropped from memory (free 2064815420)
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 13, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 13
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(183)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 183
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 183
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(128)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 128
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 128
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(201)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 201
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 201
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(210)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 210
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 210
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(138)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 138
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 138
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(160)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 160
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 160
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(8)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 8
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 8
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 8
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 8
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_8
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_8 of size 227232 dropped from memory (free 2065042652)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_8_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_8_piece0 of size 21165 dropped from memory (free 2065063817)
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_8_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_8_piece0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 8, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 8
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(127)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 127
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#118
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 127
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(190)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 190
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 190
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(198)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 198
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 198
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(186)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 186
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 186
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(205)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 205
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 205
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(158)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 158
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 158
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(171)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 171
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 171
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(122)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 122
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 122
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(200)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 200
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 200
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#118]
 +- Relation[value#118] text                +- Relation[value#118] text
          
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(180)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 180
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 180
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(211)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 211
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 211
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(159)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 159
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 159
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(208)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 208
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 208
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(179)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 179
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 179
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(161)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 161
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 161
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(132)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 132
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 132
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(168)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 168
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 168
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(173)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 173
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 173
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(139)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 139
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 139
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(153)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 153
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 153
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(189)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 189
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 189
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(193)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 193
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 193
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(12)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 12
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 12
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 12
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 12
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_12_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_12_piece0 of size 21165 dropped from memory (free 2065084982)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_12_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_12_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_12
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_12 of size 227232 dropped from memory (free 2065312214)
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 12, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 12
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(146)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 146
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 146
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(212)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 212
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 212
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(145)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 145
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 145
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(185)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 185
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 185
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(126)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 126
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 126
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(202)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 202
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 202
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(157)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 157
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 157
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(143)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 143
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 143
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(154)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 154
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 154
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(191)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 191
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 191
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(199)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 199
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 199
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(130)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 130
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 130
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(170)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 170
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 170
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(206)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 206
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 206
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(156)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 156
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 156
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(175)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 175
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 175
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(11)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 11
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 11
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 11
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 11
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_11
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#122: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#122: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_11 of size 9144 dropped from memory (free 2065321358)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_11_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_11_piece0 of size 4742 dropped from memory (free 2065326100)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_11_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_11_piece0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 11, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 11
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(125)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 125
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 125
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(131)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 131
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 131
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(137)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 137
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 137
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(182)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 182
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 182
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(196)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 196
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#118
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 196
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(165)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 165
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 165
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(124)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 124
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 124
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(129)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 129
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 129
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(174)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 174
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 174
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(9)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning broadcast 9
2022-02-10 13:32:03 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 9
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 9
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing broadcast 9
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_9_piece0
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_9_piece0 of size 7740 dropped from memory (free 2065333840)
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#118, None)) > 0)
 +- Project [value#118]                     +- Project [value#118]
    +- Relation[value#118] text                +- Relation[value#118] text
          
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_9_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_9_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Removing block broadcast_9
2022-02-10 13:32:03 DEBUG MemoryStore:58 - Block broadcast_9 of size 14256 dropped from memory (free 2065348096)
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 9, response is 0
2022-02-10 13:32:03 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaned broadcast 9
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(178)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 178
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 178
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(172)
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#123: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#123: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 172
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 172
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(169)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 169
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 169
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(136)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 136
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 136
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(209)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 209
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 209
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(150)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 150
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 150
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(162)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 162
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 162
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(195)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 195
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 195
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(149)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 149
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 149
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(163)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 163
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 163
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(203)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 203
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 203
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(152)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 152
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 152
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(164)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 164
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 164
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(187)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 187
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 187
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(192)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 192
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 192
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(167)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 167
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 167
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(141)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 141
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 141
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(207)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 207
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 207
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(123)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 123
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 123
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(177)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 177
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 177
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(135)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 135
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 135
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(142)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 142
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 142
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(148)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 148
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 148
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(155)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 155
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 155
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(140)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 140
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 140
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(184)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 184
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 184
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(151)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 151
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#124: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#124: java.lang.String
 +- LocalRelation <empty>, [value#118]                                                                                                                                      +- LocalRelation <empty>, [value#118]
          
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 151
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(181)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 181
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 181
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(166)
2022-02-10 13:32:03 DEBUG ContextCleaner:58 - Cleaning accumulator 166
2022-02-10 13:32:03 INFO  ContextCleaner:54 - Cleaned accumulator 166
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#118, None)) > 0)      +- Filter (length(trim(value#118, None)) > 0)
!      +- Project [value#118]                             +- Relation[value#118] text
!         +- Relation[value#118] text               
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#118, None)) > 0)
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_16 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_16 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_16_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_16_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 16 from load at UseCase1.java:24
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase1.java:24
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 8 (load at UseCase1.java:24) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (load at UseCase1.java:24)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 8 (name=load at UseCase1.java:24;jobs=8))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 8 (MapPartitionsRDD[41] at load at UseCase1.java:24), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 8)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_17 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_17 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_17_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_17_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at load at UseCase1.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 8.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 8.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 8.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 8.0 (TID 8)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_17
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_17 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_16
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_16 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 8.0 (TID 8). 1162 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_8.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 8 (load at UseCase1.java:24) finished in 0.016 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 8, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 8 finished: load at UseCase1.java:24, took 0.017204 s
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#126: java.lang.String   DeserializeToObject cast(value#118 as string).toString, obj#126: java.lang.String
 +- Project [value#118]                                                                                                                                                     +- Project [value#118]
    +- Relation[value#118] text                                                                                                                                                +- Relation[value#118] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#118 as string).toString, obj#126: java.lang.String   DeserializeToObject value#118.toString, obj#126: java.lang.String
!+- Project [value#118]                                                              +- Relation[value#118] text
!   +- Relation[value#118] text                                                      
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_18 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_18 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_18_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_18_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 18 from load at UseCase1.java:24
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase1.java:24
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 9 (load at UseCase1.java:24) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (load at UseCase1.java:24)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 9 (name=load at UseCase1.java:24;jobs=9))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 9 (MapPartitionsRDD[47] at load at UseCase1.java:24), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 9)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_19 locally took  3 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_19 without replication took  3 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_19_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_19_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[47] at load at UseCase1.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 9.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 9.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 9.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 9.0 (TID 9)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_19
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_19 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_18
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_18 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 9.0 (TID 9). 1473 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_9.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 52 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 9 (load at UseCase1.java:24) finished in 0.072 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 9, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 9 finished: load at UseCase1.java:24, took 0.059128 s
2022-02-10 13:32:03 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 1 paths.
2022-02-10 13:32:03 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#146
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#146]
 +- Relation[value#146] text                +- Relation[value#146] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#150: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#150: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 13:32:03 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#146
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#146, None)) > 0)
 +- Project [value#146]                     +- Project [value#146]
    +- Relation[value#146] text                +- Relation[value#146] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#151: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#151: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#152: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#152: java.lang.String
 +- LocalRelation <empty>, [value#146]                                                                                                                                      +- LocalRelation <empty>, [value#146]
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#146, None)) > 0)      +- Filter (length(trim(value#146, None)) > 0)
!      +- Project [value#146]                             +- Relation[value#146] text
!         +- Relation[value#146] text               
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#146, None)) > 0)
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 221.9 KB, free 1968.9 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_20 locally took  2 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_20 without replication took  2 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_20_piece0 locally took  2 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_20_piece0 without replication took  3 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 20 from load at UseCase2.java:19
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 10 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (load at UseCase2.java:19)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 10 (name=load at UseCase2.java:19;jobs=10))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 10 (MapPartitionsRDD[51] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 10)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 8.9 KB, free 1968.9 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_21 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_21 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.9 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_21_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_21_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 10.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 10.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 10.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 10.0 (TID 10)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_21
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_21 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_20
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_20 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 INFO  Executor:54 - Finished task 0.0 in stage 10.0 (TID 10). 1128 bytes result sent to driver
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_10.0, runningTasks: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2022-02-10 13:32:03 INFO  DAGScheduler:54 - ResultStage 10 (load at UseCase2.java:19) finished in 0.000 s
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - After removal of stage 10, remaining stages = 0
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Job 10 finished: load at UseCase2.java:19, took 0.013577 s
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#154: java.lang.String   DeserializeToObject cast(value#146 as string).toString, obj#154: java.lang.String
 +- Project [value#146]                                                                                                                                                     +- Project [value#146]
    +- Relation[value#146] text                                                                                                                                                +- Relation[value#146] text
          
2022-02-10 13:32:03 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#146 as string).toString, obj#154: java.lang.String   DeserializeToObject value#146.toString, obj#154: java.lang.String
!+- Project [value#146]                                                              +- Relation[value#146] text
!   +- Relation[value#146] text                                                      
          
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:03 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:03 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 221.9 KB, free 1968.7 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_22 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_22 without replication took  0 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.7 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_22_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_22_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 22 from load at UseCase2.java:19
2022-02-10 13:32:03 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:03 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:03 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Got job 11 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Final stage: ResultStage 11 (load at UseCase2.java:19)
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitStage(ResultStage 11 (name=load at UseCase2.java:19;jobs=11))
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting ResultStage 11 (MapPartitionsRDD[57] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:03 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 11)
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 13.9 KB, free 1968.7 MB)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_23 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_23 without replication took  16 ms
2022-02-10 13:32:03 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1968.7 MB)
2022-02-10 13:32:03 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 13:32:03 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 13:32:03 DEBUG BlockManager:58 - Put block broadcast_23_piece0 locally took  0 ms
2022-02-10 13:32:03 DEBUG BlockManager:58 - Putting block broadcast_23_piece0 without replication took  0 ms
2022-02-10 13:32:03 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:03 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[57] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:03 INFO  TaskSchedulerImpl:54 - Adding task set 11.0 with 1 tasks
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Epoch for TaskSet 11.0: 0
2022-02-10 13:32:03 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 11.0: NO_PREF, ANY
2022-02-10 13:32:03 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 13:32:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:03 INFO  Executor:54 - Running task 0.0 in stage 11.0 (TID 11)
2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_23
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_23 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:03 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:03 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:03 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:03 DEBUG BlockManager:58 - Getting local block broadcast_22
2022-02-10 13:32:03 DEBUG BlockManager:58 - Level for block broadcast_22 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(245)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 245
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 245
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(317)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 317
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 317
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(231)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 231
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 231
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(19)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 19
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 19
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 19
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 19
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_19_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_19_piece0 of size 7782 dropped from memory (free 2064290367)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_19_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_19_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_19
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_19 of size 14368 dropped from memory (free 2064304735)
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 19, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 19
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(236)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 236
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 236
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(312)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 312
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 312
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(241)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 241
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 241
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(213)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 213
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 213
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(278)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 278
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 278
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(237)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 237
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 237
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(307)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 307
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 307
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(271)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 271
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 271
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(279)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 279
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 279
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(299)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 299
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 299
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(255)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 255
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 255
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(319)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 319
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 319
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(240)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 240
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 240
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(16)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 16
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 16
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 16
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 16
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_16
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_16 of size 227232 dropped from memory (free 2064531967)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_16_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_16_piece0 of size 21165 dropped from memory (free 2064553132)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_16_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_16_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 16, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 16
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(314)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 314
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 314
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(14)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 14
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 14
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 14
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 14
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_14
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_14 of size 227072 dropped from memory (free 2064780204)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_14_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_14_piece0 of size 21151 dropped from memory (free 2064801355)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_14_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_14_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 14, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 14
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(266)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 266
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 266
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(219)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 219
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 219
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(244)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 244
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 244
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(228)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 228
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 228
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(272)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 272
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 272
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(17)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 17
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 17
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 17
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 17
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_17
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_17 of size 9144 dropped from memory (free 2064810499)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_17_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_17_piece0 of size 4742 dropped from memory (free 2064815241)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_17_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_17_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 17, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 17
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(310)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 310
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 310
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(327)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 327
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 327
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(332)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 332
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 332
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(295)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 295
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 295
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(318)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 318
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 318
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(270)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 270
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 270
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(215)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 215
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 215
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(333)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 333
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 333
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(227)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 227
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 227
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(276)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 276
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 276
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(256)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 256
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 256
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(277)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 277
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 277
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(308)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 308
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 308
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(274)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 274
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 274
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(290)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 290
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 290
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(275)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 275
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 275
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(234)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 234
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 234
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(15)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 15
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 15
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 15
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 15
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_15
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_15 of size 14200 dropped from memory (free 2064829441)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_15_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_15_piece0 of size 6837 dropped from memory (free 2064836278)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 6.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_15_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_15_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 15, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 15
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(293)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 293
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 293
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(239)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 239
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 239
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(325)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 325
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 325
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(313)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 313
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 313
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(304)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 304
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 304
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(330)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 330
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 330
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(309)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 309
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 309
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(315)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 315
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 315
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(248)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 248
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 248
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(323)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 323
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 323
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(281)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 281
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 281
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(238)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 238
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 238
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(216)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 216
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 216
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(287)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 287
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 287
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(326)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 326
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 326
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(268)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 268
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 268
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(297)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 297
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 297
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(265)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 265
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 265
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(262)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 262
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 262
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(18)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 18
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 18
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 18
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 18
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_18_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_18_piece0 of size 21165 dropped from memory (free 2064857443)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_18_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_18_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_18
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_18 of size 227232 dropped from memory (free 2065084675)
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 18, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 18
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(252)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 252
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 252
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(223)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 223
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 223
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(254)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 254
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 254
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(232)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 232
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 232
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(273)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 273
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 273
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(289)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 289
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 289
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(300)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 300
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 300
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(261)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 261
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 261
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(214)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 214
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 214
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(324)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 324
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 324
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(306)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 306
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 306
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(283)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 283
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 283
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(320)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 320
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 320
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(284)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 284
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 284
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(285)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 285
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 285
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(305)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 305
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 305
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(269)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 269
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 269
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(321)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 321
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 321
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(302)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 302
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 302
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(264)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 264
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 264
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(280)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 280
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 280
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(294)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 294
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 294
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(288)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 288
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 288
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(334)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 334
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 334
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(233)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 233
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 233
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(230)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 230
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 230
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(301)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 301
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 301
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(253)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 253
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 253
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(311)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 311
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 311
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(243)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 243
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 243
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(225)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 225
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 225
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(220)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 220
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 220
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(259)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 259
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 259
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(226)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 226
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 226
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(249)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 249
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 249
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(282)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 282
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 282
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(250)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 250
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 250
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(263)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 263
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 263
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(322)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 322
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 322
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(329)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 329
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 329
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(331)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 331
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 331
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(224)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 224
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 224
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(251)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 251
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 251
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(21)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 21
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 21
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 21
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 21
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_21_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_21_piece0 of size 4744 dropped from memory (free 2065089419)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_21_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_21_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_21
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_21 of size 9144 dropped from memory (free 2065098563)
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 21, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 21
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(246)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 246
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 246
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(296)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 296
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 296
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(218)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 218
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 218
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(291)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 291
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 291
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(316)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 316
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 316
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(292)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 292
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 292
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(222)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 222
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 222
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(298)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 298
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 298
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(221)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 221
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 221
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(229)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 229
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 229
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(303)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 303
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 303
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(258)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 258
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 258
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(235)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 235
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 235
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(242)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 242
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 242
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(20)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 20
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 20
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 20
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 20
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_20_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_20_piece0 of size 21165 dropped from memory (free 2065119728)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_20_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_20_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_20
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_20 of size 227232 dropped from memory (free 2065346960)
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 20, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 20
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(257)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 257
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 257
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(260)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 260
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 260
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(247)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 247
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 247
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(267)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 267
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 267
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(286)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 286
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 286
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(328)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 328
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 328
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(217)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 217
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 217
2022-02-10 13:32:04 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:04 INFO  Executor:54 - Finished task 0.0 in stage 11.0 (TID 11). 1544 bytes result sent to driver
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_11.0, runningTasks: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 623 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2022-02-10 13:32:04 INFO  DAGScheduler:54 - ResultStage 11 (load at UseCase2.java:19) finished in 0.639 s
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - After removal of stage 11, remaining stages = 0
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Job 11 finished: load at UseCase2.java:19, took 0.627848 s
2022-02-10 13:32:04 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:04 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:04 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:04 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#164
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#164]
 +- Relation[value#164] text                +- Relation[value#164] text
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#168: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#168: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 13:32:04 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#164
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#164, None)) > 0)
 +- Project [value#164]                     +- Project [value#164]
    +- Relation[value#164] text                +- Relation[value#164] text
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#169: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#169: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#170: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#170: java.lang.String
 +- LocalRelation <empty>, [value#164]                                                                                                                                      +- LocalRelation <empty>, [value#164]
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#164, None)) > 0)      +- Filter (length(trim(value#164, None)) > 0)
!      +- Project [value#164]                             +- Relation[value#164] text
!         +- Relation[value#164] text               
          
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#164, None)) > 0)
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:04 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:04 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_24 locally took  16 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_24 without replication took  16 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_24_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_24_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 24 from load at UseCase2.java:24
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:04 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Got job 12 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (load at UseCase2.java:24)
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitStage(ResultStage 12 (name=load at UseCase2.java:24;jobs=12))
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting ResultStage 12 (MapPartitionsRDD[61] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 12)
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_25 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_25 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_25 without replication took  0 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_25_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_25_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 25 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[61] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Adding task set 12.0 with 1 tasks
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Epoch for TaskSet 12.0: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 12.0: NO_PREF, ANY
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:04 INFO  Executor:54 - Running task 0.0 in stage 12.0 (TID 12)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_25
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_25 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:04 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_24
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_24 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 INFO  Executor:54 - Finished task 0.0 in stage 12.0 (TID 12). 1162 bytes result sent to driver
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_12.0, runningTasks: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2022-02-10 13:32:04 INFO  DAGScheduler:54 - ResultStage 12 (load at UseCase2.java:24) finished in 0.016 s
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - After removal of stage 12, remaining stages = 0
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Job 12 finished: load at UseCase2.java:24, took 0.014940 s
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#172: java.lang.String   DeserializeToObject cast(value#164 as string).toString, obj#172: java.lang.String
 +- Project [value#164]                                                                                                                                                     +- Project [value#164]
    +- Relation[value#164] text                                                                                                                                                +- Relation[value#164] text
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#164 as string).toString, obj#172: java.lang.String   DeserializeToObject value#164.toString, obj#172: java.lang.String
!+- Project [value#164]                                                              +- Relation[value#164] text
!   +- Relation[value#164] text                                                      
          
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:04 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_26 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_26 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_26 without replication took  0 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_26_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_26_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 26 from load at UseCase2.java:24
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:04 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Got job 13 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Final stage: ResultStage 13 (load at UseCase2.java:24)
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitStage(ResultStage 13 (name=load at UseCase2.java:24;jobs=13))
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting ResultStage 13 (MapPartitionsRDD[67] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 13)
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_27 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_27 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_27 without replication took  0 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_27_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_27_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 27 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[67] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Adding task set 13.0 with 1 tasks
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Epoch for TaskSet 13.0: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 13.0: NO_PREF, ANY
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:04 INFO  Executor:54 - Running task 0.0 in stage 13.0 (TID 13)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_27
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_27 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:04 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:04 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_26
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_26 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:04 INFO  Executor:54 - Finished task 0.0 in stage 13.0 (TID 13). 1473 bytes result sent to driver
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_13.0, runningTasks: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 47 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2022-02-10 13:32:04 INFO  DAGScheduler:54 - ResultStage 13 (load at UseCase2.java:24) finished in 0.079 s
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - After removal of stage 13, remaining stages = 0
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Job 13 finished: load at UseCase2.java:24, took 0.065305 s
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (order_date#157 LIKE 2014-01% && isnull(order_id#156))                                                                                                                                     Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))
 +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                      +- Join RightOuter, (order_customer_id#158 = customer_id#174)
    :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv      +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias(cast(customer_id#174 as string), None), unresolvedalias(cast(customer_fname#175 as string), None), unresolvedalias(cast(customer_lname#176 as string), None), unresolvedalias(cast(customer_email#177 as string), None), unresolvedalias(cast(customer_password#178 as string), None), unresolvedalias(cast(customer_street#179 as string), None), unresolvedalias(cast(customer_city#180 as string), None), unresolvedalias(cast(customer_state#181 as string), None), unresolvedalias(cast(customer_zipcode#182 as string), None)]   Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
 +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  +- Sort [customer_id#174 ASC NULLS FIRST], true
    +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                                +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
       +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))
          +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    +- Join RightOuter, (order_customer_id#158 = customer_id#174)
             :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                              :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
             +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]   Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
 +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          +- Sort [customer_id#174 ASC NULLS FIRST], true
    +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                        +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
       +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))                                                                                                                                                                                                                                                                                                                                                                                                                                                                         +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))
          +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- Join RightOuter, (order_customer_id#158 = customer_id#174)
             :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                      :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
             +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                            +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- LocalLimit 21
!   +- Project [cast(customer_id#174 as string) AS customer_id#249, cast(customer_fname#175 as string) AS customer_fname#250, cast(customer_lname#176 as string) AS customer_lname#251, cast(customer_email#177 as string) AS customer_email#252, cast(customer_password#178 as string) AS customer_password#253, cast(customer_street#179 as string) AS customer_street#254, cast(customer_city#180 as string) AS customer_city#255, cast(customer_state#181 as string) AS customer_state#256, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                                                                                                                                                                                                                                                                                              +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
!            +- Filter (cast(order_date#157 as string) LIKE 2014-01% && isnull(order_id#156))                                                                                                                                                                                                                                                                                                                                                                                                                                                                               +- Join Inner, (order_customer_id#158 = customer_id#174)
!               +- Join RightOuter, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  :- Project [order_customer_id#158]
!                  :- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                                                                                                                                                                                                                                                                                                         :  +- Filter (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156))
!                  +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                                                                                                                                                                                                                                                                                               :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Infer Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
!               :- Project [order_customer_id#158]                                                                                                                                                                                                                                                :- Filter isnotnull(order_customer_id#158)
!               :  +- Filter (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156))                                                                                                                                                                                        :  +- Project [order_customer_id#158]
!               :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                         :     +- Filter (isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156)))
!               +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                     :        +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!                                                                                                                                                                                                                                                                                                 +- Filter isnotnull(customer_id#174)
!                                                                                                                                                                                                                                                                                                    +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(422)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 422
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 422
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(374)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 374
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 374
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(395)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 395
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 395
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(401)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 401
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 401
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(400)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 400
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 400
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(376)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 376
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 376
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(418)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 418
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 418
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(24)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 24
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 24
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 24
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 24
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_24
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_24 of size 227232 dropped from memory (free 2065041362)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_24_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_24_piece0 of size 21165 dropped from memory (free 2065062527)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_24_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_24_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_24_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 24, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 24
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(410)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 410
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 410
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(425)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 425
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 425
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(378)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 378
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 378
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(343)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 343
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 343
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(362)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 362
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 362
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(368)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 368
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 368
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(339)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 339
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 339
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(416)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 416
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 416
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(351)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 351
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 351
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(336)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 336
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 336
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(415)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 415
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 415
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(372)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 372
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 372
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(411)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 411
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 411
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(360)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 360
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 360
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(417)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 417
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 417
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(350)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 350
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 350
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(342)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 342
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 342
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(393)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 393
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 393
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(424)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 424
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 424
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(367)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 367
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 367
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(392)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 392
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 392
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(389)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 389
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 389
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(402)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 402
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 402
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(396)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 396
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 396
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(347)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 347
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 347
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(405)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 405
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 405
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(399)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 399
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 399
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(349)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 349
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 349
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(419)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 419
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 419
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(397)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 397
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 397
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(344)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 344
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 344
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(398)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 398
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 398
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(383)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 383
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 383
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(387)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 387
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 387
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(370)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 370
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 370
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(404)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 404
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 404
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(346)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 346
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 346
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(358)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 358
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 358
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(341)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 341
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 341
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(363)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 363
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 363
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(413)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 413
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 413
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(354)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 354
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 354
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(421)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 421
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 421
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(420)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 420
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 420
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(348)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 348
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 348
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(384)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 384
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 384
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(345)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 345
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 345
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(408)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 408
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 408
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(352)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 352
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 352
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(364)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 364
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 364
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(386)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 386
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 386
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(391)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 391
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 391
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(375)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 375
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 375
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(355)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 355
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 355
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(381)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 381
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 381
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(27)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 27
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 27
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 27
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 27
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_27
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_27 of size 14368 dropped from memory (free 2065076895)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_27_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_27_piece0 of size 7780 dropped from memory (free 2065084675)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_27_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_27_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_27_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 27, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 27
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(359)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 359
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 359
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(409)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 409
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 409
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(373)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 373
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 373
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(380)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 380
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 380
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(335)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 335
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 335
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(371)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 371
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 371
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(388)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 388
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 388
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(385)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 385
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 385
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(407)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 407
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 407
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(412)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 412
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 412
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(338)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 338
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 338
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(382)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 382
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 382
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(366)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 366
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 366
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(356)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 356
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 356
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(406)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 406
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 406
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(361)
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization after Inferring Filters ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
!               :- Filter isnotnull(order_customer_id#158)                                                                                                                                                                                                                                        :- Project [order_customer_id#158]
!               :  +- Project [order_customer_id#158]                                                                                                                                                                                                                                             :  +- Filter ((isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156))) && isnotnull(order_customer_id#158))
!               :     +- Filter (isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156)))                                                                                                                                                      :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
!               :        +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                      +- Filter isnotnull(customer_id#174)
!               +- Filter isnotnull(customer_id#174)                                                                                                                                                                                                                                                 +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
!                  +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                   
          
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 361
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 361
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(337)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 337
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 337
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(369)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 369
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 369
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(25)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 25
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 25
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 25
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 25
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_25
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_25 of size 9144 dropped from memory (free 2065093819)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_25_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_25_piece0 of size 4744 dropped from memory (free 2065098563)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_25_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_25_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_25_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 25, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 25
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(394)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 394
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 394
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(377)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 377
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 377
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(414)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 414
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 414
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(26)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 26
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 26
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 26
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 26
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_26
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_26 of size 227232 dropped from memory (free 2065325795)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_26_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_26_piece0 of size 21165 dropped from memory (free 2065346960)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_26_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_26_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_26_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 26, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 26
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(357)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 357
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 357
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(379)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 379
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 379
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(23)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 23
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 23
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 23
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 23
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_23
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_23 of size 14256 dropped from memory (free 2065361216)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_23_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_23_piece0 of size 7743 dropped from memory (free 2065368959)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_23_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_23_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 23, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 23
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(353)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 353
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 353
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(423)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 423
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 423
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(340)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 340
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 340
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(365)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 365
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 365
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(22)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning broadcast 22
2022-02-10 13:32:04 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 22
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 22
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing broadcast 22
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_22
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_22 of size 227232 dropped from memory (free 2065596191)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Removing block broadcast_22_piece0
2022-02-10 13:32:04 DEBUG MemoryStore:58 - Block broadcast_22_piece0 of size 21165 dropped from memory (free 2065617356)
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch RewriteSubquery ===
 GlobalLimit 21                                                                                                                                                                                                                                                                    GlobalLimit 21
 +- LocalLimit 21                                                                                                                                                                                                                                                                  +- LocalLimit 21
    +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]      +- Project [cast(customer_id#174 as string) AS customer_id#249, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, cast(customer_zipcode#182 as string) AS customer_zipcode#257]
       +- Sort [customer_id#174 ASC NULLS FIRST], true                                                                                                                                                                                                                                   +- Sort [customer_id#174 ASC NULLS FIRST], true
          +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]                                                                                 +- Project [customer_id#174, customer_fname#175, customer_lname#176, customer_email#177, customer_password#178, customer_street#179, customer_city#180, customer_state#181, customer_zipcode#182]
             +- Join Inner, (order_customer_id#158 = customer_id#174)                                                                                                                                                                                                                          +- Join Inner, (order_customer_id#158 = customer_id#174)
                :- Project [order_customer_id#158]                                                                                                                                                                                                                                                :- Project [order_customer_id#158]
!               :  +- Filter ((isnotnull(order_date#157) && (StartsWith(cast(order_date#157 as string), 2014-01) && isnull(order_id#156))) && isnotnull(order_customer_id#158))                                                                                                                   :  +- Filter (((isnotnull(order_date#157) && StartsWith(cast(order_date#157 as string), 2014-01)) && isnull(order_id#156)) && isnotnull(order_customer_id#158))
                :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv                                                                                                                                                                                         :     +- Relation[order_id#156,order_date#157,order_customer_id#158,order_status#159] csv
                +- Filter isnotnull(customer_id#174)                                                                                                                                                                                                                                              +- Filter isnotnull(customer_id#174)
                   +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv                                                                                     +- Relation[customer_id#174,customer_fname#175,customer_lname#176,customer_email#177,customer_password#178,customer_street#179,customer_city#180,customer_state#181,customer_zipcode#182] csv
          
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.4 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_22_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_22_piece0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 22, response is 0
2022-02-10 13:32:04 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaned broadcast 22
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(390)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 390
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 390
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(403)
2022-02-10 13:32:04 DEBUG ContextCleaner:58 - Cleaning accumulator 403
2022-02-10 13:32:04 INFO  ContextCleaner:54 - Cleaned accumulator 403
2022-02-10 13:32:04 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#158 = customer_id#174))
2022-02-10 13:32:04 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#158) | rightKeys:List(customer_id#174)
2022-02-10 13:32:04 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#158 = customer_id#174))
2022-02-10 13:32:04 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#158) | rightKeys:List(customer_id#174)
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(order_date#157),StartsWith(cast(order_date#157 as string), 2014-01),isnull(order_id#156),isnotnull(order_customer_id#158)
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_id: int, order_date: timestamp, order_customer_id: int ... 1 more fields>
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(order_date),IsNull(order_id),IsNotNull(order_customer_id)
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(customer_id#174)
2022-02-10 13:32:04 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int, customer_fname: string, customer_lname: string, customer_email: string, customer_password: string ... 7 more fields>
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(customer_id)
2022-02-10 13:32:04 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StringType).toString, getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true))), obj#267: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(customer_id#249.toString, customer_fname#250.toString, customer_lname#251.toString, customer_email#252.toString, customer_password#253.toString, customer_street#254.toString, customer_city#255.toString, customer_state#256.toString, customer_zipcode#257.toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)), obj#267: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [customer_id#249, customer_fname#250, customer_lname#251, customer_email#252, customer_password#253, customer_street#254, customer_city#255, customer_state#256, customer_zipcode#257]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       +- LocalRelation <empty>, [customer_id#249, customer_fname#250, customer_lname#251, customer_email#252, customer_password#253, customer_street#254, customer_city#255, customer_state#256, customer_zipcode#257]
          
2022-02-10 13:32:04 DEBUG GenerateSafeProjection:58 - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, true].toString, input[7, string, true].toString, input[8, string, true].toString, StructField(customer_id,StringType,true), StructField(customer_fname,StringType,true), StructField(customer_lname,StringType,true), StructField(customer_email,StringType,true), StructField(customer_password,StringType,true), StructField(customer_street,StringType,true), StructField(customer_city,StringType,true), StructField(customer_state,StringType,true), StructField(customer_zipcode,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */     boolean isNull_16 = i.isNullAt(7);
/* 064 */     UTF8String value_16 = isNull_16 ?
/* 065 */     null : (i.getUTF8String(7));
/* 066 */     boolean isNull_15 = true;
/* 067 */     java.lang.String value_15 = null;
/* 068 */     if (!isNull_16) {
/* 069 */
/* 070 */       isNull_15 = false;
/* 071 */       if (!isNull_15) {
/* 072 */
/* 073 */         Object funcResult_7 = null;
/* 074 */         funcResult_7 = value_16.toString();
/* 075 */         value_15 = (java.lang.String) funcResult_7;
/* 076 */
/* 077 */       }
/* 078 */     }
/* 079 */     if (isNull_15) {
/* 080 */       values_0[7] = null;
/* 081 */     } else {
/* 082 */       values_0[7] = value_15;
/* 083 */     }
/* 084 */
/* 085 */     boolean isNull_18 = i.isNullAt(8);
/* 086 */     UTF8String value_18 = isNull_18 ?
/* 087 */     null : (i.getUTF8String(8));
/* 088 */     boolean isNull_17 = true;
/* 089 */     java.lang.String value_17 = null;
/* 090 */     if (!isNull_18) {
/* 091 */
/* 092 */       isNull_17 = false;
/* 093 */       if (!isNull_17) {
/* 094 */
/* 095 */         Object funcResult_8 = null;
/* 096 */         funcResult_8 = value_18.toString();
/* 097 */         value_17 = (java.lang.String) funcResult_8;
/* 098 */
/* 099 */       }
/* 100 */     }
/* 101 */     if (isNull_17) {
/* 102 */       values_0[8] = null;
/* 103 */     } else {
/* 104 */       values_0[8] = value_17;
/* 105 */     }
/* 106 */
/* 107 */   }
/* 108 */
/* 109 */
/* 110 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 111 */
/* 112 */     boolean isNull_8 = i.isNullAt(3);
/* 113 */     UTF8String value_8 = isNull_8 ?
/* 114 */     null : (i.getUTF8String(3));
/* 115 */     boolean isNull_7 = true;
/* 116 */     java.lang.String value_7 = null;
/* 117 */     if (!isNull_8) {
/* 118 */
/* 119 */       isNull_7 = false;
/* 120 */       if (!isNull_7) {
/* 121 */
/* 122 */         Object funcResult_3 = null;
/* 123 */         funcResult_3 = value_8.toString();
/* 124 */         value_7 = (java.lang.String) funcResult_3;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_7) {
/* 129 */       values_0[3] = null;
/* 130 */     } else {
/* 131 */       values_0[3] = value_7;
/* 132 */     }
/* 133 */
/* 134 */     boolean isNull_10 = i.isNullAt(4);
/* 135 */     UTF8String value_10 = isNull_10 ?
/* 136 */     null : (i.getUTF8String(4));
/* 137 */     boolean isNull_9 = true;
/* 138 */     java.lang.String value_9 = null;
/* 139 */     if (!isNull_10) {
/* 140 */
/* 141 */       isNull_9 = false;
/* 142 */       if (!isNull_9) {
/* 143 */
/* 144 */         Object funcResult_4 = null;
/* 145 */         funcResult_4 = value_10.toString();
/* 146 */         value_9 = (java.lang.String) funcResult_4;
/* 147 */
/* 148 */       }
/* 149 */     }
/* 150 */     if (isNull_9) {
/* 151 */       values_0[4] = null;
/* 152 */     } else {
/* 153 */       values_0[4] = value_9;
/* 154 */     }
/* 155 */
/* 156 */     boolean isNull_12 = i.isNullAt(5);
/* 157 */     UTF8String value_12 = isNull_12 ?
/* 158 */     null : (i.getUTF8String(5));
/* 159 */     boolean isNull_11 = true;
/* 160 */     java.lang.String value_11 = null;
/* 161 */     if (!isNull_12) {
/* 162 */
/* 163 */       isNull_11 = false;
/* 164 */       if (!isNull_11) {
/* 165 */
/* 166 */         Object funcResult_5 = null;
/* 167 */         funcResult_5 = value_12.toString();
/* 168 */         value_11 = (java.lang.String) funcResult_5;
/* 169 */
/* 170 */       }
/* 171 */     }
/* 172 */     if (isNull_11) {
/* 173 */       values_0[5] = null;
/* 174 */     } else {
/* 175 */       values_0[5] = value_11;
/* 176 */     }
/* 177 */
/* 178 */   }
/* 179 */
/* 180 */
/* 181 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 182 */
/* 183 */     boolean isNull_2 = i.isNullAt(0);
/* 184 */     UTF8String value_2 = isNull_2 ?
/* 185 */     null : (i.getUTF8String(0));
/* 186 */     boolean isNull_1 = true;
/* 187 */     java.lang.String value_1 = null;
/* 188 */     if (!isNull_2) {
/* 189 */
/* 190 */       isNull_1 = false;
/* 191 */       if (!isNull_1) {
/* 192 */
/* 193 */         Object funcResult_0 = null;
/* 194 */         funcResult_0 = value_2.toString();
/* 195 */         value_1 = (java.lang.String) funcResult_0;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_1) {
/* 200 */       values_0[0] = null;
/* 201 */     } else {
/* 202 */       values_0[0] = value_1;
/* 203 */     }
/* 204 */
/* 205 */     boolean isNull_4 = i.isNullAt(1);
/* 206 */     UTF8String value_4 = isNull_4 ?
/* 207 */     null : (i.getUTF8String(1));
/* 208 */     boolean isNull_3 = true;
/* 209 */     java.lang.String value_3 = null;
/* 210 */     if (!isNull_4) {
/* 211 */
/* 212 */       isNull_3 = false;
/* 213 */       if (!isNull_3) {
/* 214 */
/* 215 */         Object funcResult_1 = null;
/* 216 */         funcResult_1 = value_4.toString();
/* 217 */         value_3 = (java.lang.String) funcResult_1;
/* 218 */
/* 219 */       }
/* 220 */     }
/* 221 */     if (isNull_3) {
/* 222 */       values_0[1] = null;
/* 223 */     } else {
/* 224 */       values_0[1] = value_3;
/* 225 */     }
/* 226 */
/* 227 */     boolean isNull_6 = i.isNullAt(2);
/* 228 */     UTF8String value_6 = isNull_6 ?
/* 229 */     null : (i.getUTF8String(2));
/* 230 */     boolean isNull_5 = true;
/* 231 */     java.lang.String value_5 = null;
/* 232 */     if (!isNull_6) {
/* 233 */
/* 234 */       isNull_5 = false;
/* 235 */       if (!isNull_5) {
/* 236 */
/* 237 */         Object funcResult_2 = null;
/* 238 */         funcResult_2 = value_6.toString();
/* 239 */         value_5 = (java.lang.String) funcResult_2;
/* 240 */
/* 241 */       }
/* 242 */     }
/* 243 */     if (isNull_5) {
/* 244 */       values_0[2] = null;
/* 245 */     } else {
/* 246 */       values_0[2] = value_5;
/* 247 */     }
/* 248 */
/* 249 */   }
/* 250 */
/* 251 */ }

2022-02-10 13:32:04 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:04 DEBUG CodeGenerator:58 - 
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:04 INFO  CodeGenerator:54 - Code generated in 8.239901 ms
2022-02-10 13:32:04 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 031 */         long scan_value_1 = scan_isNull_1 ?
/* 032 */         -1L : (scan_row_0.getLong(1));
/* 033 */
/* 034 */         if (!(!scan_isNull_1)) continue;
/* 035 */
/* 036 */         boolean filter_isNull_3 = scan_isNull_1;
/* 037 */         UTF8String filter_value_3 = null;
/* 038 */         if (!scan_isNull_1) {
/* 039 */           filter_value_3 = UTF8String.fromString(
/* 040 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 041 */         }
/* 042 */
/* 043 */         boolean filter_value_2 = false;
/* 044 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 045 */         if (!filter_value_2) continue;
/* 046 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 047 */         int scan_value_0 = scan_isNull_0 ?
/* 048 */         -1 : (scan_row_0.getInt(0));
/* 049 */
/* 050 */         if (!scan_isNull_0) continue;
/* 051 */
/* 052 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 053 */         int scan_value_2 = scan_isNull_2 ?
/* 054 */         -1 : (scan_row_0.getInt(2));
/* 055 */
/* 056 */         if (!(!scan_isNull_2)) continue;
/* 057 */
/* 058 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 059 */
/* 060 */         filter_mutableStateArray_0[1].reset();
/* 061 */
/* 062 */         if (false) {
/* 063 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 064 */         } else {
/* 065 */           filter_mutableStateArray_0[1].write(0, scan_value_2);
/* 066 */         }
/* 067 */         append((filter_mutableStateArray_0[1].getRow()));
/* 068 */
/* 069 */       } while(false);
/* 070 */       if (shouldStop()) return;
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 13:32:04 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 031 */         long scan_value_1 = scan_isNull_1 ?
/* 032 */         -1L : (scan_row_0.getLong(1));
/* 033 */
/* 034 */         if (!(!scan_isNull_1)) continue;
/* 035 */
/* 036 */         boolean filter_isNull_3 = scan_isNull_1;
/* 037 */         UTF8String filter_value_3 = null;
/* 038 */         if (!scan_isNull_1) {
/* 039 */           filter_value_3 = UTF8String.fromString(
/* 040 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 041 */         }
/* 042 */
/* 043 */         boolean filter_value_2 = false;
/* 044 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 045 */         if (!filter_value_2) continue;
/* 046 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 047 */         int scan_value_0 = scan_isNull_0 ?
/* 048 */         -1 : (scan_row_0.getInt(0));
/* 049 */
/* 050 */         if (!scan_isNull_0) continue;
/* 051 */
/* 052 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 053 */         int scan_value_2 = scan_isNull_2 ?
/* 054 */         -1 : (scan_row_0.getInt(2));
/* 055 */
/* 056 */         if (!(!scan_isNull_2)) continue;
/* 057 */
/* 058 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 059 */
/* 060 */         filter_mutableStateArray_0[1].reset();
/* 061 */
/* 062 */         if (false) {
/* 063 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 064 */         } else {
/* 065 */           filter_mutableStateArray_0[1].write(0, scan_value_2);
/* 066 */         }
/* 067 */         append((filter_mutableStateArray_0[1].getRow()));
/* 068 */
/* 069 */       } while(false);
/* 070 */       if (shouldStop()) return;
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 13:32:04 INFO  CodeGenerator:54 - Code generated in 8.9132 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_28 stored as values in memory (estimated size 221.8 KB, free 1969.7 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_28 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_28 without replication took  0 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_28_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.7 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_28_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_28_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_28_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 28 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:04 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:04 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:04 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Got job 14 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitStage(ResultStage 14 (name=run at ThreadPoolExecutor.java:1149;jobs=14))
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting ResultStage 14 (MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 13:32:04 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 14)
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_29 stored as values in memory (estimated size 11.9 KB, free 1969.7 MB)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_29 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_29 without replication took  0 ms
2022-02-10 13:32:04 INFO  MemoryStore:54 - Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1969.7 MB)
2022-02-10 13:32:04 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 13:32:04 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Told master about block broadcast_29_piece0
2022-02-10 13:32:04 DEBUG BlockManager:58 - Put block broadcast_29_piece0 locally took  0 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Putting block broadcast_29_piece0 without replication took  0 ms
2022-02-10 13:32:04 INFO  SparkContext:54 - Created broadcast 29 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:04 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[70] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:04 INFO  TaskSchedulerImpl:54 - Adding task set 14.0 with 1 tasks
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Epoch for TaskSet 14.0: 0
2022-02-10 13:32:04 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 14.0: NO_PREF, ANY
2022-02-10 13:32:04 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 13:32:04 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 13:32:04 INFO  Executor:54 - Running task 0.0 in stage 14.0 (TID 14)
2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_29
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_29 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:04 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:04 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

2022-02-10 13:32:04 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

2022-02-10 13:32:04 INFO  CodeGenerator:54 - Code generated in 7.7677 ms
2022-02-10 13:32:04 DEBUG BlockManager:58 - Getting local block broadcast_28
2022-02-10 13:32:04 DEBUG BlockManager:58 - Level for block broadcast_28 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 INFO  Executor:54 - Finished task 0.0 in stage 14.0 (TID 14). 1337 bytes result sent to driver
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_14.0, runningTasks: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:05 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 476 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2022-02-10 13:32:05 INFO  DAGScheduler:54 - ResultStage 14 (run at ThreadPoolExecutor.java:1149) finished in 0.492 s
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - After removal of stage 14, remaining stages = 0
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Job 14 finished: run at ThreadPoolExecutor.java:1149, took 0.489932 s
2022-02-10 13:32:05 DEBUG TaskMemoryManager:224 - Task 0 acquired 1024.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@181acc30
2022-02-10 13:32:05 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:05 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:05 INFO  CodeGenerator:54 - Code generated in 4.6655 ms
2022-02-10 13:32:05 DEBUG TaskMemoryManager:224 - Task 0 acquired 16.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@181acc30
2022-02-10 13:32:05 DEBUG TaskMemoryManager:233 - Task 0 release 16.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@181acc30
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_30 stored as values in memory (estimated size 1024.0 KB, free 1968.7 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_30 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_30 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_30_piece0 stored as bytes in memory (estimated size 181.0 B, free 1968.7 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 181.0 B, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_30_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_30_piece0 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_30_piece0 without replication took  0 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 30 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_30
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_30 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 023 */
/* 024 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 025 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 026 */
/* 027 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 224);
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       do {
/* 037 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 038 */         int scan_value_0 = scan_isNull_0 ?
/* 039 */         -1 : (scan_row_0.getInt(0));
/* 040 */
/* 041 */         if (!(!scan_isNull_0)) continue;
/* 042 */
/* 043 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 044 */
/* 045 */         // generate join key for stream side
/* 046 */         boolean bhj_isNull_0 = false;
/* 047 */         long bhj_value_0 = -1L;
/* 048 */         if (!false) {
/* 049 */           bhj_value_0 = (long) scan_value_0;
/* 050 */         }
/* 051 */         // find matches from HashedRelation
/* 052 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 053 */         if (bhj_matched_0 != null) {
/* 054 */           {
/* 055 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 056 */
/* 057 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 058 */             UTF8String scan_value_1 = scan_isNull_1 ?
/* 059 */             null : (scan_row_0.getUTF8String(1));
/* 060 */             boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 061 */             UTF8String scan_value_2 = scan_isNull_2 ?
/* 062 */             null : (scan_row_0.getUTF8String(2));
/* 063 */             boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 064 */             UTF8String scan_value_3 = scan_isNull_3 ?
/* 065 */             null : (scan_row_0.getUTF8String(3));
/* 066 */             boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 067 */             UTF8String scan_value_4 = scan_isNull_4 ?
/* 068 */             null : (scan_row_0.getUTF8String(4));
/* 069 */             boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 070 */             UTF8String scan_value_5 = scan_isNull_5 ?
/* 071 */             null : (scan_row_0.getUTF8String(5));
/* 072 */             boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 073 */             UTF8String scan_value_6 = scan_isNull_6 ?
/* 074 */             null : (scan_row_0.getUTF8String(6));
/* 075 */             boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 076 */             UTF8String scan_value_7 = scan_isNull_7 ?
/* 077 */             null : (scan_row_0.getUTF8String(7));
/* 078 */             boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 079 */             int scan_value_8 = scan_isNull_8 ?
/* 080 */             -1 : (scan_row_0.getInt(8));
/* 081 */             filter_mutableStateArray_0[3].reset();
/* 082 */
/* 083 */             filter_mutableStateArray_0[3].zeroOutNullBytes();
/* 084 */
/* 085 */             if (false) {
/* 086 */               filter_mutableStateArray_0[3].setNullAt(0);
/* 087 */             } else {
/* 088 */               filter_mutableStateArray_0[3].write(0, scan_value_0);
/* 089 */             }
/* 090 */
/* 091 */             if (scan_isNull_1) {
/* 092 */               filter_mutableStateArray_0[3].setNullAt(1);
/* 093 */             } else {
/* 094 */               filter_mutableStateArray_0[3].write(1, scan_value_1);
/* 095 */             }
/* 096 */
/* 097 */             if (scan_isNull_2) {
/* 098 */               filter_mutableStateArray_0[3].setNullAt(2);
/* 099 */             } else {
/* 100 */               filter_mutableStateArray_0[3].write(2, scan_value_2);
/* 101 */             }
/* 102 */
/* 103 */             if (scan_isNull_3) {
/* 104 */               filter_mutableStateArray_0[3].setNullAt(3);
/* 105 */             } else {
/* 106 */               filter_mutableStateArray_0[3].write(3, scan_value_3);
/* 107 */             }
/* 108 */
/* 109 */             if (scan_isNull_4) {
/* 110 */               filter_mutableStateArray_0[3].setNullAt(4);
/* 111 */             } else {
/* 112 */               filter_mutableStateArray_0[3].write(4, scan_value_4);
/* 113 */             }
/* 114 */
/* 115 */             if (scan_isNull_5) {
/* 116 */               filter_mutableStateArray_0[3].setNullAt(5);
/* 117 */             } else {
/* 118 */               filter_mutableStateArray_0[3].write(5, scan_value_5);
/* 119 */             }
/* 120 */
/* 121 */             if (scan_isNull_6) {
/* 122 */               filter_mutableStateArray_0[3].setNullAt(6);
/* 123 */             } else {
/* 124 */               filter_mutableStateArray_0[3].write(6, scan_value_6);
/* 125 */             }
/* 126 */
/* 127 */             if (scan_isNull_7) {
/* 128 */               filter_mutableStateArray_0[3].setNullAt(7);
/* 129 */             } else {
/* 130 */               filter_mutableStateArray_0[3].write(7, scan_value_7);
/* 131 */             }
/* 132 */
/* 133 */             if (scan_isNull_8) {
/* 134 */               filter_mutableStateArray_0[3].setNullAt(8);
/* 135 */             } else {
/* 136 */               filter_mutableStateArray_0[3].write(8, scan_value_8);
/* 137 */             }
/* 138 */             append((filter_mutableStateArray_0[3].getRow()));
/* 139 */
/* 140 */           }
/* 141 */         }
/* 142 */
/* 143 */       } while(false);
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */   }
/* 147 */
/* 148 */ }

2022-02-10 13:32:05 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 023 */
/* 024 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 025 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 026 */
/* 027 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 224);
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       do {
/* 037 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 038 */         int scan_value_0 = scan_isNull_0 ?
/* 039 */         -1 : (scan_row_0.getInt(0));
/* 040 */
/* 041 */         if (!(!scan_isNull_0)) continue;
/* 042 */
/* 043 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 044 */
/* 045 */         // generate join key for stream side
/* 046 */         boolean bhj_isNull_0 = false;
/* 047 */         long bhj_value_0 = -1L;
/* 048 */         if (!false) {
/* 049 */           bhj_value_0 = (long) scan_value_0;
/* 050 */         }
/* 051 */         // find matches from HashedRelation
/* 052 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 053 */         if (bhj_matched_0 != null) {
/* 054 */           {
/* 055 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 056 */
/* 057 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 058 */             UTF8String scan_value_1 = scan_isNull_1 ?
/* 059 */             null : (scan_row_0.getUTF8String(1));
/* 060 */             boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 061 */             UTF8String scan_value_2 = scan_isNull_2 ?
/* 062 */             null : (scan_row_0.getUTF8String(2));
/* 063 */             boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 064 */             UTF8String scan_value_3 = scan_isNull_3 ?
/* 065 */             null : (scan_row_0.getUTF8String(3));
/* 066 */             boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 067 */             UTF8String scan_value_4 = scan_isNull_4 ?
/* 068 */             null : (scan_row_0.getUTF8String(4));
/* 069 */             boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 070 */             UTF8String scan_value_5 = scan_isNull_5 ?
/* 071 */             null : (scan_row_0.getUTF8String(5));
/* 072 */             boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 073 */             UTF8String scan_value_6 = scan_isNull_6 ?
/* 074 */             null : (scan_row_0.getUTF8String(6));
/* 075 */             boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 076 */             UTF8String scan_value_7 = scan_isNull_7 ?
/* 077 */             null : (scan_row_0.getUTF8String(7));
/* 078 */             boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 079 */             int scan_value_8 = scan_isNull_8 ?
/* 080 */             -1 : (scan_row_0.getInt(8));
/* 081 */             filter_mutableStateArray_0[3].reset();
/* 082 */
/* 083 */             filter_mutableStateArray_0[3].zeroOutNullBytes();
/* 084 */
/* 085 */             if (false) {
/* 086 */               filter_mutableStateArray_0[3].setNullAt(0);
/* 087 */             } else {
/* 088 */               filter_mutableStateArray_0[3].write(0, scan_value_0);
/* 089 */             }
/* 090 */
/* 091 */             if (scan_isNull_1) {
/* 092 */               filter_mutableStateArray_0[3].setNullAt(1);
/* 093 */             } else {
/* 094 */               filter_mutableStateArray_0[3].write(1, scan_value_1);
/* 095 */             }
/* 096 */
/* 097 */             if (scan_isNull_2) {
/* 098 */               filter_mutableStateArray_0[3].setNullAt(2);
/* 099 */             } else {
/* 100 */               filter_mutableStateArray_0[3].write(2, scan_value_2);
/* 101 */             }
/* 102 */
/* 103 */             if (scan_isNull_3) {
/* 104 */               filter_mutableStateArray_0[3].setNullAt(3);
/* 105 */             } else {
/* 106 */               filter_mutableStateArray_0[3].write(3, scan_value_3);
/* 107 */             }
/* 108 */
/* 109 */             if (scan_isNull_4) {
/* 110 */               filter_mutableStateArray_0[3].setNullAt(4);
/* 111 */             } else {
/* 112 */               filter_mutableStateArray_0[3].write(4, scan_value_4);
/* 113 */             }
/* 114 */
/* 115 */             if (scan_isNull_5) {
/* 116 */               filter_mutableStateArray_0[3].setNullAt(5);
/* 117 */             } else {
/* 118 */               filter_mutableStateArray_0[3].write(5, scan_value_5);
/* 119 */             }
/* 120 */
/* 121 */             if (scan_isNull_6) {
/* 122 */               filter_mutableStateArray_0[3].setNullAt(6);
/* 123 */             } else {
/* 124 */               filter_mutableStateArray_0[3].write(6, scan_value_6);
/* 125 */             }
/* 126 */
/* 127 */             if (scan_isNull_7) {
/* 128 */               filter_mutableStateArray_0[3].setNullAt(7);
/* 129 */             } else {
/* 130 */               filter_mutableStateArray_0[3].write(7, scan_value_7);
/* 131 */             }
/* 132 */
/* 133 */             if (scan_isNull_8) {
/* 134 */               filter_mutableStateArray_0[3].setNullAt(8);
/* 135 */             } else {
/* 136 */               filter_mutableStateArray_0[3].write(8, scan_value_8);
/* 137 */             }
/* 138 */             append((filter_mutableStateArray_0[3].getRow()));
/* 139 */
/* 140 */           }
/* 141 */         }
/* 142 */
/* 143 */       } while(false);
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */   }
/* 147 */
/* 148 */ }

2022-02-10 13:32:05 INFO  CodeGenerator:54 - Code generated in 12.2354 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_31 stored as values in memory (estimated size 221.8 KB, free 1968.5 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_31 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_31 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_31_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.4 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_31_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_31_piece0 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_31_piece0 without replication took  0 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 31 from show at UseCase2.java:69
2022-02-10 13:32:05 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3.apply(org.apache.spark.sql.catalyst.InternalRow)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.TakeOrderedAndProjectExec$$anonfun$3) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1 org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.$outer
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32.apply(scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[73] at show at UseCase2.java:69
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1,Set(ord$9, num$5))
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[73] at show at UseCase2.java:69)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.$outer
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final int org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.num$5
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.math.Ordering org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.ord$9
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply()
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[73] at show at UseCase2.java:69
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1,Set(ord$9, num$5))
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[73] at show at UseCase2.java:69)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + the starting closure doesn't actually need MapPartitionsRDD[73] at show at UseCase2.java:69, so we null it out
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$32) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.util.BoundedPriorityQueue org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49.apply(org.apache.spark.util.BoundedPriorityQueue,org.apache.spark.util.BoundedPriorityQueue)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$apply$49) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:05 INFO  SparkContext:54 - Starting job: show at UseCase2.java:69
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Got job 15 (show at UseCase2.java:69) with 1 output partitions
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (show at UseCase2.java:69)
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitStage(ResultStage 15 (name=show at UseCase2.java:69;jobs=15))
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting ResultStage 15 (MapPartitionsRDD[74] at show at UseCase2.java:69), which has no missing parents
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 15)
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_32 stored as values in memory (estimated size 16.3 KB, free 1968.4 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_32 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_32 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.8 KB, free 1968.4 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.8 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_32_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_32_piece0 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_32_piece0 without replication took  0 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 32 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[74] at show at UseCase2.java:69) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Adding task set 15.0 with 1 tasks
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Epoch for TaskSet 15.0: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 15.0: NO_PREF, ANY
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 13:32:05 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 13:32:05 INFO  Executor:54 - Running task 0.0 in stage 15.0 (TID 15)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_32
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_32 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:05 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:05 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_31
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_31 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(459)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 459
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 459
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(456)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 456
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 456
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(451)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 451
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 451
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(454)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 454
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 454
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(460)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 460
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 460
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(462)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 462
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 462
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(444)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 444
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 444
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(464)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 464
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 464
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(467)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 467
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 467
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(450)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 450
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 450
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(466)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 466
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 466
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(465)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 465
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 465
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(29)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning broadcast 29
2022-02-10 13:32:05 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 29
2022-02-10 13:32:05 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 29
2022-02-10 13:32:05 DEBUG BlockManager:58 - Removing broadcast 29
2022-02-10 13:32:05 DEBUG BlockManager:58 - Removing block broadcast_29_piece0
2022-02-10 13:32:05 DEBUG MemoryStore:58 - Block broadcast_29_piece0 of size 6553 dropped from memory (free 2064035286)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Removed broadcast_29_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_29_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_29_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Removing block broadcast_29
2022-02-10 13:32:05 DEBUG MemoryStore:58 - Block broadcast_29 of size 12192 dropped from memory (free 2064047478)
2022-02-10 13:32:05 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 29, response is 0
2022-02-10 13:32:05 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaned broadcast 29
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(447)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 447
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 447
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(443)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 443
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 443
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(445)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 445
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 445
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(449)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 449
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 449
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(458)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 458
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 458
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(453)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 453
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 453
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(463)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 463
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 463
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(457)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 457
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 457
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(448)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 448
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 448
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(446)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 446
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 446
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(452)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 452
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 452
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(455)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 455
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 455
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(461)
2022-02-10 13:32:05 DEBUG ContextCleaner:58 - Cleaning accumulator 461
2022-02-10 13:32:05 INFO  ContextCleaner:54 - Cleaned accumulator 461
2022-02-10 13:32:05 INFO  Executor:54 - Finished task 0.0 in stage 15.0 (TID 15). 2797 bytes result sent to driver
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_15.0, runningTasks: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:05 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:05 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 80 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2022-02-10 13:32:05 INFO  DAGScheduler:54 - ResultStage 15 (show at UseCase2.java:69) finished in 0.095 s
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - After removal of stage 15, remaining stages = 0
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Job 15 finished: show at UseCase2.java:69, took 0.086829 s
2022-02-10 13:32:05 DEBUG package$ExpressionCanonicalizer:58 - 
=== Result of Batch CleanExpressions ===
!cast(input[0, int, true] as string) AS customer_id#249   cast(input[0, int, true] as string)
!+- cast(input[0, int, true] as string)                   +- input[0, int, true]
!   +- input[0, int, true]                                
          
2022-02-10 13:32:05 DEBUG package$ExpressionCanonicalizer:58 - 
=== Result of Batch CleanExpressions ===
!cast(input[8, int, true] as string) AS customer_zipcode#257   cast(input[8, int, true] as string)
!+- cast(input[8, int, true] as string)                        +- input[8, int, true]
!   +- input[8, int, true]                                     
          
2022-02-10 13:32:05 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as string),input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],cast(input[8, int, true] as string):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_6 = i.isNullAt(5);
/* 039 */     UTF8String value_6 = isNull_6 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_6) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_6);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_7 = i.isNullAt(6);
/* 048 */     UTF8String value_7 = isNull_7 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_7) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_7);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_8 = i.isNullAt(7);
/* 057 */     UTF8String value_8 = isNull_8 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_8) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_8);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_10 = i.isNullAt(8);
/* 066 */     int value_10 = isNull_10 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     boolean isNull_9 = isNull_10;
/* 069 */     UTF8String value_9 = null;
/* 070 */     if (!isNull_10) {
/* 071 */       value_9 = UTF8String.fromString(String.valueOf(value_10));
/* 072 */     }
/* 073 */     if (isNull_9) {
/* 074 */       mutableStateArray_0[0].setNullAt(8);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(8, value_9);
/* 077 */     }
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */
/* 082 */   private void writeFields_0_0(InternalRow i) {
/* 083 */
/* 084 */     boolean isNull_1 = i.isNullAt(0);
/* 085 */     int value_1 = isNull_1 ?
/* 086 */     -1 : (i.getInt(0));
/* 087 */     boolean isNull_0 = isNull_1;
/* 088 */     UTF8String value_0 = null;
/* 089 */     if (!isNull_1) {
/* 090 */       value_0 = UTF8String.fromString(String.valueOf(value_1));
/* 091 */     }
/* 092 */     if (isNull_0) {
/* 093 */       mutableStateArray_0[0].setNullAt(0);
/* 094 */     } else {
/* 095 */       mutableStateArray_0[0].write(0, value_0);
/* 096 */     }
/* 097 */
/* 098 */     boolean isNull_2 = i.isNullAt(1);
/* 099 */     UTF8String value_2 = isNull_2 ?
/* 100 */     null : (i.getUTF8String(1));
/* 101 */     if (isNull_2) {
/* 102 */       mutableStateArray_0[0].setNullAt(1);
/* 103 */     } else {
/* 104 */       mutableStateArray_0[0].write(1, value_2);
/* 105 */     }
/* 106 */
/* 107 */     boolean isNull_3 = i.isNullAt(2);
/* 108 */     UTF8String value_3 = isNull_3 ?
/* 109 */     null : (i.getUTF8String(2));
/* 110 */     if (isNull_3) {
/* 111 */       mutableStateArray_0[0].setNullAt(2);
/* 112 */     } else {
/* 113 */       mutableStateArray_0[0].write(2, value_3);
/* 114 */     }
/* 115 */
/* 116 */     boolean isNull_4 = i.isNullAt(3);
/* 117 */     UTF8String value_4 = isNull_4 ?
/* 118 */     null : (i.getUTF8String(3));
/* 119 */     if (isNull_4) {
/* 120 */       mutableStateArray_0[0].setNullAt(3);
/* 121 */     } else {
/* 122 */       mutableStateArray_0[0].write(3, value_4);
/* 123 */     }
/* 124 */
/* 125 */     boolean isNull_5 = i.isNullAt(4);
/* 126 */     UTF8String value_5 = isNull_5 ?
/* 127 */     null : (i.getUTF8String(4));
/* 128 */     if (isNull_5) {
/* 129 */       mutableStateArray_0[0].setNullAt(4);
/* 130 */     } else {
/* 131 */       mutableStateArray_0[0].write(4, value_5);
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */ }

2022-02-10 13:32:05 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_6 = i.isNullAt(5);
/* 039 */     UTF8String value_6 = isNull_6 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_6) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_6);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_7 = i.isNullAt(6);
/* 048 */     UTF8String value_7 = isNull_7 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_7) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_7);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_8 = i.isNullAt(7);
/* 057 */     UTF8String value_8 = isNull_8 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_8) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_8);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_10 = i.isNullAt(8);
/* 066 */     int value_10 = isNull_10 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     boolean isNull_9 = isNull_10;
/* 069 */     UTF8String value_9 = null;
/* 070 */     if (!isNull_10) {
/* 071 */       value_9 = UTF8String.fromString(String.valueOf(value_10));
/* 072 */     }
/* 073 */     if (isNull_9) {
/* 074 */       mutableStateArray_0[0].setNullAt(8);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(8, value_9);
/* 077 */     }
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */
/* 082 */   private void writeFields_0_0(InternalRow i) {
/* 083 */
/* 084 */     boolean isNull_1 = i.isNullAt(0);
/* 085 */     int value_1 = isNull_1 ?
/* 086 */     -1 : (i.getInt(0));
/* 087 */     boolean isNull_0 = isNull_1;
/* 088 */     UTF8String value_0 = null;
/* 089 */     if (!isNull_1) {
/* 090 */       value_0 = UTF8String.fromString(String.valueOf(value_1));
/* 091 */     }
/* 092 */     if (isNull_0) {
/* 093 */       mutableStateArray_0[0].setNullAt(0);
/* 094 */     } else {
/* 095 */       mutableStateArray_0[0].write(0, value_0);
/* 096 */     }
/* 097 */
/* 098 */     boolean isNull_2 = i.isNullAt(1);
/* 099 */     UTF8String value_2 = isNull_2 ?
/* 100 */     null : (i.getUTF8String(1));
/* 101 */     if (isNull_2) {
/* 102 */       mutableStateArray_0[0].setNullAt(1);
/* 103 */     } else {
/* 104 */       mutableStateArray_0[0].write(1, value_2);
/* 105 */     }
/* 106 */
/* 107 */     boolean isNull_3 = i.isNullAt(2);
/* 108 */     UTF8String value_3 = isNull_3 ?
/* 109 */     null : (i.getUTF8String(2));
/* 110 */     if (isNull_3) {
/* 111 */       mutableStateArray_0[0].setNullAt(2);
/* 112 */     } else {
/* 113 */       mutableStateArray_0[0].write(2, value_3);
/* 114 */     }
/* 115 */
/* 116 */     boolean isNull_4 = i.isNullAt(3);
/* 117 */     UTF8String value_4 = isNull_4 ?
/* 118 */     null : (i.getUTF8String(3));
/* 119 */     if (isNull_4) {
/* 120 */       mutableStateArray_0[0].setNullAt(3);
/* 121 */     } else {
/* 122 */       mutableStateArray_0[0].write(3, value_4);
/* 123 */     }
/* 124 */
/* 125 */     boolean isNull_5 = i.isNullAt(4);
/* 126 */     UTF8String value_5 = isNull_5 ?
/* 127 */     null : (i.getUTF8String(4));
/* 128 */     if (isNull_5) {
/* 129 */       mutableStateArray_0[0].setNullAt(4);
/* 130 */     } else {
/* 131 */       mutableStateArray_0[0].write(4, value_5);
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */ }

2022-02-10 13:32:05 INFO  CodeGenerator:54 - Code generated in 8.131499 ms
2022-02-10 13:32:05 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:05 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:05 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:05 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#280
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#280]
 +- Relation[value#280] text                +- Relation[value#280] text
          
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#284: java.lang.String   DeserializeToObject cast(value#280 as string).toString, obj#284: java.lang.String
 +- LocalRelation <empty>, [value#280]                                                                                                                                      +- LocalRelation <empty>, [value#280]
          
2022-02-10 13:32:05 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#280
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#280, None)) > 0)
 +- Project [value#280]                     +- Project [value#280]
    +- Relation[value#280] text                +- Relation[value#280] text
          
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#285: java.lang.String   DeserializeToObject cast(value#280 as string).toString, obj#285: java.lang.String
 +- LocalRelation <empty>, [value#280]                                                                                                                                      +- LocalRelation <empty>, [value#280]
          
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#286: java.lang.String   DeserializeToObject cast(value#280 as string).toString, obj#286: java.lang.String
 +- LocalRelation <empty>, [value#280]                                                                                                                                      +- LocalRelation <empty>, [value#280]
          
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#280, None)) > 0)      +- Filter (length(trim(value#280, None)) > 0)
!      +- Project [value#280]                             +- Relation[value#280] text
!         +- Relation[value#280] text               
          
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#280, None)) > 0)
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:05 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:05 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:05 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_33 stored as values in memory (estimated size 221.9 KB, free 1968.2 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_33 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_33 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_33_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.2 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_33_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_33_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_33_piece0 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_33_piece0 without replication took  0 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 33 from load at UseCase2.java:19
2022-02-10 13:32:05 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:05 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Got job 16 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (load at UseCase2.java:19)
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitStage(ResultStage 16 (name=load at UseCase2.java:19;jobs=16))
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting ResultStage 16 (MapPartitionsRDD[78] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 16)
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_34 stored as values in memory (estimated size 8.9 KB, free 1968.2 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_34 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_34 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.2 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_34_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_34_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_34_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_34_piece0 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_34_piece0 without replication took  0 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 34 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[78] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Adding task set 16.0 with 1 tasks
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Epoch for TaskSet 16.0: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 16.0: NO_PREF, ANY
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 13:32:05 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:05 INFO  Executor:54 - Running task 0.0 in stage 16.0 (TID 16)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_34
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_34 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:05 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_33
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_33 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 INFO  Executor:54 - Finished task 0.0 in stage 16.0 (TID 16). 1128 bytes result sent to driver
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_16.0, runningTasks: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:05 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2022-02-10 13:32:05 INFO  DAGScheduler:54 - ResultStage 16 (load at UseCase2.java:19) finished in 0.000 s
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - After removal of stage 16, remaining stages = 0
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Job 16 finished: load at UseCase2.java:19, took 0.010979 s
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#288: java.lang.String   DeserializeToObject cast(value#280 as string).toString, obj#288: java.lang.String
 +- Project [value#280]                                                                                                                                                     +- Project [value#280]
    +- Relation[value#280] text                                                                                                                                                +- Relation[value#280] text
          
2022-02-10 13:32:05 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#280 as string).toString, obj#288: java.lang.String   DeserializeToObject value#280.toString, obj#288: java.lang.String
!+- Project [value#280]                                                              +- Relation[value#280] text
!   +- Relation[value#280] text                                                      
          
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:05 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:05 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:05 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_35 stored as values in memory (estimated size 221.9 KB, free 1968.0 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_35 locally took  10 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_35 without replication took  10 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_35_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.9 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_35_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_35_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_35_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_35_piece0 locally took  1 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_35_piece0 without replication took  2 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 35 from load at UseCase2.java:19
2022-02-10 13:32:05 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:05 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:05 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Got job 17 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Final stage: ResultStage 17 (load at UseCase2.java:19)
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitStage(ResultStage 17 (name=load at UseCase2.java:19;jobs=17))
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting ResultStage 17 (MapPartitionsRDD[84] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:05 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 17)
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_36 stored as values in memory (estimated size 13.9 KB, free 1967.9 MB)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_36 locally took  0 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_36 without replication took  0 ms
2022-02-10 13:32:05 INFO  MemoryStore:54 - Block broadcast_36_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1967.9 MB)
2022-02-10 13:32:05 INFO  BlockManagerInfo:54 - Added broadcast_36_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:05 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_36_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Told master about block broadcast_36_piece0
2022-02-10 13:32:05 DEBUG BlockManager:58 - Put block broadcast_36_piece0 locally took  1 ms
2022-02-10 13:32:05 DEBUG BlockManager:58 - Putting block broadcast_36_piece0 without replication took  1 ms
2022-02-10 13:32:05 INFO  SparkContext:54 - Created broadcast 36 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:05 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[84] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:05 INFO  TaskSchedulerImpl:54 - Adding task set 17.0 with 1 tasks
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Epoch for TaskSet 17.0: 0
2022-02-10 13:32:05 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 17.0: NO_PREF, ANY
2022-02-10 13:32:05 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 13:32:05 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:05 INFO  Executor:54 - Running task 0.0 in stage 17.0 (TID 17)
2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_36
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_36 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:05 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:05 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:05 DEBUG BlockManager:58 - Getting local block broadcast_35
2022-02-10 13:32:05 DEBUG BlockManager:58 - Level for block broadcast_35 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:05 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(494)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 494
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 494
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(30)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 30
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 30
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 30
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 30
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_30_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_30_piece0 of size 181 dropped from memory (free 2063514986)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_30_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 181.0 B, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_30_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_30_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_30
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_30 of size 1048616 dropped from memory (free 2064563602)
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 30, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 30
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(520)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 520
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 520
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(473)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 473
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 473
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(491)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 491
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 491
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(429)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 429
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 429
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(493)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 493
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 493
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(435)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 435
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 435
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(513)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 513
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 513
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(481)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 481
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 481
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(488)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 488
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 488
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(517)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 517
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 517
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(519)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 519
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 519
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(436)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 436
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 436
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(489)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 489
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 489
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(500)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 500
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 500
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(509)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 509
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 509
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(490)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 490
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 490
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(474)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 474
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 474
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(432)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 432
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 432
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(486)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 486
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 486
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(487)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 487
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 487
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(439)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 439
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 439
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(515)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 515
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 515
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(426)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 426
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 426
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(33)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 33
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 33
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 33
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 33
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_33_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_33_piece0 of size 21165 dropped from memory (free 2064584767)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_33_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_33_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_33_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_33
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_33 of size 227232 dropped from memory (free 2064811999)
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 33, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 33
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(472)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 472
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 472
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(508)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 508
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 508
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(506)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 506
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 506
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(514)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 514
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 514
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(497)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 497
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 497
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(484)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 484
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 484
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(440)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 440
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 440
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(476)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 476
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 476
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(434)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 434
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 434
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(469)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 469
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 469
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(483)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 483
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 483
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(496)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 496
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 496
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(521)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 521
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 521
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(478)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 478
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 478
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(431)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 431
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 431
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(485)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 485
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 485
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(516)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 516
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 516
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(427)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 427
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 427
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(441)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 441
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 441
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(502)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 502
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 502
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(507)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 507
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 507
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(468)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 468
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 468
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(482)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 482
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 482
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(499)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 499
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 499
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(503)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 503
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 503
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(438)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 438
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 438
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(512)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 512
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 512
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(479)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 479
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 479
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(437)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 437
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 437
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(492)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 492
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 492
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(32)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 32
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 32
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 32
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 32
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_32_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_32_piece0 of size 7987 dropped from memory (free 2064819986)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_32_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.8 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_32_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_32_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_32
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_32 of size 16648 dropped from memory (free 2064836634)
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 32, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 32
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(34)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 34
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 34
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 34
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 34
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_34
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_34 of size 9144 dropped from memory (free 2064845778)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_34_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_34_piece0 of size 4742 dropped from memory (free 2064850520)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_34_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_34_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_34_piece0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 34, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 34
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(522)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 522
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 522
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(510)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 510
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 510
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(430)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 430
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 430
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(518)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 518
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 518
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(498)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 498
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 498
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(475)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 475
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 475
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(523)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 523
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 523
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(470)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 470
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 470
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(501)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 501
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 501
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(511)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 511
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 511
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(480)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 480
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 480
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(428)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 428
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 428
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(28)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 28
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 28
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 28
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 28
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_28_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_28_piece0 of size 21151 dropped from memory (free 2064871671)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_28_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_28_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_28_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_28
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_28 of size 227072 dropped from memory (free 2065098743)
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 28, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 28
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(477)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 477
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 477
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(442)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 442
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 442
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(505)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 505
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 505
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(433)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 433
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 433
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(471)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 471
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 471
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(31)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning broadcast 31
2022-02-10 13:32:06 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 31
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 31
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing broadcast 31
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_31
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_31 of size 227072 dropped from memory (free 2065325815)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Removing block broadcast_31_piece0
2022-02-10 13:32:06 DEBUG MemoryStore:58 - Block broadcast_31_piece0 of size 21151 dropped from memory (free 2065346966)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Removed broadcast_31_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_31_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_31_piece0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 31, response is 0
2022-02-10 13:32:06 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaned broadcast 31
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(504)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 504
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 504
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(495)
2022-02-10 13:32:06 DEBUG ContextCleaner:58 - Cleaning accumulator 495
2022-02-10 13:32:06 INFO  ContextCleaner:54 - Cleaned accumulator 495
2022-02-10 13:32:06 INFO  Executor:54 - Finished task 0.0 in stage 17.0 (TID 17). 1630 bytes result sent to driver
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_17.0, runningTasks: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 542 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2022-02-10 13:32:06 INFO  DAGScheduler:54 - ResultStage 17 (load at UseCase2.java:19) finished in 0.546 s
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - After removal of stage 17, remaining stages = 0
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Job 17 finished: load at UseCase2.java:19, took 0.557749 s
2022-02-10 13:32:06 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:06 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:06 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 13:32:06 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#298
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#298]
 +- Relation[value#298] text                +- Relation[value#298] text
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#302: java.lang.String   DeserializeToObject cast(value#298 as string).toString, obj#302: java.lang.String
 +- LocalRelation <empty>, [value#298]                                                                                                                                      +- LocalRelation <empty>, [value#298]
          
2022-02-10 13:32:06 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#298
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#298, None)) > 0)
 +- Project [value#298]                     +- Project [value#298]
    +- Relation[value#298] text                +- Relation[value#298] text
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#303: java.lang.String   DeserializeToObject cast(value#298 as string).toString, obj#303: java.lang.String
 +- LocalRelation <empty>, [value#298]                                                                                                                                      +- LocalRelation <empty>, [value#298]
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#304: java.lang.String   DeserializeToObject cast(value#298 as string).toString, obj#304: java.lang.String
 +- LocalRelation <empty>, [value#298]                                                                                                                                      +- LocalRelation <empty>, [value#298]
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#298, None)) > 0)      +- Filter (length(trim(value#298, None)) > 0)
!      +- Project [value#298]                             +- Relation[value#298] text
!         +- Relation[value#298] text               
          
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#298, None)) > 0)
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:06 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_37 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_37 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_37 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_37_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_37_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_37_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_37_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_37_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_37_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 37 from load at UseCase2.java:24
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:06 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Got job 18 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Final stage: ResultStage 18 (load at UseCase2.java:24)
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitStage(ResultStage 18 (name=load at UseCase2.java:24;jobs=18))
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting ResultStage 18 (MapPartitionsRDD[88] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 18)
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_38 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_38 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_38 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_38_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_38_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_38_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_38_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_38_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_38_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 38 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[88] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Adding task set 18.0 with 1 tasks
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Epoch for TaskSet 18.0: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 18.0: NO_PREF, ANY
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_18.0, runningTasks: 0
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:06 INFO  Executor:54 - Running task 0.0 in stage 18.0 (TID 18)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_38
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_38 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:06 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_37
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_37 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  Executor:54 - Finished task 0.0 in stage 18.0 (TID 18). 1162 bytes result sent to driver
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_18.0, runningTasks: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 18.0 (TID 18) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2022-02-10 13:32:06 INFO  DAGScheduler:54 - ResultStage 18 (load at UseCase2.java:24) finished in 0.000 s
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - After removal of stage 18, remaining stages = 0
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Job 18 finished: load at UseCase2.java:24, took 0.011406 s
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#306: java.lang.String   DeserializeToObject cast(value#298 as string).toString, obj#306: java.lang.String
 +- Project [value#298]                                                                                                                                                     +- Project [value#298]
    +- Relation[value#298] text                                                                                                                                                +- Relation[value#298] text
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#298 as string).toString, obj#306: java.lang.String   DeserializeToObject value#298.toString, obj#306: java.lang.String
!+- Project [value#298]                                                              +- Relation[value#298] text
!   +- Relation[value#298] text                                                      
          
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_39 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_39 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_39 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_39_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_39_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_39_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_39_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_39_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_39_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 39 from load at UseCase2.java:24
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:06 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Got job 19 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Final stage: ResultStage 19 (load at UseCase2.java:24)
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitStage(ResultStage 19 (name=load at UseCase2.java:24;jobs=19))
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting ResultStage 19 (MapPartitionsRDD[94] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 19)
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_40 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_40 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_40 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_40_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_40_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_40_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_40_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_40_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_40_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 40 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[94] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Adding task set 19.0 with 1 tasks
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Epoch for TaskSet 19.0: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 19.0: NO_PREF, ANY
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:06 INFO  Executor:54 - Running task 0.0 in stage 19.0 (TID 19)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_40
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_40 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:06 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:06 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_39
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_39 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:06 INFO  Executor:54 - Finished task 0.0 in stage 19.0 (TID 19). 1473 bytes result sent to driver
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_19.0, runningTasks: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 19.0 (TID 19) in 47 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2022-02-10 13:32:06 INFO  DAGScheduler:54 - ResultStage 19 (load at UseCase2.java:24) finished in 0.047 s
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - After removal of stage 19, remaining stages = 0
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Job 19 finished: load at UseCase2.java:24, took 0.055815 s
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (order_date#291 LIKE 2014-01% && isnull(order_id#290))                                                                                                                                     Filter (cast(order_date#291 as string) LIKE 2014-01% && isnull(order_id#290))
 +- Join RightOuter, (order_customer_id#292 = customer_id#308)                                                                                                                                      +- Join RightOuter, (order_customer_id#292 = customer_id#308)
    :- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                :- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
    +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv      +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Cleanup ===
 Aggregate [count(1) AS count#384L]                                                                                                                                                                          Aggregate [count(1) AS count#384L]
 +- Sort [customer_id#308 ASC NULLS FIRST], true                                                                                                                                                             +- Sort [customer_id#308 ASC NULLS FIRST], true
    +- Project [customer_id#308, customer_fname#309, customer_lname#310, customer_email#311, customer_password#312, customer_street#313, customer_city#314, customer_state#315, customer_zipcode#316]           +- Project [customer_id#308, customer_fname#309, customer_lname#310, customer_email#311, customer_password#312, customer_street#313, customer_city#314, customer_state#315, customer_zipcode#316]
       +- Filter (cast(order_date#291 as string) LIKE 2014-01% && isnull(order_id#290))                                                                                                                            +- Filter (cast(order_date#291 as string) LIKE 2014-01% && isnull(order_id#290))
          +- Join RightOuter, (order_customer_id#292 = customer_id#308)                                                                                                                                               +- Join RightOuter, (order_customer_id#292 = customer_id#308)
             :- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                         :- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
             +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv               +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 Aggregate [count(1) AS count#384L]                                                                                                                                                                          Aggregate [count(1) AS count#384L]
!+- Sort [customer_id#308 ASC NULLS FIRST], true                                                                                                                                                             +- Project
!   +- Project [customer_id#308, customer_fname#309, customer_lname#310, customer_email#311, customer_password#312, customer_street#313, customer_city#314, customer_state#315, customer_zipcode#316]           +- Sort [customer_id#308 ASC NULLS FIRST], true
!      +- Filter (cast(order_date#291 as string) LIKE 2014-01% && isnull(order_id#290))                                                                                                                            +- Project [customer_id#308]
!         +- Join RightOuter, (order_customer_id#292 = customer_id#308)                                                                                                                                               +- Join Inner, (order_customer_id#292 = customer_id#308)
!            :- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                         :- Project [order_customer_id#292]
!            +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv               :  +- Filter (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290))
!                                                                                                                                                                                                                        :     +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
!                                                                                                                                                                                                                        +- Project [customer_id#308]
!                                                                                                                                                                                                                           +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Infer Filters ===
 Aggregate [count(1) AS count#384L]                                                                                                                                                                             Aggregate [count(1) AS count#384L]
 +- Project                                                                                                                                                                                                     +- Project
    +- Sort [customer_id#308 ASC NULLS FIRST], true                                                                                                                                                                +- Sort [customer_id#308 ASC NULLS FIRST], true
       +- Project [customer_id#308]                                                                                                                                                                                   +- Project [customer_id#308]
          +- Join Inner, (order_customer_id#292 = customer_id#308)                                                                                                                                                       +- Join Inner, (order_customer_id#292 = customer_id#308)
!            :- Project [order_customer_id#292]                                                                                                                                                                             :- Filter isnotnull(order_customer_id#292)
!            :  +- Filter (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290))                                                                                                                     :  +- Project [order_customer_id#292]
!            :     +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                      :     +- Filter (isnotnull(order_date#291) && (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290)))
!            +- Project [customer_id#308]                                                                                                                                                                                   :        +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
!               +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv               +- Filter isnotnull(customer_id#308)
!                                                                                                                                                                                                                              +- Project [customer_id#308]
!                                                                                                                                                                                                                                 +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization after Inferring Filters ===
 Aggregate [count(1) AS count#384L]                                                                                                                                                                                Aggregate [count(1) AS count#384L]
 +- Project                                                                                                                                                                                                        +- Project
    +- Sort [customer_id#308 ASC NULLS FIRST], true                                                                                                                                                                   +- Sort [customer_id#308 ASC NULLS FIRST], true
       +- Project [customer_id#308]                                                                                                                                                                                      +- Project [customer_id#308]
          +- Join Inner, (order_customer_id#292 = customer_id#308)                                                                                                                                                          +- Join Inner, (order_customer_id#292 = customer_id#308)
!            :- Filter isnotnull(order_customer_id#292)                                                                                                                                                                        :- Project [order_customer_id#292]
!            :  +- Project [order_customer_id#292]                                                                                                                                                                             :  +- Filter ((isnotnull(order_date#291) && (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290))) && isnotnull(order_customer_id#292))
!            :     +- Filter (isnotnull(order_date#291) && (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290)))                                                                                      :     +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
!            :        +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                      +- Project [customer_id#308]
!            +- Filter isnotnull(customer_id#308)                                                                                                                                                                                 +- Filter isnotnull(customer_id#308)
!               +- Project [customer_id#308]                                                                                                                                                                                         +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
!                  +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv   
          
2022-02-10 13:32:06 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch RewriteSubquery ===
 Aggregate [count(1) AS count#384L]                                                                                                                                                                                Aggregate [count(1) AS count#384L]
 +- Project                                                                                                                                                                                                        +- Project
    +- Sort [customer_id#308 ASC NULLS FIRST], true                                                                                                                                                                   +- Sort [customer_id#308 ASC NULLS FIRST], true
       +- Project [customer_id#308]                                                                                                                                                                                      +- Project [customer_id#308]
          +- Join Inner, (order_customer_id#292 = customer_id#308)                                                                                                                                                          +- Join Inner, (order_customer_id#292 = customer_id#308)
             :- Project [order_customer_id#292]                                                                                                                                                                                :- Project [order_customer_id#292]
!            :  +- Filter ((isnotnull(order_date#291) && (StartsWith(cast(order_date#291 as string), 2014-01) && isnull(order_id#290))) && isnotnull(order_customer_id#292))                                                   :  +- Filter (((isnotnull(order_date#291) && StartsWith(cast(order_date#291 as string), 2014-01)) && isnull(order_id#290)) && isnotnull(order_customer_id#292))
             :     +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv                                                                                                                         :     +- Relation[order_id#290,order_date#291,order_customer_id#292,order_status#293] csv
             +- Project [customer_id#308]                                                                                                                                                                                      +- Project [customer_id#308]
                +- Filter isnotnull(customer_id#308)                                                                                                                                                                              +- Filter isnotnull(customer_id#308)
                   +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv                     +- Relation[customer_id#308,customer_fname#309,customer_lname#310,customer_email#311,customer_password#312,customer_street#313,customer_city#314,customer_state#315,customer_zipcode#316] csv
          
2022-02-10 13:32:06 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#292 = customer_id#308))
2022-02-10 13:32:06 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#292) | rightKeys:List(customer_id#308)
2022-02-10 13:32:06 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#292 = customer_id#308))
2022-02-10 13:32:06 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#292) | rightKeys:List(customer_id#308)
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(order_date#291),StartsWith(cast(order_date#291 as string), 2014-01),isnull(order_id#290),isnotnull(order_customer_id#292)
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_id: int, order_date: timestamp, order_customer_id: int ... 1 more fields>
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(order_date),IsNull(order_id),IsNotNull(order_customer_id)
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(customer_id#308)
2022-02-10 13:32:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int>
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(customer_id)
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: IsNull(none),IsNotNull(none),IsNotNull(none)
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(none)
2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         filter_mutableStateArray_0[1].reset();
/* 039 */
/* 040 */         if (false) {
/* 041 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 042 */         } else {
/* 043 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 044 */         }
/* 045 */         append((filter_mutableStateArray_0[1].getRow()));
/* 046 */
/* 047 */       } while(false);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */   }
/* 051 */
/* 052 */ }

2022-02-10 13:32:06 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 031 */         int scan_value_0 = scan_isNull_0 ?
/* 032 */         -1 : (scan_row_0.getInt(0));
/* 033 */
/* 034 */         if (!(!scan_isNull_0)) continue;
/* 035 */
/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 037 */
/* 038 */         filter_mutableStateArray_0[1].reset();
/* 039 */
/* 040 */         if (false) {
/* 041 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 042 */         } else {
/* 043 */           filter_mutableStateArray_0[1].write(0, scan_value_0);
/* 044 */         }
/* 045 */         append((filter_mutableStateArray_0[1].getRow()));
/* 046 */
/* 047 */       } while(false);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */   }
/* 051 */
/* 052 */ }

2022-02-10 13:32:06 INFO  CodeGenerator:54 - Code generated in 5.1168 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_41 stored as values in memory (estimated size 221.8 KB, free 1968.9 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_41 locally took  15 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_41 without replication took  15 ms
2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_41_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 13:32:06 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private scala.collection.Iterator inputadapter_input_0;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */
/* 023 */     inputadapter_input_0 = inputs[0];
/* 024 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 025 */
/* 026 */   }
/* 027 */
/* 028 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 029 */     // initialize aggregation buffer
/* 030 */     agg_bufIsNull_0 = false;
/* 031 */     agg_bufValue_0 = 0L;
/* 032 */
/* 033 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 035 */       long inputadapter_value_0 = inputadapter_row_0.getLong(0);
/* 036 */
/* 037 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, long agg_expr_0_0) throws java.io.IOException {
/* 044 */     // do aggregate
/* 045 */     // common sub-expressions
/* 046 */
/* 047 */     // evaluate aggregate function
/* 048 */     long agg_value_3 = -1L;
/* 049 */     agg_value_3 = agg_bufValue_0 + agg_expr_0_0;
/* 050 */     // update aggregation buffer
/* 051 */     agg_bufIsNull_0 = false;
/* 052 */     agg_bufValue_0 = agg_value_3;
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     while (!agg_initAgg_0) {
/* 058 */       agg_initAgg_0 = true;
/* 059 */       long agg_beforeAgg_0 = System.nanoTime();
/* 060 */       agg_doAggregateWithoutKey_0();
/* 061 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 062 */
/* 063 */       // output the result
/* 064 */
/* 065 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 066 */       agg_mutableStateArray_0[0].reset();
/* 067 */
/* 068 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 069 */
/* 070 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 071 */       append((agg_mutableStateArray_0[0].getRow()));
/* 072 */     }
/* 073 */   }
/* 074 */
/* 075 */ }

2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_41_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_41_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_41_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_41_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_41_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 41 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 INFO  CodeGenerator:54 - Code generated in 8.6252 ms
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:06 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Got job 20 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Final stage: ResultStage 20 (run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitStage(ResultStage 20 (name=run at ThreadPoolExecutor.java:1149;jobs=20))
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting ResultStage 20 (MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 20)
2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private boolean sort_needToSort_0;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 015 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 016 */   private scala.collection.Iterator inputadapter_input_0;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 018 */
/* 019 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 020 */     this.references = references;
/* 021 */   }
/* 022 */
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     sort_needToSort_0 = true;
/* 028 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 029 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 036 */     // initialize aggregation buffer
/* 037 */     agg_bufIsNull_0 = false;
/* 038 */     agg_bufValue_0 = 0L;
/* 039 */
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       agg_doConsume_0();
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */
/* 059 */   }
/* 060 */
/* 061 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 062 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 063 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 064 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 065 */       if (shouldStop()) return;
/* 066 */     }
/* 067 */
/* 068 */   }
/* 069 */
/* 070 */   private void agg_doConsume_0() throws java.io.IOException {
/* 071 */     // do aggregate
/* 072 */     // common sub-expressions
/* 073 */
/* 074 */     // evaluate aggregate function
/* 075 */     long agg_value_1 = -1L;
/* 076 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 077 */     // update aggregation buffer
/* 078 */     agg_bufIsNull_0 = false;
/* 079 */     agg_bufValue_0 = agg_value_1;
/* 080 */
/* 081 */   }
/* 082 */
/* 083 */   protected void processNext() throws java.io.IOException {
/* 084 */     while (!agg_initAgg_0) {
/* 085 */       agg_initAgg_0 = true;
/* 086 */       long agg_beforeAgg_0 = System.nanoTime();
/* 087 */       agg_doAggregateWithoutKey_0();
/* 088 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 089 */
/* 090 */       // output the result
/* 091 */
/* 092 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 093 */       agg_mutableStateArray_0[0].reset();
/* 094 */
/* 095 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 096 */
/* 097 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 098 */       append((agg_mutableStateArray_0[0].getRow()));
/* 099 */     }
/* 100 */   }
/* 101 */
/* 102 */ }

2022-02-10 13:32:06 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean agg_initAgg_0;
/* 010 */   private boolean agg_bufIsNull_0;
/* 011 */   private long agg_bufValue_0;
/* 012 */   private boolean sort_needToSort_0;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 015 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 016 */   private scala.collection.Iterator inputadapter_input_0;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 018 */
/* 019 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 020 */     this.references = references;
/* 021 */   }
/* 022 */
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */
/* 027 */     sort_needToSort_0 = true;
/* 028 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 029 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 030 */     inputadapter_input_0 = inputs[0];
/* 031 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private void agg_doAggregateWithoutKey_0() throws java.io.IOException {
/* 036 */     // initialize aggregation buffer
/* 037 */     agg_bufIsNull_0 = false;
/* 038 */     agg_bufValue_0 = 0L;
/* 039 */
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       agg_doConsume_0();
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */
/* 059 */   }
/* 060 */
/* 061 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 062 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 063 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 064 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 065 */       if (shouldStop()) return;
/* 066 */     }
/* 067 */
/* 068 */   }
/* 069 */
/* 070 */   private void agg_doConsume_0() throws java.io.IOException {
/* 071 */     // do aggregate
/* 072 */     // common sub-expressions
/* 073 */
/* 074 */     // evaluate aggregate function
/* 075 */     long agg_value_1 = -1L;
/* 076 */     agg_value_1 = agg_bufValue_0 + 1L;
/* 077 */     // update aggregation buffer
/* 078 */     agg_bufIsNull_0 = false;
/* 079 */     agg_bufValue_0 = agg_value_1;
/* 080 */
/* 081 */   }
/* 082 */
/* 083 */   protected void processNext() throws java.io.IOException {
/* 084 */     while (!agg_initAgg_0) {
/* 085 */       agg_initAgg_0 = true;
/* 086 */       long agg_beforeAgg_0 = System.nanoTime();
/* 087 */       agg_doAggregateWithoutKey_0();
/* 088 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - agg_beforeAgg_0) / 1000000);
/* 089 */
/* 090 */       // output the result
/* 091 */
/* 092 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 093 */       agg_mutableStateArray_0[0].reset();
/* 094 */
/* 095 */       agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 096 */
/* 097 */       agg_mutableStateArray_0[0].write(0, agg_bufValue_0);
/* 098 */       append((agg_mutableStateArray_0[0].getRow()));
/* 099 */     }
/* 100 */   }
/* 101 */
/* 102 */ }

2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_42 stored as values in memory (estimated size 11.1 KB, free 1968.9 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_42 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_42 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1968.9 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_42_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 6.0 KB, free: 1970.3 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_42_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_42_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_42_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_42_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 42 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[97] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Adding task set 20.0 with 1 tasks
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Epoch for TaskSet 20.0: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 20.0: NO_PREF, ANY
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 13:32:06 INFO  Executor:54 - Running task 0.0 in stage 20.0 (TID 20)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_42
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_42 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:06 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:06 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:06 INFO  CodeGenerator:54 - Code generated in 7.8357 ms
2022-02-10 13:32:06 INFO  CodeGenerator:54 - Code generated in 19.0822 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_41
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_41 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  Executor:54 - Finished task 0.0 in stage 20.0 (TID 20). 65566 bytes result sent to driver
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_20.0, runningTasks: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 20.0 (TID 20) in 49 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2022-02-10 13:32:06 INFO  DAGScheduler:54 - ResultStage 20 (run at ThreadPoolExecutor.java:1149) finished in 0.049 s
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - After removal of stage 20, remaining stages = 0
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Job 20 finished: run at ThreadPoolExecutor.java:1149, took 0.059111 s
2022-02-10 13:32:06 DEBUG TaskMemoryManager:224 - Task 0 acquired 1280.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@2c112d26
2022-02-10 13:32:06 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:06 DEBUG TaskMemoryManager:224 - Task 0 acquired 512.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@2c112d26
2022-02-10 13:32:06 DEBUG TaskMemoryManager:233 - Task 0 release 256.0 KB from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@2c112d26
2022-02-10 13:32:06 DEBUG TaskMemoryManager:224 - Task 0 acquired 97.1 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@2c112d26
2022-02-10 13:32:06 DEBUG TaskMemoryManager:233 - Task 0 release 512.0 KB from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@2c112d26
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_43 stored as values in memory (estimated size 1121.2 KB, free 1967.8 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_43 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_43 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_43_piece0 stored as bytes in memory (estimated size 124.3 KB, free 1967.7 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_43_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 124.3 KB, free: 1970.1 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_43_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_43_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_43_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_43_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 43 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_43
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_43 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 023 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 024 */
/* 025 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[4] /* broadcast */).value()).asReadOnlyCopy();
/* 026 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 027 */
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 029 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 030 */
/* 031 */   }
/* 032 */
/* 033 */   private void bhj_doConsume_0(int bhj_expr_0_0, boolean bhj_exprIsNull_0_0) throws java.io.IOException {
/* 034 */     // generate join key for stream side
/* 035 */     boolean bhj_isNull_1 = bhj_exprIsNull_0_0;
/* 036 */     long bhj_value_1 = -1L;
/* 037 */     if (!bhj_exprIsNull_0_0) {
/* 038 */       bhj_value_1 = (long) bhj_expr_0_0;
/* 039 */     }
/* 040 */     // find matches from HashedRelation
/* 041 */     UnsafeRow bhj_matched_0 = bhj_isNull_1 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_1);
/* 042 */     if (bhj_matched_0 != null) {
/* 043 */       {
/* 044 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 045 */
/* 046 */         boolean bhj_isNull_3 = bhj_matched_0.isNullAt(0);
/* 047 */         int bhj_value_3 = bhj_isNull_3 ?
/* 048 */         -1 : (bhj_matched_0.getInt(0));
/* 049 */         filter_mutableStateArray_0[4].reset();
/* 050 */
/* 051 */         filter_mutableStateArray_0[4].zeroOutNullBytes();
/* 052 */
/* 053 */         if (bhj_isNull_3) {
/* 054 */           filter_mutableStateArray_0[4].setNullAt(0);
/* 055 */         } else {
/* 056 */           filter_mutableStateArray_0[4].write(0, bhj_value_3);
/* 057 */         }
/* 058 */         append((filter_mutableStateArray_0[4].getRow()));
/* 059 */
/* 060 */       }
/* 061 */     }
/* 062 */
/* 063 */   }
/* 064 */
/* 065 */   protected void processNext() throws java.io.IOException {
/* 066 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 067 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 068 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 069 */       do {
/* 070 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 071 */         long scan_value_1 = scan_isNull_1 ?
/* 072 */         -1L : (scan_row_0.getLong(1));
/* 073 */
/* 074 */         if (!(!scan_isNull_1)) continue;
/* 075 */
/* 076 */         boolean filter_isNull_3 = scan_isNull_1;
/* 077 */         UTF8String filter_value_3 = null;
/* 078 */         if (!scan_isNull_1) {
/* 079 */           filter_value_3 = UTF8String.fromString(
/* 080 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 081 */         }
/* 082 */
/* 083 */         boolean filter_value_2 = false;
/* 084 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 085 */         if (!filter_value_2) continue;
/* 086 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 087 */         int scan_value_0 = scan_isNull_0 ?
/* 088 */         -1 : (scan_row_0.getInt(0));
/* 089 */
/* 090 */         if (!scan_isNull_0) continue;
/* 091 */
/* 092 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 093 */         int scan_value_2 = scan_isNull_2 ?
/* 094 */         -1 : (scan_row_0.getInt(2));
/* 095 */
/* 096 */         if (!(!scan_isNull_2)) continue;
/* 097 */
/* 098 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 099 */
/* 100 */         bhj_doConsume_0(scan_value_2, false);
/* 101 */
/* 102 */       } while(false);
/* 103 */       if (shouldStop()) return;
/* 104 */     }
/* 105 */   }
/* 106 */
/* 107 */ }

2022-02-10 13:32:06 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 023 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 024 */
/* 025 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[4] /* broadcast */).value()).asReadOnlyCopy();
/* 026 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 027 */
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 029 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 030 */
/* 031 */   }
/* 032 */
/* 033 */   private void bhj_doConsume_0(int bhj_expr_0_0, boolean bhj_exprIsNull_0_0) throws java.io.IOException {
/* 034 */     // generate join key for stream side
/* 035 */     boolean bhj_isNull_1 = bhj_exprIsNull_0_0;
/* 036 */     long bhj_value_1 = -1L;
/* 037 */     if (!bhj_exprIsNull_0_0) {
/* 038 */       bhj_value_1 = (long) bhj_expr_0_0;
/* 039 */     }
/* 040 */     // find matches from HashedRelation
/* 041 */     UnsafeRow bhj_matched_0 = bhj_isNull_1 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_1);
/* 042 */     if (bhj_matched_0 != null) {
/* 043 */       {
/* 044 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 045 */
/* 046 */         boolean bhj_isNull_3 = bhj_matched_0.isNullAt(0);
/* 047 */         int bhj_value_3 = bhj_isNull_3 ?
/* 048 */         -1 : (bhj_matched_0.getInt(0));
/* 049 */         filter_mutableStateArray_0[4].reset();
/* 050 */
/* 051 */         filter_mutableStateArray_0[4].zeroOutNullBytes();
/* 052 */
/* 053 */         if (bhj_isNull_3) {
/* 054 */           filter_mutableStateArray_0[4].setNullAt(0);
/* 055 */         } else {
/* 056 */           filter_mutableStateArray_0[4].write(0, bhj_value_3);
/* 057 */         }
/* 058 */         append((filter_mutableStateArray_0[4].getRow()));
/* 059 */
/* 060 */       }
/* 061 */     }
/* 062 */
/* 063 */   }
/* 064 */
/* 065 */   protected void processNext() throws java.io.IOException {
/* 066 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 067 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 068 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 069 */       do {
/* 070 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 071 */         long scan_value_1 = scan_isNull_1 ?
/* 072 */         -1L : (scan_row_0.getLong(1));
/* 073 */
/* 074 */         if (!(!scan_isNull_1)) continue;
/* 075 */
/* 076 */         boolean filter_isNull_3 = scan_isNull_1;
/* 077 */         UTF8String filter_value_3 = null;
/* 078 */         if (!scan_isNull_1) {
/* 079 */           filter_value_3 = UTF8String.fromString(
/* 080 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 081 */         }
/* 082 */
/* 083 */         boolean filter_value_2 = false;
/* 084 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 085 */         if (!filter_value_2) continue;
/* 086 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 087 */         int scan_value_0 = scan_isNull_0 ?
/* 088 */         -1 : (scan_row_0.getInt(0));
/* 089 */
/* 090 */         if (!scan_isNull_0) continue;
/* 091 */
/* 092 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 093 */         int scan_value_2 = scan_isNull_2 ?
/* 094 */         -1 : (scan_row_0.getInt(2));
/* 095 */
/* 096 */         if (!(!scan_isNull_2)) continue;
/* 097 */
/* 098 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 099 */
/* 100 */         bhj_doConsume_0(scan_value_2, false);
/* 101 */
/* 102 */       } while(false);
/* 103 */       if (shouldStop()) return;
/* 104 */     }
/* 105 */   }
/* 106 */
/* 107 */ }

2022-02-10 13:32:06 INFO  CodeGenerator:54 - Code generated in 11.1733 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_44 stored as values in memory (estimated size 221.8 KB, free 1967.5 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_44 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_44 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_44_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.5 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_44_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_44_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_44_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_44_piece0 locally took  16 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_44_piece0 without replication took  16 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 44 from count at UseCase2.java:58
2022-02-10 13:32:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:06 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$9.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(scala.Product2)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$13.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.sampleSizePerPartition$2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.reflect.ClassTag org.apache.spark.RangePartitioner$$anonfun$13.evidence$5$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.shift$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.RangePartitioner$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[102] at count at UseCase2.java:58
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[102] at count at UseCase2.java:58)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[102] at count at UseCase2.java:58
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[102] at count at UseCase2.java:58)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:06 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:06 INFO  SparkContext:54 - Starting job: count at UseCase2.java:58
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Got job 21 (count at UseCase2.java:58) with 1 output partitions
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Final stage: ResultStage 21 (count at UseCase2.java:58)
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitStage(ResultStage 21 (name=count at UseCase2.java:58;jobs=21))
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting ResultStage 21 (MapPartitionsRDD[102] at count at UseCase2.java:58), which has no missing parents
2022-02-10 13:32:06 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 21)
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_45 stored as values in memory (estimated size 14.3 KB, free 1967.4 MB)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_45 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_45 without replication took  0 ms
2022-02-10 13:32:06 INFO  MemoryStore:54 - Block broadcast_45_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1967.4 MB)
2022-02-10 13:32:06 INFO  BlockManagerInfo:54 - Added broadcast_45_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.4 KB, free: 1970.1 MB)
2022-02-10 13:32:06 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_45_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Told master about block broadcast_45_piece0
2022-02-10 13:32:06 DEBUG BlockManager:58 - Put block broadcast_45_piece0 locally took  0 ms
2022-02-10 13:32:06 DEBUG BlockManager:58 - Putting block broadcast_45_piece0 without replication took  0 ms
2022-02-10 13:32:06 INFO  SparkContext:54 - Created broadcast 45 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[102] at count at UseCase2.java:58) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:06 INFO  TaskSchedulerImpl:54 - Adding task set 21.0 with 1 tasks
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Epoch for TaskSet 21.0: 0
2022-02-10 13:32:06 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 21.0: NO_PREF, ANY
2022-02-10 13:32:06 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_21.0, runningTasks: 0
2022-02-10 13:32:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 13:32:06 INFO  Executor:54 - Running task 0.0 in stage 21.0 (TID 21)
2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_45
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_45 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:06 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:06 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

2022-02-10 13:32:06 DEBUG BlockManager:58 - Getting local block broadcast_44
2022-02-10 13:32:06 DEBUG BlockManager:58 - Level for block broadcast_44 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(612)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 612
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 612
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(609)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 609
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 609
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(580)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 580
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 580
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(606)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 606
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 606
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(602)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 602
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 602
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(545)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 545
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 545
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(608)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 608
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 608
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(674)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 674
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 674
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(532)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 532
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 532
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(36)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 36
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 36
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 36
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 36
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_36_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_36_piece0 of size 7737 dropped from memory (free 2063010367)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_36_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_36_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_36_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_36
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_36 of size 14256 dropped from memory (free 2063024623)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 36, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 36
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(584)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 584
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 584
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(661)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 661
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 661
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(539)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 539
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 539
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(677)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 677
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 677
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(42)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 42
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 42
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 42
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 42
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_42_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_42_piece0 of size 6151 dropped from memory (free 2063030774)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_42_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 6.0 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_42_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_42_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_42
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_42 of size 11376 dropped from memory (free 2063042150)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 42, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 42
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(601)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 601
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 601
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(573)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 573
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 573
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(579)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 579
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 579
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(546)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 546
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 546
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(576)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 576
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 576
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(664)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 664
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 664
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(655)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 655
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 655
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(38)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 38
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 38
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 38
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 38
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_38
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_38 of size 9144 dropped from memory (free 2063051294)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_38_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_38_piece0 of size 4742 dropped from memory (free 2063056036)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_38_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_38_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_38_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 38, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 38
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(669)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 669
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 669
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(572)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 572
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 572
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(568)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 568
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 568
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(654)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 654
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 654
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(589)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 589
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 589
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(547)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 547
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 547
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(566)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 566
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 566
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(578)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 578
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 578
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(656)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 656
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 656
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(665)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 665
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 665
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(556)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 556
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 556
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(671)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 671
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 671
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(585)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 585
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 585
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(538)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 538
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 538
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(563)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 563
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 563
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(600)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 600
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 600
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(552)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 552
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 552
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(540)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 540
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 540
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(527)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 527
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 527
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(668)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 668
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 668
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(583)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 583
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 583
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(570)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 570
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 570
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(586)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 586
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 586
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(672)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 672
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 672
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(569)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 569
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 569
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(37)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 37
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 37
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 37
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 37
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_37_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_37_piece0 of size 21165 dropped from memory (free 2063077201)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_37_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_37_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_37_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_37
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_37 of size 227232 dropped from memory (free 2063304433)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 37, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 37
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(528)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 528
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 528
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(593)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 593
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 593
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(594)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 594
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 594
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(673)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 673
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 673
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(595)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 595
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 595
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(534)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 534
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 534
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(571)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 571
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 571
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(667)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 667
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 667
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(607)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 607
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 607
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(588)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 588
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 588
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(541)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 541
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 541
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(574)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 574
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 574
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(544)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 544
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 544
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(615)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 615
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 615
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(619)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 619
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 619
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(604)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 604
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 604
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(613)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 613
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 613
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(557)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 557
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 557
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(526)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 526
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 526
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(555)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 555
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 555
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(587)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 587
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 587
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(559)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 559
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 559
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(550)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 550
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 550
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(533)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 533
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 533
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(543)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 543
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 543
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(575)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 575
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 575
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(39)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 39
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 39
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 39
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 39
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_39
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_39 of size 227232 dropped from memory (free 2063531665)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_39_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_39_piece0 of size 21165 dropped from memory (free 2063552830)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_39_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_39_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_39_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 39, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 39
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(530)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 530
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 530
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(605)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 605
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 605
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(598)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 598
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 598
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(529)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 529
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 529
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(554)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 554
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 554
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(653)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 653
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 653
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(657)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 657
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 657
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(610)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 610
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 610
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(553)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 553
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 553
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(551)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 551
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 551
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(564)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 564
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 564
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(40)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 40
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 40
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 40
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 40
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_40_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_40_piece0 of size 7775 dropped from memory (free 2063560605)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_40_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_40_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_40_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_40
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_40 of size 14368 dropped from memory (free 2063574973)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 40, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 40
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(562)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 562
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 562
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(599)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 599
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 599
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(581)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 581
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 581
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(548)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 548
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 548
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(558)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 558
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 558
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(603)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 603
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 603
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(565)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 565
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 565
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(537)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 537
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 537
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(676)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 676
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 676
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(582)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 582
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 582
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(616)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 616
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 616
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(611)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 611
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 611
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(670)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 670
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 670
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(561)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 561
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 561
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(531)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 531
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 531
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(549)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 549
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 549
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(535)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 535
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 535
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(658)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 658
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 658
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(663)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 663
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 663
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(524)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 524
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 524
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(592)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 592
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 592
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(560)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 560
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 560
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(525)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 525
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 525
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(660)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 660
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 660
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(591)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 591
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 591
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(675)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 675
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 675
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(542)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 542
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 542
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(659)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 659
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 659
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(614)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 614
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 614
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(666)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 666
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 666
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(662)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 662
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 662
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(536)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 536
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 536
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(577)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 577
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 577
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(590)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 590
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 590
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(567)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 567
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 567
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(35)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 35
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 35
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 35
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 35
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_35_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_35_piece0 of size 21165 dropped from memory (free 2063596138)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_35_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_35_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_35_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_35
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_35 of size 227232 dropped from memory (free 2063823370)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 35, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 35
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(597)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 597
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 597
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(596)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 596
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 596
2022-02-10 13:32:07 INFO  Executor:54 - Finished task 0.0 in stage 21.0 (TID 21). 1541 bytes result sent to driver
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_21.0, runningTasks: 0
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 21.0 (TID 21) in 430 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2022-02-10 13:32:07 INFO  DAGScheduler:54 - ResultStage 21 (count at UseCase2.java:58) finished in 0.430 s
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - After removal of stage 21, remaining stages = 0
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Job 21 finished: count at UseCase2.java:58, took 0.437823 s
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[109] at count at UseCase2.java:58
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[109] at count at UseCase2.java:58)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[109] at count at UseCase2.java:58
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[109] at count at UseCase2.java:58)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:07 INFO  SparkContext:54 - Starting job: count at UseCase2.java:58
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Registering RDD 103 (count at UseCase2.java:58) as input to shuffle 0
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Registering RDD 106 (count at UseCase2.java:58) as input to shuffle 1
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Got job 22 (count at UseCase2.java:58) with 1 output partitions
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Final stage: ResultStage 24 (count at UseCase2.java:58)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 23)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 23)
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ResultStage 24 (name=count at UseCase2.java:58;jobs=22))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 23)
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 23 (name=count at UseCase2.java:58;jobs=22))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 22)
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 22 (name=count at UseCase2.java:58;jobs=22))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 22 (MapPartitionsRDD[103] at count at UseCase2.java:58), which has no missing parents
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 22)
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_46 stored as values in memory (estimated size 16.4 KB, free 1968.2 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_46 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_46 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.4 KB, free 1968.2 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_46_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 8.4 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_46_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_46_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_46_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_46_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 46 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[103] at count at UseCase2.java:58) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Adding task set 22.0 with 1 tasks
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Epoch for TaskSet 22.0: 0
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 22.0: NO_PREF, ANY
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_22.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 8307 bytes)
2022-02-10 13:32:07 INFO  Executor:54 - Running task 0.0 in stage 22.0 (TID 22)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_46
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_46 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:07 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:07 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

2022-02-10 13:32:07 INFO  Executor:54 - Finished task 0.0 in stage 22.0 (TID 22). 1482 bytes result sent to driver
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_22.0, runningTasks: 0
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 22.0 (TID 22) in 394 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 13:32:07 INFO  DAGScheduler:54 - ShuffleMapStage 22 (count at UseCase2.java:58) finished in 0.394 s
2022-02-10 13:32:07 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 13:32:07 INFO  DAGScheduler:54 - running: Set()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 24, ShuffleMapStage 23)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 13:32:07 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 1
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 23 (name=count at UseCase2.java:58;jobs=22))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 23 (MapPartitionsRDD[106] at count at UseCase2.java:58), which has no missing parents
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 23)
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_47 stored as values in memory (estimated size 27.5 KB, free 1968.2 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_47 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_47 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_47_piece0 stored as bytes in memory (estimated size 13.1 KB, free 1968.2 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_47_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 13.1 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_47_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_47_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_47_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_47_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 47 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[106] at count at UseCase2.java:58) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Adding task set 23.0 with 1 tasks
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Epoch for TaskSet 23.0: 1
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 23.0: NO_PREF, ANY
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_23.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 7756 bytes)
2022-02-10 13:32:07 INFO  Executor:54 - Running task 0.0 in stage 23.0 (TID 23)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_47
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_47 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 0, partitions 0-1
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 13:32:07 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 13:32:07 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 13:32:07 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:07 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:07 DEBUG CodeGenerator:58 - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:07 INFO  CodeGenerator:54 - Code generated in 3.859599 ms
2022-02-10 13:32:07 DEBUG TaskMemoryManager:224 - Task 23 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@260f2e4
2022-02-10 13:32:07 DEBUG TaskMemoryManager:233 - Task 23 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@260f2e4
2022-02-10 13:32:07 INFO  Executor:54 - Finished task 0.0 in stage 23.0 (TID 23). 3625 bytes result sent to driver
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_23.0, runningTasks: 0
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 23.0 (TID 23) in 63 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 13:32:07 INFO  DAGScheduler:54 - ShuffleMapStage 23 (count at UseCase2.java:58) finished in 0.063 s
2022-02-10 13:32:07 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 13:32:07 INFO  DAGScheduler:54 - running: Set()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - waiting: Set(ResultStage 24)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 13:32:07 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 2
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ResultStage 24 (name=count at UseCase2.java:58;jobs=22))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting ResultStage 24 (MapPartitionsRDD[109] at count at UseCase2.java:58), which has no missing parents
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 24)
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_48 stored as values in memory (estimated size 7.3 KB, free 1968.1 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_48 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_48 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_48_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1968.1 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_48_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 3.9 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_48_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_48_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_48_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_48_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 48 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[109] at count at UseCase2.java:58) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Adding task set 24.0 with 1 tasks
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Epoch for TaskSet 24.0: 2
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 24.0: ANY
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_24.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, ANY, 7767 bytes)
2022-02-10 13:32:07 INFO  Executor:54 - Running task 0.0 in stage 24.0 (TID 24)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_48
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_48 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 1, partitions 0-1
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 13:32:07 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2022-02-10 13:32:07 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: shuffle_1_0_0
2022-02-10 13:32:07 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 13:32:07 INFO  Executor:54 - Finished task 0.0 in stage 24.0 (TID 24). 1689 bytes result sent to driver
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_24.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 24.0 (TID 24) in 15 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2022-02-10 13:32:07 INFO  DAGScheduler:54 - ResultStage 24 (count at UseCase2.java:58) finished in 0.031 s
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - After removal of stage 23, remaining stages = 2
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - After removal of stage 22, remaining stages = 1
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - After removal of stage 24, remaining stages = 0
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Job 22 finished: count at UseCase2.java:58, took 0.494833 s
2022-02-10 13:32:07 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:07 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:07 INFO  InMemoryFileIndex:54 - It took 1 ms to list leaf files for 2 paths.
2022-02-10 13:32:07 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#392
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#392]
 +- Relation[value#392] text                +- Relation[value#392] text
          
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#396: java.lang.String   DeserializeToObject cast(value#392 as string).toString, obj#396: java.lang.String
 +- LocalRelation <empty>, [value#392]                                                                                                                                      +- LocalRelation <empty>, [value#392]
          
2022-02-10 13:32:07 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#392
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#392, None)) > 0)
 +- Project [value#392]                     +- Project [value#392]
    +- Relation[value#392] text                +- Relation[value#392] text
          
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#397: java.lang.String   DeserializeToObject cast(value#392 as string).toString, obj#397: java.lang.String
 +- LocalRelation <empty>, [value#392]                                                                                                                                      +- LocalRelation <empty>, [value#392]
          
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#398: java.lang.String   DeserializeToObject cast(value#392 as string).toString, obj#398: java.lang.String
 +- LocalRelation <empty>, [value#392]                                                                                                                                      +- LocalRelation <empty>, [value#392]
          
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#392, None)) > 0)      +- Filter (length(trim(value#392, None)) > 0)
!      +- Project [value#392]                             +- Relation[value#392] text
!         +- Relation[value#392] text               
          
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#392, None)) > 0)
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:07 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:07 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:07 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_49 stored as values in memory (estimated size 221.9 KB, free 1967.9 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_49 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_49 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_49_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.9 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_49_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_49_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_49_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_49_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_49_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 49 from load at UseCase2.java:19
2022-02-10 13:32:07 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:07 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Got job 23 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Final stage: ResultStage 25 (load at UseCase2.java:19)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ResultStage 25 (name=load at UseCase2.java:19;jobs=23))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting ResultStage 25 (MapPartitionsRDD[113] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 25)
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_50 stored as values in memory (estimated size 8.9 KB, free 1967.9 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_50 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_50 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_50_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1967.9 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_50_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_50_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_50_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_50_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_50_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 50 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[113] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Adding task set 25.0 with 1 tasks
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Epoch for TaskSet 25.0: 2
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 25.0: NO_PREF, ANY
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_25.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:07 INFO  Executor:54 - Running task 0.0 in stage 25.0 (TID 25)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_50
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_50 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:07 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_49
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_49 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 INFO  Executor:54 - Finished task 0.0 in stage 25.0 (TID 25). 1128 bytes result sent to driver
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_25.0, runningTasks: 0
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 25.0 (TID 25) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2022-02-10 13:32:07 INFO  DAGScheduler:54 - ResultStage 25 (load at UseCase2.java:19) finished in 0.000 s
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - After removal of stage 25, remaining stages = 0
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Job 23 finished: load at UseCase2.java:19, took 0.014739 s
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#400: java.lang.String   DeserializeToObject cast(value#392 as string).toString, obj#400: java.lang.String
 +- Project [value#392]                                                                                                                                                     +- Project [value#392]
    +- Relation[value#392] text                                                                                                                                                +- Relation[value#392] text
          
2022-02-10 13:32:07 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#392 as string).toString, obj#400: java.lang.String   DeserializeToObject value#392.toString, obj#400: java.lang.String
!+- Project [value#392]                                                              +- Relation[value#392] text
!   +- Relation[value#392] text                                                      
          
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:07 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:07 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:07 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_51 stored as values in memory (estimated size 221.9 KB, free 1967.7 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_51 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_51 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_51_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1967.7 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_51_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_51_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_51_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_51_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_51_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 51 from load at UseCase2.java:19
2022-02-10 13:32:07 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:07 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:07 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Got job 24 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Final stage: ResultStage 26 (load at UseCase2.java:19)
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitStage(ResultStage 26 (name=load at UseCase2.java:19;jobs=24))
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting ResultStage 26 (MapPartitionsRDD[119] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:07 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 26)
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_52 stored as values in memory (estimated size 13.9 KB, free 1967.6 MB)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_52 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_52 without replication took  0 ms
2022-02-10 13:32:07 INFO  MemoryStore:54 - Block broadcast_52_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1967.6 MB)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Added broadcast_52_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_52_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_52_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Put block broadcast_52_piece0 locally took  0 ms
2022-02-10 13:32:07 DEBUG BlockManager:58 - Putting block broadcast_52_piece0 without replication took  0 ms
2022-02-10 13:32:07 INFO  SparkContext:54 - Created broadcast 52 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[119] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:07 INFO  TaskSchedulerImpl:54 - Adding task set 26.0 with 1 tasks
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Epoch for TaskSet 26.0: 2
2022-02-10 13:32:07 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 26.0: NO_PREF, ANY
2022-02-10 13:32:07 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_26.0, runningTasks: 0
2022-02-10 13:32:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:07 INFO  Executor:54 - Running task 0.0 in stage 26.0 (TID 26)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_52
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_52 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:07 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:07 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:07 DEBUG BlockManager:58 - Getting local block broadcast_51
2022-02-10 13:32:07 DEBUG BlockManager:58 - Level for block broadcast_51 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(688)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 688
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 688
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(702)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 702
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 702
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(693)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 693
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 693
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(649)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 649
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 649
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(714)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 714
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 714
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(745)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 745
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 745
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(747)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 747
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 747
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(781)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 781
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 781
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(771)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 771
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 771
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(47)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 47
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 47
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 47
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 47
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_47
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_47 of size 28152 dropped from memory (free 2063240404)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_47_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_47_piece0 of size 13450 dropped from memory (free 2063253854)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_47_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 13.1 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_47_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_47_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 47, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 47
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(797)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 797
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 797
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(780)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 780
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 780
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(0)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning shuffle 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 0
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned shuffle 0
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(618)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 618
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 618
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(738)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 738
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 738
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(752)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 752
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 752
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(737)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 737
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 737
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(681)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 681
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 681
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(720)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 720
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 720
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(755)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 755
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 755
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(636)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 636
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 636
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(777)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 777
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 777
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(617)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 617
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 617
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(761)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 761
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 761
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(626)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 626
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 626
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(698)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 698
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 698
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(724)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 724
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 724
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(805)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 805
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 805
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(768)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 768
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 768
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(800)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 800
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 800
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(703)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 703
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 703
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(700)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 700
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 700
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(796)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 796
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 796
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(678)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 678
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 678
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(748)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 748
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 748
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(48)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 48
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 48
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 48
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 48
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_48
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_48 of size 7432 dropped from memory (free 2063261286)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_48_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 0, response is true
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_48_piece0 of size 4007 dropped from memory (free 2063265293)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_48_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 3.9 KB, free: 1970.1 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_48_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_48_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 48, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 48
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(640)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 640
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 640
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(742)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 742
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 742
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(716)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 716
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 716
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(758)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 758
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 758
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(680)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 680
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 680
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(41)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 41
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 41
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 41
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 41
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_41_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_41_piece0 of size 21151 dropped from memory (free 2063286444)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_41_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_41_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_41_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_41
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_41 of size 227072 dropped from memory (free 2063513516)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 41, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 41
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(686)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 686
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 686
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(647)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 647
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 647
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(695)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 695
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 695
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(740)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 740
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 740
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(774)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 774
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 774
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(791)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 791
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 791
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(704)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 704
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 704
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(710)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 710
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 710
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(637)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 637
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 637
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(690)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 690
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 690
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(639)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 639
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 639
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(679)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 679
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 679
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(706)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 706
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 706
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(694)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 694
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 694
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(638)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 638
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 638
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(624)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 624
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 624
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(789)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 789
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 789
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(45)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 45
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 45
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 45
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 45
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_45_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_45_piece0 of size 7568 dropped from memory (free 2063521084)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_45_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.4 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_45_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_45_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_45
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_45 of size 14608 dropped from memory (free 2063535692)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 45, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 45
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(769)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 769
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 769
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(749)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 749
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 749
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(723)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 723
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 723
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(620)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 620
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 620
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(753)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 753
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 753
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(775)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 775
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 775
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(788)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 788
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 788
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(733)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 733
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 733
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(634)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 634
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 634
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(794)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 794
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 794
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(625)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 625
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 625
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(644)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 644
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 644
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(641)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 641
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 641
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(770)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 770
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 770
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(754)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 754
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 754
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(793)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 793
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 793
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(807)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 807
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 807
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(757)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 757
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 757
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(725)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 725
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 725
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(808)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 808
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 808
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(707)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 707
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 707
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(718)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 718
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 718
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(799)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 799
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 799
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(778)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 778
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 778
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(699)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 699
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 699
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(790)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 790
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 790
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(728)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 728
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 728
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(643)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 643
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 643
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(764)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 764
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 764
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(786)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 786
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 786
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(744)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 744
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 744
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(717)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 717
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 717
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(763)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 763
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 763
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(783)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 783
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 783
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(44)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 44
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 44
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 44
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 44
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_44
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_44 of size 227072 dropped from memory (free 2063762764)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_44_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_44_piece0 of size 21151 dropped from memory (free 2063783915)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_44_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_44_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_44_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 44, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 44
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(801)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 801
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 801
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(632)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 632
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 632
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(759)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 759
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 759
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(741)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 741
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 741
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(43)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 43
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 43
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 43
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 43
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_43_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_43_piece0 of size 127284 dropped from memory (free 2063911199)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_43_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 124.3 KB, free: 1970.3 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_43_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_43_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_43
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_43 of size 1148080 dropped from memory (free 2065059279)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 43, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 43
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(622)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 622
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 622
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(701)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 701
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 701
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanShuffle(1)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning shuffle 1
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned shuffle 1
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing shuffle 1
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(798)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 798
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 798
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(727)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 727
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 727
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(726)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 726
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 726
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(787)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 787
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 787
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(732)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 732
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 732
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(785)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 785
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 785
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(739)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 739
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 739
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(627)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 627
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 627
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(621)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 621
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 621
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(735)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 735
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 735
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(782)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 782
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 782
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(731)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 731
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 731
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(730)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 730
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 730
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(692)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 692
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 692
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(49)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 49
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 49
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 49
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 49
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_49_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_49_piece0 of size 21165 dropped from memory (free 2065080444)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_49_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_49_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_49_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_49
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_49 of size 227232 dropped from memory (free 2065307676)
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 49, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 49
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(722)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 722
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 722
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(773)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 773
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 773
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(630)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 630
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 630
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(708)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 708
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 708
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(631)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 631
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 631
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(760)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 760
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 760
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(633)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 633
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 633
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(784)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 784
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 784
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(803)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 803
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 803
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(651)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 651
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 651
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(696)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 696
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 696
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(743)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 743
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 743
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(779)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 779
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 779
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(765)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 765
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 765
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(746)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 746
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 746
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(750)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 750
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 750
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(762)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 762
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 762
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing shuffle 1, response is true
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: true to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(806)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 806
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 806
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(646)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 646
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 646
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(648)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 648
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 648
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(650)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 650
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 650
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(652)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 652
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 652
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(736)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 736
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 736
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(635)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 635
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 635
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(712)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 712
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(751)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 751
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 751
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(697)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 697
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 697
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(642)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 642
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 642
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(687)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 687
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 687
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(629)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 629
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 629
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(645)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 645
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 645
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(715)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 715
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 715
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(713)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 713
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 713
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(766)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 766
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 766
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(767)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 767
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 767
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(46)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 46
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 46
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 46
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 46
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_46
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_46 of size 16768 dropped from memory (free 2065324444)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_46_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_46_piece0 of size 8629 dropped from memory (free 2065333073)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_46_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 8.4 KB, free: 1970.3 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_46_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_46_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 46, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 46
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(683)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 683
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 683
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(772)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 772
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 772
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(50)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning broadcast 50
2022-02-10 13:32:07 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 50
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 50
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing broadcast 50
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_50
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_50 of size 9144 dropped from memory (free 2065342217)
2022-02-10 13:32:07 DEBUG BlockManager:58 - Removing block broadcast_50_piece0
2022-02-10 13:32:07 DEBUG MemoryStore:58 - Block broadcast_50_piece0 of size 4744 dropped from memory (free 2065346961)
2022-02-10 13:32:07 INFO  BlockManagerInfo:54 - Removed broadcast_50_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:07 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_50_piece0
2022-02-10 13:32:07 DEBUG BlockManager:58 - Told master about block broadcast_50_piece0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 50, response is 0
2022-02-10 13:32:07 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaned broadcast 50
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(756)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 756
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 756
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(776)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 776
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 776
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(792)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 792
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 792
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(721)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 721
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 721
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(795)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 795
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 795
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(689)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 689
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 689
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(719)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 719
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 719
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(682)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 682
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 682
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(685)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 685
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 685
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(729)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 729
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 729
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(691)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 691
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 691
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(705)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 705
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 705
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(802)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 802
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 802
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(734)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 734
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 734
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(623)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 623
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 623
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(804)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 804
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 804
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(711)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 711
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 711
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(709)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 709
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 709
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(684)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 684
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 684
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(628)
2022-02-10 13:32:07 DEBUG ContextCleaner:58 - Cleaning accumulator 628
2022-02-10 13:32:07 INFO  ContextCleaner:54 - Cleaned accumulator 628
2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:08 INFO  Executor:54 - Finished task 0.0 in stage 26.0 (TID 26). 1544 bytes result sent to driver
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_26.0, runningTasks: 0
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 26.0 (TID 26) in 567 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2022-02-10 13:32:08 INFO  DAGScheduler:54 - ResultStage 26 (load at UseCase2.java:19) finished in 0.567 s
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - After removal of stage 26, remaining stages = 0
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Job 24 finished: load at UseCase2.java:19, took 0.567778 s
2022-02-10 13:32:08 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:08 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:08 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:08 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#410
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#410]
 +- Relation[value#410] text                +- Relation[value#410] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#414: java.lang.String   DeserializeToObject cast(value#410 as string).toString, obj#414: java.lang.String
 +- LocalRelation <empty>, [value#410]                                                                                                                                      +- LocalRelation <empty>, [value#410]
          
2022-02-10 13:32:08 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#410
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#410, None)) > 0)
 +- Project [value#410]                     +- Project [value#410]
    +- Relation[value#410] text                +- Relation[value#410] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#415: java.lang.String   DeserializeToObject cast(value#410 as string).toString, obj#415: java.lang.String
 +- LocalRelation <empty>, [value#410]                                                                                                                                      +- LocalRelation <empty>, [value#410]
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#416: java.lang.String   DeserializeToObject cast(value#410 as string).toString, obj#416: java.lang.String
 +- LocalRelation <empty>, [value#410]                                                                                                                                      +- LocalRelation <empty>, [value#410]
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#410, None)) > 0)      +- Filter (length(trim(value#410, None)) > 0)
!      +- Project [value#410]                             +- Relation[value#410] text
!         +- Relation[value#410] text               
          
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#410, None)) > 0)
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:08 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:08 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_53 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_53 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_53 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_53_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_53_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_53_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_53_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_53_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_53_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 53 from load at UseCase2.java:24
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:08 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Got job 25 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Final stage: ResultStage 27 (load at UseCase2.java:24)
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitStage(ResultStage 27 (name=load at UseCase2.java:24;jobs=25))
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting ResultStage 27 (MapPartitionsRDD[123] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 27)
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_54 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_54 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_54 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_54_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_54_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_54_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_54_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_54_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_54_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 54 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[123] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Adding task set 27.0 with 1 tasks
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Epoch for TaskSet 27.0: 2
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 27.0: NO_PREF, ANY
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_27.0, runningTasks: 0
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:08 INFO  Executor:54 - Running task 0.0 in stage 27.0 (TID 27)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_54
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_54 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:08 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_53
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_53 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  Executor:54 - Finished task 0.0 in stage 27.0 (TID 27). 1162 bytes result sent to driver
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_27.0, runningTasks: 0
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 27.0 (TID 27) in 0 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2022-02-10 13:32:08 INFO  DAGScheduler:54 - ResultStage 27 (load at UseCase2.java:24) finished in 0.000 s
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - After removal of stage 27, remaining stages = 0
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Job 25 finished: load at UseCase2.java:24, took 0.010780 s
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#418: java.lang.String   DeserializeToObject cast(value#410 as string).toString, obj#418: java.lang.String
 +- Project [value#410]                                                                                                                                                     +- Project [value#410]
    +- Relation[value#410] text                                                                                                                                                +- Relation[value#410] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#410 as string).toString, obj#418: java.lang.String   DeserializeToObject value#410.toString, obj#418: java.lang.String
!+- Project [value#410]                                                              +- Relation[value#410] text
!   +- Relation[value#410] text                                                      
          
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:08 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_55 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_55 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_55 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_55_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_55_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_55_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_55_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_55_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_55_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 55 from load at UseCase2.java:24
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:08 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Got job 26 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Final stage: ResultStage 28 (load at UseCase2.java:24)
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitStage(ResultStage 28 (name=load at UseCase2.java:24;jobs=26))
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting ResultStage 28 (MapPartitionsRDD[129] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 28)
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_56 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_56 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_56 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_56_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_56_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_56_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_56_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_56_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_56_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 56 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[129] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Adding task set 28.0 with 1 tasks
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Epoch for TaskSet 28.0: 2
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 28.0: NO_PREF, ANY
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_28.0, runningTasks: 0
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:08 INFO  Executor:54 - Running task 0.0 in stage 28.0 (TID 28)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_56
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_56 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:08 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_55
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_55 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:08 INFO  Executor:54 - Finished task 0.0 in stage 28.0 (TID 28). 1473 bytes result sent to driver
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_28.0, runningTasks: 0
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 28.0 (TID 28) in 47 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2022-02-10 13:32:08 INFO  DAGScheduler:54 - ResultStage 28 (load at UseCase2.java:24) finished in 0.047 s
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - After removal of stage 28, remaining stages = 0
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Job 26 finished: load at UseCase2.java:24, took 0.045925 s
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (order_date#403 LIKE 2014-01% && isnull(order_id#402))                                                                                                                                     Filter (cast(order_date#403 as string) LIKE 2014-01% && isnull(order_id#402))
 +- Join RightOuter, (order_customer_id#404 = customer_id#420)                                                                                                                                      +- Join RightOuter, (order_customer_id#404 = customer_id#420)
    :- Relation[order_id#402,order_date#403,order_customer_id#404,order_status#405] csv                                                                                                                :- Relation[order_id#402,order_date#403,order_customer_id#404,order_status#405] csv
    +- Relation[customer_id#420,customer_fname#421,customer_lname#422,customer_email#423,customer_password#424,customer_street#425,customer_city#426,customer_state#427,customer_zipcode#428] csv      +- Relation[customer_id#420,customer_fname#421,customer_lname#422,customer_email#423,customer_password#424,customer_street#425,customer_city#426,customer_state#427,customer_zipcode#428] csv
          
2022-02-10 13:32:08 INFO  UseCase2:73 - ------------------------------------------Write Result--------------------------------------------------
2022-02-10 13:32:08 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:08 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:08 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:08 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#486
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#486]
 +- Relation[value#486] text                +- Relation[value#486] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#490: java.lang.String   DeserializeToObject cast(value#486 as string).toString, obj#490: java.lang.String
 +- LocalRelation <empty>, [value#486]                                                                                                                                      +- LocalRelation <empty>, [value#486]
          
2022-02-10 13:32:08 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#486
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#486, None)) > 0)
 +- Project [value#486]                     +- Project [value#486]
    +- Relation[value#486] text                +- Relation[value#486] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#491: java.lang.String   DeserializeToObject cast(value#486 as string).toString, obj#491: java.lang.String
 +- LocalRelation <empty>, [value#486]                                                                                                                                      +- LocalRelation <empty>, [value#486]
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#492: java.lang.String   DeserializeToObject cast(value#486 as string).toString, obj#492: java.lang.String
 +- LocalRelation <empty>, [value#486]                                                                                                                                      +- LocalRelation <empty>, [value#486]
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#486, None)) > 0)      +- Filter (length(trim(value#486, None)) > 0)
!      +- Project [value#486]                             +- Relation[value#486] text
!         +- Relation[value#486] text               
          
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#486, None)) > 0)
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:08 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:08 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_57 stored as values in memory (estimated size 221.9 KB, free 1968.9 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_57 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_57 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_57_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.9 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_57_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_57_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_57_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_57_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_57_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 57 from load at UseCase2.java:19
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:08 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Got job 27 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Final stage: ResultStage 29 (load at UseCase2.java:19)
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitStage(ResultStage 29 (name=load at UseCase2.java:19;jobs=27))
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting ResultStage 29 (MapPartitionsRDD[133] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 29)
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_58 stored as values in memory (estimated size 8.9 KB, free 1968.9 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_58 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_58 without replication took  0 ms
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_58_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1968.9 MB)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_58_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_58_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_58_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_58_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_58_piece0 without replication took  0 ms
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 58 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[133] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Adding task set 29.0 with 1 tasks
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Epoch for TaskSet 29.0: 2
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 29.0: NO_PREF, ANY
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_29.0, runningTasks: 0
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:08 INFO  Executor:54 - Running task 0.0 in stage 29.0 (TID 29)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_58
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_58 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:08 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_57
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_57 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  Executor:54 - Finished task 0.0 in stage 29.0 (TID 29). 1214 bytes result sent to driver
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_29.0, runningTasks: 0
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 29.0 (TID 29) in 16 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2022-02-10 13:32:08 INFO  DAGScheduler:54 - ResultStage 29 (load at UseCase2.java:19) finished in 0.016 s
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - After removal of stage 29, remaining stages = 0
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Job 27 finished: load at UseCase2.java:19, took 0.011910 s
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#494: java.lang.String   DeserializeToObject cast(value#486 as string).toString, obj#494: java.lang.String
 +- Project [value#486]                                                                                                                                                     +- Project [value#486]
    +- Relation[value#486] text                                                                                                                                                +- Relation[value#486] text
          
2022-02-10 13:32:08 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#486 as string).toString, obj#494: java.lang.String   DeserializeToObject value#486.toString, obj#494: java.lang.String
!+- Project [value#486]                                                              +- Relation[value#486] text
!   +- Relation[value#486] text                                                      
          
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:08 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:08 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_59 stored as values in memory (estimated size 221.9 KB, free 1968.7 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_59 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_59 without replication took  0 ms
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(849)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 849
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 849
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(904)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 904
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 904
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(858)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 858
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 858
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(819)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 819
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 819
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(872)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 872
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 872
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(815)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 815
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 815
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(910)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 910
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 910
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(812)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 812
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 812
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(836)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 836
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 836
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(857)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 857
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 857
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(896)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 896
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 896
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(921)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 921
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 921
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(829)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 829
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 829
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(846)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 846
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 846
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(900)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 900
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 900
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(847)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 847
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 847
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(873)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 873
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 873
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(52)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 52
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 52
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 52
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 52
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_52
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_52 of size 14256 dropped from memory (free 2064338874)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_52_piece0
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_59_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.7 MB)
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_52_piece0 of size 7742 dropped from memory (free 2064325451)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_59_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.2 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_59_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_59_piece0
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_52_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.2 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_59_piece0 locally took  0 ms
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_52_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_59_piece0 without replication took  0 ms
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_52_piece0
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 59 from load at UseCase2.java:19
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 52, response is 0
2022-02-10 13:32:08 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 14388598 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 52
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(893)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 893
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 893
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(823)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 823
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 823
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(840)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 840
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 840
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(844)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 844
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 844
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(852)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 852
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 852
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(839)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 839
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 839
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(922)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 922
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 922
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(908)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 908
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 908
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(882)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 882
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 882
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(55)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 55
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 55
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 55
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 55
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_55_piece0
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_55_piece0 of size 21165 dropped from memory (free 2064346616)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_55_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_55_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_55_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_55
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_55 of size 227232 dropped from memory (free 2064573848)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 55, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 55
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(821)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 821
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 821
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(848)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 848
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 848
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(822)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 822
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 822
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(915)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 915
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 915
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(845)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 845
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 845
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(905)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 905
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 905
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(918)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 918
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 918
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(899)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 899
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 899
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(831)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 831
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 831
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(926)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 926
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 926
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(914)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 914
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 914
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(830)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 830
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 830
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(865)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 865
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 865
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(916)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 916
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 916
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(878)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 878
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 878
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(832)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 832
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 832
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(51)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 51
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 51
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 51
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 51
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_51_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_51_piece0 of size 21165 dropped from memory (free 2064595013)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_51_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_51_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_51_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_51
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_51 of size 227232 dropped from memory (free 2064822245)
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 51, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 51
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(818)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 818
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 818
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(888)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 888
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 888
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(814)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 814
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 814
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(892)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 892
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 892
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(923)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 923
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 923
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(843)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 843
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 843
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(930)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 930
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 930
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(889)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 889
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 889
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(903)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 903
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 903
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(53)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 53
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 53
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 53
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 53
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_53
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_53 of size 227232 dropped from memory (free 2065049477)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_53_piece0
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_53_piece0 of size 21165 dropped from memory (free 2065070642)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_53_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_53_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_53_piece0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 53, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 53
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(898)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 898
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 898
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(841)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 841
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 841
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(929)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 929
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 929
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(869)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 869
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 869
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(895)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 895
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 895
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(917)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 917
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 917
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(891)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 891
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 891
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(919)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 919
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 919
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(833)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 833
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 833
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(838)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 838
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 838
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(884)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 884
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 884
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(928)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 928
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 928
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(827)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 827
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 827
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(883)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 883
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 883
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(861)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 861
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 861
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(875)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 875
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 875
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(813)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 813
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 813
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(850)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 850
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 850
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(897)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 897
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 897
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(859)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 859
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 859
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(54)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 54
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 54
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 54
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 54
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_54
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_54 of size 9144 dropped from memory (free 2065079786)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_54_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_54_piece0 of size 4744 dropped from memory (free 2065084530)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_54_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_54_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_54_piece0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 54, response is 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 54
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(817)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 817
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 817
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(863)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 863
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 863
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(920)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 920
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 920
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(870)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 870
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 870
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(902)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 902
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 902
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(851)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 851
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 851
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(58)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 58
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 58
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 58
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 58
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_58_piece0
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_58_piece0 of size 4744 dropped from memory (free 2065089274)
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:08 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_58_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_58_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_58_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_58
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_58 of size 9144 dropped from memory (free 2065098418)
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 58, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 58
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(894)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 894
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 894
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(927)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 927
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 927
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(868)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 868
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 868
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(912)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 912
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 912
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(886)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 886
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 886
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(925)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 925
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 925
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(887)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 887
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 887
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(816)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 816
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 816
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(860)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 860
2022-02-10 13:32:08 INFO  SparkContext:54 - Starting job: load at UseCase2.java:19
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 860
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(853)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 853
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 853
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(825)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 825
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 825
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(909)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 909
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 909
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(57)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 57
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 57
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 57
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 57
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Got job 28 (load at UseCase2.java:19) with 1 output partitions
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Final stage: ResultStage 30 (load at UseCase2.java:19)
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_57
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_57 of size 227232 dropped from memory (free 2065325650)
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitStage(ResultStage 30 (name=load at UseCase2.java:19;jobs=28))
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_57_piece0
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_57_piece0 of size 21165 dropped from memory (free 2065346815)
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting ResultStage 30 (MapPartitionsRDD[139] at load at UseCase2.java:19), which has no missing parents
2022-02-10 13:32:08 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 30)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_57_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_57_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_57_piece0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 57, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 57
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(911)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 911
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 911
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(874)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 874
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 874
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(834)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 834
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 834
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(837)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 837
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 837
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(890)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 890
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 890
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(824)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 824
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 824
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(879)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 879
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 879
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(835)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 835
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 835
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(880)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 880
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 880
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(810)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 810
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 810
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(876)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 876
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 876
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(826)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 826
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_60 stored as values in memory (estimated size 13.9 KB, free 1969.7 MB)
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 826
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(854)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 854
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_60 locally took  0 ms
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 854
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_60 without replication took  0 ms
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(864)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 864
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 864
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(906)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 906
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 906
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(856)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 856
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 856
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(881)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 881
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 881
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(56)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning broadcast 56
2022-02-10 13:32:08 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 56
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 56
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing broadcast 56
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_56_piece0
2022-02-10 13:32:08 INFO  MemoryStore:54 - Block broadcast_60_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.6 MB)
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_56_piece0 of size 7776 dropped from memory (free 2065332594)
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Added broadcast_60_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_60_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_60_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Put block broadcast_60_piece0 locally took  0 ms
2022-02-10 13:32:08 INFO  BlockManagerInfo:54 - Removed broadcast_56_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Putting block broadcast_60_piece0 without replication took  0 ms
2022-02-10 13:32:08 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_56_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Told master about block broadcast_56_piece0
2022-02-10 13:32:08 DEBUG BlockManager:58 - Removing block broadcast_56
2022-02-10 13:32:08 INFO  SparkContext:54 - Created broadcast 60 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:08 DEBUG MemoryStore:58 - Block broadcast_56 of size 14368 dropped from memory (free 2065346962)
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 56, response is 0
2022-02-10 13:32:08 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[139] at load at UseCase2.java:19) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaned broadcast 56
2022-02-10 13:32:08 INFO  TaskSchedulerImpl:54 - Adding task set 30.0 with 1 tasks
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(866)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 866
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 866
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(924)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 924
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 924
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(907)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 907
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 907
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(855)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 855
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 855
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(809)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 809
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 809
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(828)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 828
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 828
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(877)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 877
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 877
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(820)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 820
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 820
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(811)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 811
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 811
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(885)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 885
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 885
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(901)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 901
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 901
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(913)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 913
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 913
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(867)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 867
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 867
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(871)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 871
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 871
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(842)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 842
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 842
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(862)
2022-02-10 13:32:08 DEBUG ContextCleaner:58 - Cleaning accumulator 862
2022-02-10 13:32:08 INFO  ContextCleaner:54 - Cleaned accumulator 862
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Epoch for TaskSet 30.0: 2
2022-02-10 13:32:08 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 30.0: NO_PREF, ANY
2022-02-10 13:32:08 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_30.0, runningTasks: 0
2022-02-10 13:32:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
2022-02-10 13:32:08 INFO  Executor:54 - Running task 0.0 in stage 30.0 (TID 30)
2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_60
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_60 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:08 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:08 DEBUG BlockManager:58 - Getting local block broadcast_59
2022-02-10 13:32:08 DEBUG BlockManager:58 - Level for block broadcast_59 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:08 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 30.0 (TID 30). 1501 bytes result sent to driver
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_30.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 30.0 (TID 30) in 550 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ResultStage 30 (load at UseCase2.java:19) finished in 0.550 s
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - After removal of stage 30, remaining stages = 0
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Job 28 finished: load at UseCase2.java:19, took 0.553952 s
2022-02-10 13:32:09 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some spark core configurations may not take effect.
2022-02-10 13:32:09 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 1 paths.
2022-02-10 13:32:09 INFO  InMemoryFileIndex:54 - It took 0 ms to list leaf files for 2 paths.
2022-02-10 13:32:09 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#504
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Project [unresolvedalias('value, None)]   Project [value#504]
 +- Relation[value#504] text                +- Relation[value#504] text
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#508: java.lang.String   DeserializeToObject cast(value#504 as string).toString, obj#508: java.lang.String
 +- LocalRelation <empty>, [value#504]                                                                                                                                      +- LocalRelation <empty>, [value#504]
          
2022-02-10 13:32:09 DEBUG Analyzer$ResolveReferences:58 - Resolving 'value to value#504
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (length(trim('value, None)) > 0)   Filter (length(trim(value#504, None)) > 0)
 +- Project [value#504]                     +- Project [value#504]
    +- Relation[value#504] text                +- Relation[value#504] text
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#509: java.lang.String   DeserializeToObject cast(value#504 as string).toString, obj#509: java.lang.String
 +- LocalRelation <empty>, [value#504]                                                                                                                                      +- LocalRelation <empty>, [value#504]
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#510: java.lang.String   DeserializeToObject cast(value#504 as string).toString, obj#510: java.lang.String
 +- LocalRelation <empty>, [value#504]                                                                                                                                      +- LocalRelation <empty>, [value#504]
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 GlobalLimit 1                                      GlobalLimit 1
 +- LocalLimit 1                                    +- LocalLimit 1
    +- Filter (length(trim(value#504, None)) > 0)      +- Filter (length(trim(value#504, None)) > 0)
!      +- Project [value#504]                             +- Relation[value#504] text
!         +- Relation[value#504] text               
          
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#504, None)) > 0)
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:09 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 026 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 028 */       do {
/* 029 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 030 */         UTF8String scan_value_0 = scan_isNull_0 ?
/* 031 */         null : (scan_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (scan_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = scan_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (scan_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, scan_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_61 stored as values in memory (estimated size 221.9 KB, free 1969.5 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_61 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_61 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_61_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_61_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_61_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_61_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_61_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_61_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 61 from load at UseCase2.java:24
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.Tuple2)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$6.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final byte[] org.apache.spark.sql.execution.SparkPlan$$anonfun$6.apply(scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$6) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:09 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Got job 29 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Final stage: ResultStage 31 (load at UseCase2.java:24)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 31 (name=load at UseCase2.java:24;jobs=29))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ResultStage 31 (MapPartitionsRDD[143] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 31)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_62 stored as values in memory (estimated size 8.9 KB, free 1969.4 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_62 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_62 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_62_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1969.4 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_62_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_62_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_62_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_62_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_62_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 62 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[143] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 31.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 31.0: 2
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 31.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_31.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 31.0 (TID 31)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_62
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_62 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_61
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_61 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 31.0 (TID 31). 1248 bytes result sent to driver
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_31.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 31.0 (TID 31) in 15 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ResultStage 31 (load at UseCase2.java:24) finished in 0.015 s
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - After removal of stage 31, remaining stages = 0
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Job 29 finished: load at UseCase2.java:24, took 0.012441 s
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, StringType), StringType, - root class: "java.lang.String").toString), obj#512: java.lang.String   DeserializeToObject cast(value#504 as string).toString, obj#512: java.lang.String
 +- Project [value#504]                                                                                                                                                     +- Project [value#504]
    +- Relation[value#504] text                                                                                                                                                +- Relation[value#504] text
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
!DeserializeToObject cast(value#504 as string).toString, obj#512: java.lang.String   DeserializeToObject value#504.toString, obj#512: java.lang.String
!+- Project [value#504]                                                              +- Relation[value#504] text
!   +- Relation[value#504] text                                                      
          
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Post-Scan Filters: 
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Pushed Filters: 
2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 010 */
/* 011 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 012 */     this.references = references;
/* 013 */   }
/* 014 */
/* 015 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 016 */     partitionIndex = index;
/* 017 */     this.inputs = inputs;
/* 018 */     scan_mutableStateArray_0[0] = inputs[0];
/* 019 */
/* 020 */   }
/* 021 */
/* 022 */   protected void processNext() throws java.io.IOException {
/* 023 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 024 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 025 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 026 */       append(scan_row_0);
/* 027 */       if (shouldStop()) return;
/* 028 */     }
/* 029 */   }
/* 030 */
/* 031 */ }

2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_63 stored as values in memory (estimated size 221.9 KB, free 1969.2 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_63 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_63 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_63_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.2 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_63_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_63_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_63_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_63_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_63_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 63 from load at UseCase2.java:24
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 10296320 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.Dataset$$anonfun$rdd$1.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType org.apache.spark.sql.Dataset$$anonfun$rdd$1.objectType$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.Dataset$$anonfun$rdd$1.apply(scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.Dataset$$anonfun$rdd$1$$anonfun$apply$16
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.Dataset$$anonfun$rdd$1) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 3
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.Option org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.maybeFirstLine$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.parsedOptions$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9.apply(scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9$$anonfun$apply$3
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$$anonfun$9) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.csv.CSVOptions org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.options$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2.apply(org.apache.spark.sql.types.DataType[],java.lang.String[])
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$2) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.types.DataType[] org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3.apply(org.apache.spark.sql.types.DataType[],org.apache.spark.sql.types.DataType[])
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$3) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$36) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$36.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$36.processPartition$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$36.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$36) is now cleaned +++
2022-02-10 13:32:09 INFO  SparkContext:54 - Starting job: load at UseCase2.java:24
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Got job 30 (load at UseCase2.java:24) with 1 output partitions
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Final stage: ResultStage 32 (load at UseCase2.java:24)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 32 (name=load at UseCase2.java:24;jobs=30))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ResultStage 32 (MapPartitionsRDD[149] at load at UseCase2.java:24), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 32)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_64 stored as values in memory (estimated size 14.0 KB, free 1969.2 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_64 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_64 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_64_piece0 stored as bytes in memory (estimated size 7.6 KB, free 1969.2 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_64_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_64_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_64_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_64_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_64_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 64 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[149] at load at UseCase2.java:24) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 32.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 32.0: 2
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 32.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_32.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 8353 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 32.0 (TID 32)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_64
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_64 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 DEBUG GenerateSafeProjection:58 - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_63
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_63 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(59)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning broadcast 59
2022-02-10 13:32:09 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 59
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 59
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing broadcast 59
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_59_piece0
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_59_piece0 of size 21165 dropped from memory (free 2064835293)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Removed broadcast_59_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_59_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_59_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_59
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_59 of size 227232 dropped from memory (free 2065062525)
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 59, response is 0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaned broadcast 59
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(942)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 942
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 942
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(975)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 975
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 975
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(976)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 976
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 976
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(961)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 961
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 961
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(962)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 962
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 962
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(987)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 987
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 987
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(965)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 965
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 965
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(989)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 989
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 989
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(935)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 935
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 935
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(941)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 941
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 941
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(932)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 932
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 932
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(940)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 940
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 940
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(988)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 988
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 988
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(954)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 954
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 954
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(983)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 983
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 983
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(986)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 986
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 986
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(951)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 951
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 951
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(960)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 960
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 960
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(946)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 946
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 946
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(985)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 985
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 985
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(955)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 955
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 955
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(990)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 990
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 990
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(969)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 969
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 969
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(973)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 973
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 973
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(979)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 979
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 979
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(971)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 971
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 971
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(948)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 948
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 948
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(970)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 970
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 970
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(934)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 934
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 934
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(956)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 956
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 956
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(974)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 974
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 974
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(936)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 936
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 936
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(943)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 943
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 943
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(959)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 959
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 959
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(968)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 968
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 968
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(966)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 966
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 966
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(938)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 938
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 938
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(984)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 984
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 984
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(945)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 945
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 945
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(933)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 933
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 933
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(981)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 981
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 981
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(958)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 958
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 958
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(61)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning broadcast 61
2022-02-10 13:32:09 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 61
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 61
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing broadcast 61
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_61
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_61 of size 227232 dropped from memory (free 2065289757)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_61_piece0
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_61_piece0 of size 21165 dropped from memory (free 2065310922)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Removed broadcast_61_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_61_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_61_piece0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 61, response is 0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaned broadcast 61
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(964)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 964
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 964
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(972)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 972
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 972
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(949)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 949
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 949
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(62)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning broadcast 62
2022-02-10 13:32:09 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 62
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 62
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing broadcast 62
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_62_piece0
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_62_piece0 of size 4744 dropped from memory (free 2065315666)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Removed broadcast_62_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 4.6 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_62_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_62_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_62
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_62 of size 9144 dropped from memory (free 2065324810)
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 62, response is 0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaned broadcast 62
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(953)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 953
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 953
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(963)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 963
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 963
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(967)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 967
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 967
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(937)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 937
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 937
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(980)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 980
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 980
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(957)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 957
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 957
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(978)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 978
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 978
2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 32.0 (TID 32). 1516 bytes result sent to driver
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(982)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 982
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 982
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_32.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(977)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 977
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 977
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(947)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 947
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 947
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 32.0 (TID 32) in 63 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanBroadcast(60)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning broadcast 60
2022-02-10 13:32:09 DEBUG TorrentBroadcast:58 - Unpersisting TorrentBroadcast 60
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ResultStage 32 (load at UseCase2.java:24) finished in 0.063 s
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - removing broadcast 60
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing broadcast 60
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - After removal of stage 32, remaining stages = 0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_60
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Job 30 finished: load at UseCase2.java:24, took 0.075276 s
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_60 of size 14256 dropped from memory (free 2065339066)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Removing block broadcast_60_piece0
2022-02-10 13:32:09 DEBUG MemoryStore:58 - Block broadcast_60_piece0 of size 7741 dropped from memory (free 2065346807)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Removed broadcast_60_piece0 on Clairvoyant-324.mshome.net:53727 in memory (size: 7.6 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_60_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_60_piece0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Done removing broadcast 60, response is 0
2022-02-10 13:32:09 DEBUG BlockManagerSlaveEndpoint:58 - Sent response: 0 to Clairvoyant-324.mshome.net:53712
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaned broadcast 60
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(950)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 950
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 950
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(991)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 991
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 991
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(944)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 944
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 944
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(952)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 952
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 952
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(931)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 931
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 931
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Got cleaning task CleanAccum(939)
2022-02-10 13:32:09 DEBUG ContextCleaner:58 - Cleaning accumulator 939
2022-02-10 13:32:09 INFO  ContextCleaner:54 - Cleaned accumulator 939
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$1:58 - 
=== Result of Batch Resolution ===
!'Filter (order_date#497 LIKE 2014-01% && isnull(order_id#496))                                                                                                                                     Filter (cast(order_date#497 as string) LIKE 2014-01% && isnull(order_id#496))
 +- Join RightOuter, (order_customer_id#498 = customer_id#514)                                                                                                                                      +- Join RightOuter, (order_customer_id#498 = customer_id#514)
    :- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv                                                                                                                :- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv
    +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv      +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization before Inferring Filters ===
 InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]   InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]
 +- Repartition 1, false                                                                                                                                                                                                                                                                                                                                                                                                +- Repartition 1, false
    +- Sort [customer_id#514 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                        +- Sort [customer_id#514 ASC NULLS FIRST], true
       +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]                                                                                                                                                                                                                      +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]
!         +- Filter (cast(order_date#497 as string) LIKE 2014-01% && isnull(order_id#496))                                                                                                                                                                                                                                                                                                                                       +- Join Inner, (order_customer_id#498 = customer_id#514)
!            +- Join RightOuter, (order_customer_id#498 = customer_id#514)                                                                                                                                                                                                                                                                                                                                                          :- Project [order_customer_id#498]
!               :- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv                                                                                                                                                                                                                                                                                                                                 :  +- Filter (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496))
!               +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv                                                                                                                                                                                                                       :     +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv
!                                                                                                                                                                                                                                                                                                                                                                                                                                   +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Infer Filters ===
 InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]   InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]
 +- Repartition 1, false                                                                                                                                                                                                                                                                                                                                                                                                +- Repartition 1, false
    +- Sort [customer_id#514 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                        +- Sort [customer_id#514 ASC NULLS FIRST], true
       +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]                                                                                                                                                                                                                      +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]
          +- Join Inner, (order_customer_id#498 = customer_id#514)                                                                                                                                                                                                                                                                                                                                                               +- Join Inner, (order_customer_id#498 = customer_id#514)
!            :- Project [order_customer_id#498]                                                                                                                                                                                                                                                                                                                                                                                     :- Filter isnotnull(order_customer_id#498)
!            :  +- Filter (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496))                                                                                                                                                                                                                                                                                                                             :  +- Project [order_customer_id#498]
!            :     +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv                                                                                                                                                                                                                                                                                                                              :     +- Filter (isnotnull(order_date#497) && (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496)))
!            +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv                                                                                                                                                                                                                          :        +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv
!                                                                                                                                                                                                                                                                                                                                                                                                                                   +- Filter isnotnull(customer_id#514)
!                                                                                                                                                                                                                                                                                                                                                                                                                                      +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch Operator Optimization after Inferring Filters ===
 InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]   InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]
 +- Repartition 1, false                                                                                                                                                                                                                                                                                                                                                                                                +- Repartition 1, false
    +- Sort [customer_id#514 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                        +- Sort [customer_id#514 ASC NULLS FIRST], true
       +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]                                                                                                                                                                                                                      +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]
          +- Join Inner, (order_customer_id#498 = customer_id#514)                                                                                                                                                                                                                                                                                                                                                               +- Join Inner, (order_customer_id#498 = customer_id#514)
!            :- Filter isnotnull(order_customer_id#498)                                                                                                                                                                                                                                                                                                                                                                             :- Project [order_customer_id#498]
!            :  +- Project [order_customer_id#498]                                                                                                                                                                                                                                                                                                                                                                                  :  +- Filter ((isnotnull(order_date#497) && (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496))) && isnotnull(order_customer_id#498))
!            :     +- Filter (isnotnull(order_date#497) && (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496)))                                                                                                                                                                                                                                                                                           :     +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv
!            :        +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv                                                                                                                                                                                                                                                                                                                           +- Filter isnotnull(customer_id#514)
!            +- Filter isnotnull(customer_id#514)                                                                                                                                                                                                                                                                                                                                                                                      +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv
!               +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv                                                                                                                                                                                                           
          
2022-02-10 13:32:09 DEBUG BaseSessionStateBuilder$$anon$2:58 - 
=== Result of Batch RewriteSubquery ===
 InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]   InsertIntoHadoopFsRelationCommand file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2, false, CSV, Map(header -> true, path -> C:\Users\Anukul Thalkar\IdeaProjects\UseCases\src\main\resources\outputs\UseCase2), Overwrite, [customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode]
 +- Repartition 1, false                                                                                                                                                                                                                                                                                                                                                                                                +- Repartition 1, false
    +- Sort [customer_id#514 ASC NULLS FIRST], true                                                                                                                                                                                                                                                                                                                                                                        +- Sort [customer_id#514 ASC NULLS FIRST], true
       +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]                                                                                                                                                                                                                      +- Project [customer_id#514, customer_fname#515, customer_lname#516, customer_email#517, customer_password#518, customer_street#519, customer_city#520, customer_state#521, customer_zipcode#522]
          +- Join Inner, (order_customer_id#498 = customer_id#514)                                                                                                                                                                                                                                                                                                                                                               +- Join Inner, (order_customer_id#498 = customer_id#514)
             :- Project [order_customer_id#498]                                                                                                                                                                                                                                                                                                                                                                                     :- Project [order_customer_id#498]
!            :  +- Filter ((isnotnull(order_date#497) && (StartsWith(cast(order_date#497 as string), 2014-01) && isnull(order_id#496))) && isnotnull(order_customer_id#498))                                                                                                                                                                                                                                                        :  +- Filter (((isnotnull(order_date#497) && StartsWith(cast(order_date#497 as string), 2014-01)) && isnull(order_id#496)) && isnotnull(order_customer_id#498))
             :     +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv                                                                                                                                                                                                                                                                                                                              :     +- Relation[order_id#496,order_date#497,order_customer_id#498,order_status#499] csv
             +- Filter isnotnull(customer_id#514)                                                                                                                                                                                                                                                                                                                                                                                   +- Filter isnotnull(customer_id#514)
                +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv                                                                                                                                                                                                                          +- Relation[customer_id#514,customer_fname#515,customer_lname#516,customer_email#517,customer_password#518,customer_street#519,customer_city#520,customer_state#521,customer_zipcode#522] csv
          
2022-02-10 13:32:09 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#498 = customer_id#514))
2022-02-10 13:32:09 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#498) | rightKeys:List(customer_id#514)
2022-02-10 13:32:09 DEBUG ExtractEquiJoinKeys:58 - Considering join on: Some((order_customer_id#498 = customer_id#514))
2022-02-10 13:32:09 DEBUG ExtractEquiJoinKeys:58 - leftKeys:List(order_customer_id#498) | rightKeys:List(customer_id#514)
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(order_date#497),StartsWith(cast(order_date#497 as string), 2014-01),isnull(order_id#496),isnotnull(order_customer_id#498)
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Output Data Schema: struct<order_id: int, order_date: timestamp, order_customer_id: int ... 1 more fields>
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(order_date),IsNull(order_id),IsNotNull(order_customer_id)
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Pruning directories with: 
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Post-Scan Filters: isnotnull(customer_id#514)
2022-02-10 13:32:09 INFO  FileSourceStrategy:54 - Output Data Schema: struct<customer_id: int, customer_fname: string, customer_lname: string, customer_email: string, customer_password: string ... 7 more fields>
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Pushed Filters: IsNotNull(customer_id)
2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 010 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     scan_mutableStateArray_0[0] = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 027 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 028 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 029 */       do {
/* 030 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 031 */         long scan_value_1 = scan_isNull_1 ?
/* 032 */         -1L : (scan_row_0.getLong(1));
/* 033 */
/* 034 */         if (!(!scan_isNull_1)) continue;
/* 035 */
/* 036 */         boolean filter_isNull_3 = scan_isNull_1;
/* 037 */         UTF8String filter_value_3 = null;
/* 038 */         if (!scan_isNull_1) {
/* 039 */           filter_value_3 = UTF8String.fromString(
/* 040 */             org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString(scan_value_1, ((sun.util.calendar.ZoneInfo) references[2] /* timeZone */)));
/* 041 */         }
/* 042 */
/* 043 */         boolean filter_value_2 = false;
/* 044 */         filter_value_2 = (filter_value_3).startsWith(((UTF8String) references[3] /* literal */));
/* 045 */         if (!filter_value_2) continue;
/* 046 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 047 */         int scan_value_0 = scan_isNull_0 ?
/* 048 */         -1 : (scan_row_0.getInt(0));
/* 049 */
/* 050 */         if (!scan_isNull_0) continue;
/* 051 */
/* 052 */         boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 053 */         int scan_value_2 = scan_isNull_2 ?
/* 054 */         -1 : (scan_row_0.getInt(2));
/* 055 */
/* 056 */         if (!(!scan_isNull_2)) continue;
/* 057 */
/* 058 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 059 */
/* 060 */         filter_mutableStateArray_0[1].reset();
/* 061 */
/* 062 */         if (false) {
/* 063 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 064 */         } else {
/* 065 */           filter_mutableStateArray_0[1].write(0, scan_value_2);
/* 066 */         }
/* 067 */         append((filter_mutableStateArray_0[1].getRow()));
/* 068 */
/* 069 */       } while(false);
/* 070 */       if (shouldStop()) return;
/* 071 */     }
/* 072 */   }
/* 073 */
/* 074 */ }

2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_65 stored as values in memory (estimated size 221.8 KB, free 1969.5 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_65 locally took  0 ms
2022-02-10 13:32:09 DEBUG FileCommitProtocol:58 - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job a3376de5-12ae-4425-9278-1aa83a6dbee3; output=file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2; dynamic=false
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_65 without replication took  0 ms
2022-02-10 13:32:09 DEBUG FileCommitProtocol:58 - Using (String, String, Boolean) constructor
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_65_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1969.4 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_65_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_65_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_65_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_65_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_65_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 65 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 7194299 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:09 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Got job 31 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Final stage: ResultStage 33 (run at ThreadPoolExecutor.java:1149)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 33 (name=run at ThreadPoolExecutor.java:1149;jobs=31))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ResultStage 33 (MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 33)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_66 stored as values in memory (estimated size 11.9 KB, free 1969.4 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_66 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_66 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_66_piece0 stored as bytes in memory (estimated size 6.4 KB, free 1969.4 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_66_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 6.4 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_66_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_66_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_66_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_66_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 66 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[152] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 33.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 33.0: 2
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 33.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_33.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 8318 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 33.0 (TID 33)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_66
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_66 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/orders/part-00000, range: 0-2999995, partition values: [empty row]
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, timestamp, true],input[2, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     boolean isNull_1 = i.isNullAt(1);
/* 041 */     long value_1 = isNull_1 ?
/* 042 */     -1L : (i.getLong(1));
/* 043 */     if (isNull_1) {
/* 044 */       mutableStateArray_0[0].setNullAt(1);
/* 045 */     } else {
/* 046 */       mutableStateArray_0[0].write(1, value_1);
/* 047 */     }
/* 048 */
/* 049 */     boolean isNull_2 = i.isNullAt(2);
/* 050 */     int value_2 = isNull_2 ?
/* 051 */     -1 : (i.getInt(2));
/* 052 */     if (isNull_2) {
/* 053 */       mutableStateArray_0[0].setNullAt(2);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(2, value_2);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_65
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_65 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean sort_needToSort_0;
/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     sort_needToSort_0 = true;
/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 031 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 034 */       if (shouldStop()) return;
/* 035 */     }
/* 036 */
/* 037 */   }
/* 038 */
/* 039 */   protected void processNext() throws java.io.IOException {
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       append(sort_outputRow_0);
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */   }
/* 059 */
/* 060 */ }

2022-02-10 13:32:09 DEBUG CodeGenerator:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean sort_needToSort_0;
/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     sort_needToSort_0 = true;
/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 031 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 034 */       if (shouldStop()) return;
/* 035 */     }
/* 036 */
/* 037 */   }
/* 038 */
/* 039 */   protected void processNext() throws java.io.IOException {
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       append(sort_outputRow_0);
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */   }
/* 059 */
/* 060 */ }

2022-02-10 13:32:09 INFO  CodeGenerator:54 - Code generated in 6.5211 ms
2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 33.0 (TID 33). 1423 bytes result sent to driver
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_33.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 33.0 (TID 33) in 378 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ResultStage 33 (run at ThreadPoolExecutor.java:1149) finished in 0.378 s
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - After removal of stage 33, remaining stages = 0
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Job 31 finished: run at ThreadPoolExecutor.java:1149, took 0.373733 s
2022-02-10 13:32:09 DEBUG TaskMemoryManager:224 - Task 0 acquired 1024.0 KB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@36f2e9f0
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for cast(input[0, int, true] as bigint):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     boolean isNull_0 = isNull_1;
/* 035 */     long value_0 = -1L;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:09 DEBUG TaskMemoryManager:224 - Task 0 acquired 16.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@36f2e9f0
2022-02-10 13:32:09 DEBUG TaskMemoryManager:233 - Task 0 release 16.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@36f2e9f0
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_67 stored as values in memory (estimated size 1024.0 KB, free 1968.4 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_67 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_67 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_67_piece0 stored as bytes in memory (estimated size 181.0 B, free 1968.4 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_67_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 181.0 B, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_67_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_67_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_67_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_67_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 67 from run at ThreadPoolExecutor.java:1149
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_67
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_67 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];
/* 011 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     scan_mutableStateArray_0[0] = inputs[0];
/* 021 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 022 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 023 */
/* 024 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[2] /* broadcast */).value()).asReadOnlyCopy();
/* 025 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 026 */
/* 027 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(10, 224);
/* 028 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 029 */
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_mutableStateArray_0[0].hasNext()) {
/* 034 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();
/* 035 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 036 */       do {
/* 037 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 038 */         int scan_value_0 = scan_isNull_0 ?
/* 039 */         -1 : (scan_row_0.getInt(0));
/* 040 */
/* 041 */         if (!(!scan_isNull_0)) continue;
/* 042 */
/* 043 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numOutputRows */).add(1);
/* 044 */
/* 045 */         // generate join key for stream side
/* 046 */         boolean bhj_isNull_0 = false;
/* 047 */         long bhj_value_0 = -1L;
/* 048 */         if (!false) {
/* 049 */           bhj_value_0 = (long) scan_value_0;
/* 050 */         }
/* 051 */         // find matches from HashedRelation
/* 052 */         UnsafeRow bhj_matched_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 053 */         if (bhj_matched_0 != null) {
/* 054 */           {
/* 055 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* numOutputRows */).add(1);
/* 056 */
/* 057 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 058 */             UTF8String scan_value_1 = scan_isNull_1 ?
/* 059 */             null : (scan_row_0.getUTF8String(1));
/* 060 */             boolean scan_isNull_2 = scan_row_0.isNullAt(2);
/* 061 */             UTF8String scan_value_2 = scan_isNull_2 ?
/* 062 */             null : (scan_row_0.getUTF8String(2));
/* 063 */             boolean scan_isNull_3 = scan_row_0.isNullAt(3);
/* 064 */             UTF8String scan_value_3 = scan_isNull_3 ?
/* 065 */             null : (scan_row_0.getUTF8String(3));
/* 066 */             boolean scan_isNull_4 = scan_row_0.isNullAt(4);
/* 067 */             UTF8String scan_value_4 = scan_isNull_4 ?
/* 068 */             null : (scan_row_0.getUTF8String(4));
/* 069 */             boolean scan_isNull_5 = scan_row_0.isNullAt(5);
/* 070 */             UTF8String scan_value_5 = scan_isNull_5 ?
/* 071 */             null : (scan_row_0.getUTF8String(5));
/* 072 */             boolean scan_isNull_6 = scan_row_0.isNullAt(6);
/* 073 */             UTF8String scan_value_6 = scan_isNull_6 ?
/* 074 */             null : (scan_row_0.getUTF8String(6));
/* 075 */             boolean scan_isNull_7 = scan_row_0.isNullAt(7);
/* 076 */             UTF8String scan_value_7 = scan_isNull_7 ?
/* 077 */             null : (scan_row_0.getUTF8String(7));
/* 078 */             boolean scan_isNull_8 = scan_row_0.isNullAt(8);
/* 079 */             int scan_value_8 = scan_isNull_8 ?
/* 080 */             -1 : (scan_row_0.getInt(8));
/* 081 */             filter_mutableStateArray_0[3].reset();
/* 082 */
/* 083 */             filter_mutableStateArray_0[3].zeroOutNullBytes();
/* 084 */
/* 085 */             if (false) {
/* 086 */               filter_mutableStateArray_0[3].setNullAt(0);
/* 087 */             } else {
/* 088 */               filter_mutableStateArray_0[3].write(0, scan_value_0);
/* 089 */             }
/* 090 */
/* 091 */             if (scan_isNull_1) {
/* 092 */               filter_mutableStateArray_0[3].setNullAt(1);
/* 093 */             } else {
/* 094 */               filter_mutableStateArray_0[3].write(1, scan_value_1);
/* 095 */             }
/* 096 */
/* 097 */             if (scan_isNull_2) {
/* 098 */               filter_mutableStateArray_0[3].setNullAt(2);
/* 099 */             } else {
/* 100 */               filter_mutableStateArray_0[3].write(2, scan_value_2);
/* 101 */             }
/* 102 */
/* 103 */             if (scan_isNull_3) {
/* 104 */               filter_mutableStateArray_0[3].setNullAt(3);
/* 105 */             } else {
/* 106 */               filter_mutableStateArray_0[3].write(3, scan_value_3);
/* 107 */             }
/* 108 */
/* 109 */             if (scan_isNull_4) {
/* 110 */               filter_mutableStateArray_0[3].setNullAt(4);
/* 111 */             } else {
/* 112 */               filter_mutableStateArray_0[3].write(4, scan_value_4);
/* 113 */             }
/* 114 */
/* 115 */             if (scan_isNull_5) {
/* 116 */               filter_mutableStateArray_0[3].setNullAt(5);
/* 117 */             } else {
/* 118 */               filter_mutableStateArray_0[3].write(5, scan_value_5);
/* 119 */             }
/* 120 */
/* 121 */             if (scan_isNull_6) {
/* 122 */               filter_mutableStateArray_0[3].setNullAt(6);
/* 123 */             } else {
/* 124 */               filter_mutableStateArray_0[3].write(6, scan_value_6);
/* 125 */             }
/* 126 */
/* 127 */             if (scan_isNull_7) {
/* 128 */               filter_mutableStateArray_0[3].setNullAt(7);
/* 129 */             } else {
/* 130 */               filter_mutableStateArray_0[3].write(7, scan_value_7);
/* 131 */             }
/* 132 */
/* 133 */             if (scan_isNull_8) {
/* 134 */               filter_mutableStateArray_0[3].setNullAt(8);
/* 135 */             } else {
/* 136 */               filter_mutableStateArray_0[3].write(8, scan_value_8);
/* 137 */             }
/* 138 */             append((filter_mutableStateArray_0[3].getRow()));
/* 139 */
/* 140 */           }
/* 141 */         }
/* 142 */
/* 143 */       } while(false);
/* 144 */       if (shouldStop()) return;
/* 145 */     }
/* 146 */   }
/* 147 */
/* 148 */ }

2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_68 stored as values in memory (estimated size 221.8 KB, free 1968.2 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_68 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_68 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_68_piece0 stored as bytes in memory (estimated size 20.7 KB, free 1968.2 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_68_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 20.7 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_68_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_68_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_68_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_68_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 68 from csv at UseCase2.java:75
2022-02-10 13:32:09 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 5148160 bytes, open cost is considered as scanning 4194304 bytes.
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$9.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(scala.Product2)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.RangePartitioner$$anonfun$9) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.RangePartitioner$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.sampleSizePerPartition$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.reflect.ClassTag org.apache.spark.RangePartitioner$$anonfun$13.evidence$5$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final int org.apache.spark.RangePartitioner$$anonfun$13.shift$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.RangePartitioner$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.RangePartitioner$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      <function0>
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[157] at csv at UseCase2.java:75
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[157] at csv at UseCase2.java:75)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.rdd.RDD
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      MapPartitionsRDD[157] at csv at UseCase2.java:75
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 4 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class java.lang.Object,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class scala.runtime.AbstractFunction0,Set())
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outermost object is not a closure or REPL line object,so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[157] at csv at UseCase2.java:75)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
2022-02-10 13:32:09 INFO  SparkContext:54 - Starting job: csv at UseCase2.java:75
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Got job 32 (csv at UseCase2.java:75) with 1 output partitions
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Final stage: ResultStage 34 (csv at UseCase2.java:75)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Parents of final stage: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Missing parents: List()
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 34 (name=csv at UseCase2.java:75;jobs=32))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ResultStage 34 (MapPartitionsRDD[157] at csv at UseCase2.java:75), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 34)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_69 stored as values in memory (estimated size 15.6 KB, free 1968.2 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_69 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_69 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_69_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1968.2 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_69_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 7.5 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_69_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_69_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_69_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_69_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 69 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[157] at csv at UseCase2.java:75) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 34.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 34.0: 2
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 34.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_34.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 8321 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 34.0 (TID 34)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_69
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_69 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_68
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_68 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 34.0 (TID 34). 1498 bytes result sent to driver
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_34.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 34.0 (TID 34) in 32 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ResultStage 34 (csv at UseCase2.java:75) finished in 0.032 s
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - After removal of stage 34, remaining stages = 0
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Job 32 finished: csv at UseCase2.java:75, took 0.037877 s
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG WholeStageCodegenExec:58 - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean sort_needToSort_0;
/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     sort_needToSort_0 = true;
/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 031 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 034 */       if (shouldStop()) return;
/* 035 */     }
/* 036 */
/* 037 */   }
/* 038 */
/* 039 */   protected void processNext() throws java.io.IOException {
/* 040 */     if (sort_needToSort_0) {
/* 041 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 042 */       sort_addToSorter_0();
/* 043 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 044 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 047 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 048 */       sort_needToSort_0 = false;
/* 049 */     }
/* 050 */
/* 051 */     while (sort_sortedIter_0.hasNext()) {
/* 052 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 053 */
/* 054 */       append(sort_outputRow_0);
/* 055 */
/* 056 */       if (shouldStop()) return;
/* 057 */     }
/* 058 */   }
/* 059 */
/* 060 */ }

2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.cleanedSource$2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.references$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.durationMs$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13.apply(int,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13) is now cleaned +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1) +++
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared fields: 4
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public static final long org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.serialVersionUID
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.internal.io.FileCommitProtocol org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.committer$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final org.apache.spark.sql.execution.datasources.WriteJobDescription org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.description$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      private final long org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.jobIdInstant$1
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + declared methods: 2
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final java.lang.Object org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(java.lang.Object,java.lang.Object)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -      public final org.apache.spark.sql.execution.datasources.WriteTaskResult org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + inner classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer classes: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + outer objects: 0
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + populating accessed fields because this is the starting closure
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + fields accessed by starting closure: 0 classes
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  + there are no enclosing objects!
2022-02-10 13:32:09 DEBUG ClosureCleaner:58 -  +++ closure <function2> (org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1) is now cleaned +++
2022-02-10 13:32:09 INFO  SparkContext:54 - Starting job: csv at UseCase2.java:75
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Registering RDD 158 (csv at UseCase2.java:75) as input to shuffle 2
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Got job 33 (csv at UseCase2.java:75) with 1 output partitions
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Final stage: ResultStage 36 (csv at UseCase2.java:75)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 35)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 35)
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 36 (name=csv at UseCase2.java:75;jobs=33))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List(ShuffleMapStage 35)
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ShuffleMapStage 35 (name=csv at UseCase2.java:75;jobs=33))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 35 (MapPartitionsRDD[158] at csv at UseCase2.java:75), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ShuffleMapStage 35)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_70 stored as values in memory (estimated size 18.1 KB, free 1968.1 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_70 locally took  16 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_70 without replication took  16 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_70_piece0 stored as bytes in memory (estimated size 8.6 KB, free 1968.1 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_70_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 8.6 KB, free: 1970.3 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_70_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_70_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_70_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_70_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 70 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[158] at csv at UseCase2.java:75) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 35.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 35.0: 2
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 35.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_35.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 8310 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 35.0 (TID 35)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_70
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_70 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:09 INFO  FileScanRDD:54 - Reading File path: file:///C:/Users/Anukul%20Thalkar/IdeaProjects/UseCases/src/main/resources/retail_db/customers/part-00000, range: 0-953856, partition values: [empty row]
2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for input[0, int, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true],input[8, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */     boolean isNull_8 = i.isNullAt(8);
/* 066 */     int value_8 = isNull_8 ?
/* 067 */     -1 : (i.getInt(8));
/* 068 */     if (isNull_8) {
/* 069 */       mutableStateArray_0[0].setNullAt(8);
/* 070 */     } else {
/* 071 */       mutableStateArray_0[0].write(8, value_8);
/* 072 */     }
/* 073 */
/* 074 */   }
/* 075 */
/* 076 */
/* 077 */   private void writeFields_0_0(InternalRow i) {
/* 078 */
/* 079 */     boolean isNull_0 = i.isNullAt(0);
/* 080 */     int value_0 = isNull_0 ?
/* 081 */     -1 : (i.getInt(0));
/* 082 */     if (isNull_0) {
/* 083 */       mutableStateArray_0[0].setNullAt(0);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(0, value_0);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_1 = i.isNullAt(1);
/* 089 */     UTF8String value_1 = isNull_1 ?
/* 090 */     null : (i.getUTF8String(1));
/* 091 */     if (isNull_1) {
/* 092 */       mutableStateArray_0[0].setNullAt(1);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(1, value_1);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_2 = i.isNullAt(2);
/* 098 */     UTF8String value_2 = isNull_2 ?
/* 099 */     null : (i.getUTF8String(2));
/* 100 */     if (isNull_2) {
/* 101 */       mutableStateArray_0[0].setNullAt(2);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(2, value_2);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_3 = i.isNullAt(3);
/* 107 */     UTF8String value_3 = isNull_3 ?
/* 108 */     null : (i.getUTF8String(3));
/* 109 */     if (isNull_3) {
/* 110 */       mutableStateArray_0[0].setNullAt(3);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(3, value_3);
/* 113 */     }
/* 114 */
/* 115 */     boolean isNull_4 = i.isNullAt(4);
/* 116 */     UTF8String value_4 = isNull_4 ?
/* 117 */     null : (i.getUTF8String(4));
/* 118 */     if (isNull_4) {
/* 119 */       mutableStateArray_0[0].setNullAt(4);
/* 120 */     } else {
/* 121 */       mutableStateArray_0[0].write(4, value_4);
/* 122 */     }
/* 123 */
/* 124 */   }
/* 125 */
/* 126 */ }

2022-02-10 13:32:09 INFO  Executor:54 - Finished task 0.0 in stage 35.0 (TID 35). 1396 bytes result sent to driver
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_35.0, runningTasks: 0
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Finished task 0.0 in stage 35.0 (TID 35) in 31 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - ShuffleMapTask finished on driver
2022-02-10 13:32:09 INFO  DAGScheduler:54 - ShuffleMapStage 35 (csv at UseCase2.java:75) finished in 0.047 s
2022-02-10 13:32:09 INFO  DAGScheduler:54 - looking for newly runnable stages
2022-02-10 13:32:09 INFO  DAGScheduler:54 - running: Set()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - waiting: Set(ResultStage 36)
2022-02-10 13:32:09 INFO  DAGScheduler:54 - failed: Set()
2022-02-10 13:32:09 DEBUG MapOutputTrackerMaster:58 - Increasing epoch to 3
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitStage(ResultStage 36 (name=csv at UseCase2.java:75;jobs=33))
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - missing: List()
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting ResultStage 36 (CoalescedRDD[162] at csv at UseCase2.java:75), which has no missing parents
2022-02-10 13:32:09 DEBUG DAGScheduler:58 - submitMissingTasks(ResultStage 36)
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_71 stored as values in memory (estimated size 147.4 KB, free 1968.0 MB)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_71 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_71 without replication took  0 ms
2022-02-10 13:32:09 INFO  MemoryStore:54 - Block broadcast_71_piece0 stored as bytes in memory (estimated size 54.7 KB, free 1967.9 MB)
2022-02-10 13:32:09 INFO  BlockManagerInfo:54 - Added broadcast_71_piece0 in memory on Clairvoyant-324.mshome.net:53727 (size: 54.7 KB, free: 1970.2 MB)
2022-02-10 13:32:09 DEBUG BlockManagerMaster:58 - Updated info of block broadcast_71_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Told master about block broadcast_71_piece0
2022-02-10 13:32:09 DEBUG BlockManager:58 - Put block broadcast_71_piece0 locally took  0 ms
2022-02-10 13:32:09 DEBUG BlockManager:58 - Putting block broadcast_71_piece0 without replication took  0 ms
2022-02-10 13:32:09 INFO  SparkContext:54 - Created broadcast 71 from broadcast at DAGScheduler.scala:1184
2022-02-10 13:32:09 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 36 (CoalescedRDD[162] at csv at UseCase2.java:75) (first 15 tasks are for partitions Vector(0))
2022-02-10 13:32:09 INFO  TaskSchedulerImpl:54 - Adding task set 36.0 with 1 tasks
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Epoch for TaskSet 36.0: 3
2022-02-10 13:32:09 DEBUG TaskSetManager:58 - Valid locality levels for TaskSet 36.0: NO_PREF, ANY
2022-02-10 13:32:09 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_36.0, runningTasks: 0
2022-02-10 13:32:09 INFO  TaskSetManager:54 - Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 8043 bytes)
2022-02-10 13:32:09 INFO  Executor:54 - Running task 0.0 in stage 36.0 (TID 36)
2022-02-10 13:32:09 DEBUG BlockManager:58 - Getting local block broadcast_71
2022-02-10 13:32:09 DEBUG BlockManager:58 - Level for block broadcast_71 is StorageLevel(disk, memory, deserialized, 1 replicas)
2022-02-10 13:32:09 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2022-02-10 13:32:09 DEBUG NativeIO:195 - Initialized cache for IDs to User/Group mapping with a  cache timeout of 14400 seconds.
2022-02-10 13:32:09 DEBUG MapOutputTrackerMaster:58 - Fetching outputs for shuffle 2, partitions 0-1
2022-02-10 13:32:09 DEBUG ShuffleBlockFetcherIterator:58 - maxBytesInFlight: 50331648, targetRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
2022-02-10 13:32:09 INFO  ShuffleBlockFetcherIterator:54 - Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
2022-02-10 13:32:09 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 0 ms
2022-02-10 13:32:09 DEBUG ShuffleBlockFetcherIterator:58 - Start fetching local blocks: 
2022-02-10 13:32:09 DEBUG ShuffleBlockFetcherIterator:58 - Got local blocks in  0 ms
2022-02-10 13:32:09 DEBUG GenerateOrdering:58 - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     InternalRow i = null;
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA_0;
/* 021 */     int primitiveA_0;
/* 022 */     {
/* 023 */       boolean isNull_0 = i.isNullAt(0);
/* 024 */       int value_0 = isNull_0 ?
/* 025 */       -1 : (i.getInt(0));
/* 026 */       isNullA_0 = isNull_0;
/* 027 */       primitiveA_0 = value_0;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB_0;
/* 031 */     int primitiveB_0;
/* 032 */     {
/* 033 */       boolean isNull_0 = i.isNullAt(0);
/* 034 */       int value_0 = isNull_0 ?
/* 035 */       -1 : (i.getInt(0));
/* 036 */       isNullB_0 = isNull_0;
/* 037 */       primitiveB_0 = value_0;
/* 038 */     }
/* 039 */     if (isNullA_0 && isNullB_0) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA_0) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB_0) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA_0 > primitiveB_0 ? 1 : primitiveA_0 < primitiveB_0 ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

2022-02-10 13:32:09 DEBUG GenerateUnsafeProjection:58 - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

2022-02-10 13:32:09 DEBUG TaskMemoryManager:224 - Task 36 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@687e4c7d
2022-02-10 13:32:09 DEBUG TaskMemoryManager:233 - Task 36 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@687e4c7d
2022-02-10 13:32:10 DEBUG OutputCommitCoordinator:58 - Commit allowed for stage=36.0, partition=0, task attempt 0
2022-02-10 13:32:10 INFO  FileOutputCommitter:439 - Saved output of task 'attempt_20220210133209_0036_m_000000_36' to file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2/_temporary/0/task_20220210133209_0036_m_000000
2022-02-10 13:32:10 INFO  SparkHadoopMapRedUtil:54 - attempt_20220210133209_0036_m_000000_36: Committed
2022-02-10 13:32:10 INFO  Executor:54 - Finished task 0.0 in stage 36.0 (TID 36). 3904 bytes result sent to driver
2022-02-10 13:32:10 DEBUG TaskSchedulerImpl:58 - parentName: , name: TaskSet_36.0, runningTasks: 0
2022-02-10 13:32:10 DEBUG TaskSetManager:58 - No tasks for locality level NO_PREF, so moving to locality level ANY
2022-02-10 13:32:10 INFO  TaskSetManager:54 - Finished task 0.0 in stage 36.0 (TID 36) in 96 ms on localhost (executor driver) (1/1)
2022-02-10 13:32:10 INFO  TaskSchedulerImpl:54 - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2022-02-10 13:32:10 INFO  DAGScheduler:54 - ResultStage 36 (csv at UseCase2.java:75) finished in 0.112 s
2022-02-10 13:32:10 DEBUG DAGScheduler:58 - After removal of stage 35, remaining stages = 1
2022-02-10 13:32:10 DEBUG DAGScheduler:58 - After removal of stage 36, remaining stages = 0
2022-02-10 13:32:10 INFO  DAGScheduler:54 - Job 33 finished: csv at UseCase2.java:75, took 0.161329 s
2022-02-10 13:32:10 DEBUG FileOutputCommitter:337 - Merging data from DeprecatedRawLocalFileStatus{path=file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2/_temporary/0/task_20220210133209_0036_m_000000; isDirectory=true; modification_time=1644480129966; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2
2022-02-10 13:32:10 DEBUG FileOutputCommitter:337 - Merging data from DeprecatedRawLocalFileStatus{path=file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2/_temporary/0/task_20220210133209_0036_m_000000/part-00000-a3376de5-12ae-4425-9278-1aa83a6dbee3-c000.csv; isDirectory=false; length=0; replication=1; blocksize=33554432; modification_time=1644480129966; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/C:/Users/Anukul Thalkar/IdeaProjects/UseCases/src/main/resources/outputs/UseCase2/part-00000-a3376de5-12ae-4425-9278-1aa83a6dbee3-c000.csv
2022-02-10 13:32:10 DEBUG SQLHadoopMapReduceCommitProtocol:58 - Committing files staged for absolute locations Map()
2022-02-10 13:32:10 INFO  FileFormatWriter:54 - Write Job 4c89ab4c-8209-4669-a360-1d3cd61ef868 committed.
2022-02-10 13:32:10 INFO  FileFormatWriter:54 - Finished processing stats for write job 4c89ab4c-8209-4669-a360-1d3cd61ef868.
2022-02-10 13:32:10 INFO  UseCase2:77 - --------------------------------------------Completed---------------------------------------------------
2022-02-10 13:32:10 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping Server@4b85880b{STARTED}[9.4.z-SNAPSHOT]
2022-02-10 13:32:10 DEBUG Server:433 - doStop Server@4b85880b{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran SparkUI-31-acceptor-0@7db534f2-ServerConnector@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040} in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=3,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractHandlerContainer:167 - Graceful shutdown Server@4b85880b{STOPPING}[9.4.z-SNAPSHOT] by 
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping SelectorManager@Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@19b30c92{STARTED} id=3 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@1ea2cb99 on ManagedSelector@19b30c92{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@19b30c92{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@1ea2cb99
2022-02-10 13:32:10 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@19b30c92{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 waiting with 0 keys
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@38291466 on ManagedSelector@19b30c92{STOPPING} id=3 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@19b30c92{STOPPING} id=3 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@4f680774 processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@38291466
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@2c95ac9e in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=4,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.061+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@138fe6ec/SelectorProducer@5e77f0f4/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.061+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@19b30c92{STOPPED} id=3 keys=-1 selected=-1 updates=0
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@1b28f282{STARTED} id=2 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@51ecb287 on ManagedSelector@1b28f282{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@1b28f282{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@51ecb287
2022-02-10 13:32:10 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@1b28f282{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f waiting with 0 keys
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@5e53afe2 on ManagedSelector@1b28f282{STOPPING} id=2 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@1b28f282{STOPPING} id=2 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@5823aa8f processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@5e53afe2
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@363f6148/SelectorProducer@4b21844c/PRODUCING/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@379ab47b in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@363f6148/SelectorProducer@4b21844c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=5,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@1b28f282{STOPPED} id=2 keys=-1 selected=-1 updates=0
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@59aa20b3{STARTED} id=1 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@562559b2 on ManagedSelector@59aa20b3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@59aa20b3{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@562559b2
2022-02-10 13:32:10 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@59aa20b3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c waiting with 0 keys
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@26dfed34 on ManagedSelector@59aa20b3{STOPPING} id=1 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@59aa20b3{STOPPING} id=1 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@2bbb566c processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@26dfed34
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4efcf8a in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=6,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@552518c3/SelectorProducer@1a69561c/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@59aa20b3{STOPPED} id=1 keys=-1 selected=-1 updates=0
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ManagedSelector@25f9407e{STARTED} id=0 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$CloseConnections@12130be2 on ManagedSelector@25f9407e{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@25f9407e{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$CloseConnections@12130be2
2022-02-10 13:32:10 DEBUG ManagedSelector:996 - Closing 0 connections on ManagedSelector@25f9407e{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG ManagedSelector:605 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b waiting with 0 keys
2022-02-10 13:32:10 DEBUG ManagedSelector:286 - Queued change lazy=false org.spark_project.jetty.io.ManagedSelector$StopSelector@51fc9afd on ManagedSelector@25f9407e{STOPPING} id=0 keys=0 selected=0 updates=0
2022-02-10 13:32:10 DEBUG ManagedSelector:304 - Wakeup on submit ManagedSelector@25f9407e{STOPPING} id=0 keys=0 selected=0 updates=1
2022-02-10 13:32:10 DEBUG ManagedSelector:194 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b woken with none selected
2022-02-10 13:32:10 DEBUG ManagedSelector:612 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b woken up from select, 0/0/0 selected
2022-02-10 13:32:10 DEBUG ManagedSelector:628 - Selector sun.nio.ch.WindowsSelectorImpl@4774c37b processing 0 keys, 1 updates
2022-02-10 13:32:10 DEBUG ManagedSelector:558 - updateable 1
2022-02-10 13:32:10 DEBUG ManagedSelector:567 - update org.spark_project.jetty.io.ManagedSelector$StopSelector@51fc9afd
2022-02-10 13:32:10 DEBUG ManagedSelector:587 - updates 0
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.io.ManagedSelector$$Lambda$28/1209451152@4940809c in QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=7,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED EatWhatYouKill@4a9cc6cb/SelectorProducer@3b9d6699/IDLE/p=false/QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-02-10T13:32:10.077+05:30
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ManagedSelector@25f9407e{STOPPED} id=0 keys=-1 selected=-1 updates=0
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED SelectorManager@Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping HttpConnectionFactory@71104a4[HTTP/1.1]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED HttpConnectionFactory@71104a4[HTTP/1.1]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ScheduledExecutorScheduler@b91d8c4{STARTED}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ScheduledExecutorScheduler@b91d8c4{STOPPED}
2022-02-10 13:32:10 INFO  AbstractConnector:381 - Stopped Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED Spark@76a82f33{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2022-02-10 13:32:10 DEBUG AbstractHandler:107 - stopping Server@4b85880b{STOPPING}[9.4.z-SNAPSHOT]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ContextHandlerCollection@6c67e137{STARTED}
2022-02-10 13:32:10 DEBUG AbstractHandler:107 - stopping ContextHandlerCollection@6c67e137{STOPPING}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ContextHandlerCollection@6c67e137{STOPPED}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ErrorHandler@76a36b71{STARTED}
2022-02-10 13:32:10 DEBUG AbstractHandler:107 - stopping ErrorHandler@76a36b71{STOPPING}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ErrorHandler@76a36b71{STOPPED}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping QueuedThreadPool[SparkUI]@165b8a71{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:224 - Stopping QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@47da3952{s=0/8,p=0}]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:212 - stopping ReservedThreadExecutor@47da3952{s=0/8,p=0}
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED ReservedThreadExecutor@47da3952{s=-1/8,p=0}
2022-02-10 13:32:10 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-31,5,main] for 14999
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-33,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-34,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-32,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1035 - run org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-30,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-31,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-28,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-29,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1038 - ran org.spark_project.jetty.util.thread.QueuedThreadPool$$Lambda$4/1023996917@2e420bb2 in QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-28,5,main] for 14998
2022-02-10 13:32:10 DEBUG QueuedThreadPool:1065 - Thread[SparkUI-27,5,main] exited for QueuedThreadPool[SparkUI]@165b8a71{STOPPING,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG QueuedThreadPool:317 - Waiting for Thread[SparkUI-27,5,main] for 14997
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED QueuedThreadPool[SparkUI]@165b8a71{STOPPED,8<=0<=200,i=8,r=-1,q=0}[NO_TRY]
2022-02-10 13:32:10 DEBUG AbstractLifeCycle:224 - STOPPED Server@4b85880b{STOPPED}[9.4.z-SNAPSHOT]
2022-02-10 13:32:10 INFO  SparkUI:54 - Stopped Spark web UI at http://Clairvoyant-324.mshome.net:4040
2022-02-10 13:32:10 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2022-02-10 13:32:10 INFO  MemoryStore:54 - MemoryStore cleared
2022-02-10 13:32:10 INFO  BlockManager:54 - BlockManager stopped
2022-02-10 13:32:10 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2022-02-10 13:32:10 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2022-02-10 13:32:10 INFO  SparkContext:54 - Successfully stopped SparkContext
2022-02-10 13:32:10 INFO  ShutdownHookManager:54 - Shutdown hook called
2022-02-10 13:32:10 INFO  ShutdownHookManager:54 - Deleting directory C:\Users\Anukul Thalkar\AppData\Local\Temp\spark-e3667821-65a4-49cf-a972-9a4f0ef109f4
